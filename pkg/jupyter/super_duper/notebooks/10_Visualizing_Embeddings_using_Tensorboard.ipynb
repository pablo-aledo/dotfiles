{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "10_Visualizing_Embeddings_using_Tensorboard.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sMU8C4zXaIi"
      },
      "source": [
        "In this notebook we will demonstrate how you can use Tensorboard to visualize word embeddings which we created in the Training_embeddings_using_gensim.ipynb notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slEFmtT5XaIm"
      },
      "source": [
        "#installing the required libraries\n",
        "!pip install tensorflow==1.14.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIjoPVzkXaIn"
      },
      "source": [
        "#making the required imports\n",
        "import warnings #ignoring the generated warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib.tensorboard.plugins import projector\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCIlHwsuXaIn"
      },
      "source": [
        "#Loading the model\n",
        "cwd=os.getcwd() \n",
        "model = KeyedVectors.load_word2vec_format(cwd+'\\Models\\word2vec_cbow.bin', binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BReFWO1cXaIo"
      },
      "source": [
        "#get the model's vocabulary size\n",
        "max_size = len(model.wv.vocab)-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qcs52ZHkXaIo"
      },
      "source": [
        "#make a numpy array of 0s with the size of the vocabulary and dimensions of our model\n",
        "w2v = np.zeros((max_size,model.wv.vector_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyLUNFYrXaIp"
      },
      "source": [
        "#Now we create a new file called metadata.tsv where we save all the words in our model \n",
        "#we also store the embedding of each word in the w2v matrix\n",
        "if not os.path.exists('projections'):\n",
        "    os.makedirs('projections')\n",
        "    \n",
        "with open(\"projections/metadata.tsv\", 'w+',encoding=\"utf-8\") as file_metadata: #changed    added encoding=\"utf-8\"\n",
        "    \n",
        "    for i, word in enumerate(model.wv.index2word[:max_size]):\n",
        "        \n",
        "        #store the embeddings of the word\n",
        "        w2v[i] = model.wv[word]\n",
        "        \n",
        "        #write the word to a file \n",
        "        file_metadata.write(word + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mUnqlwNXaIq"
      },
      "source": [
        "#initializing tf session\n",
        "sess = tf.InteractiveSession()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-Wm6G2uXaIq"
      },
      "source": [
        "#Initialize the tensorflow variable called embeddings that holds the word embeddings:\n",
        "with tf.device(\"/cpu:0\"):\n",
        "    embedding = tf.Variable(w2v, trainable=False, name='embedding')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhbGx_DNXaIr"
      },
      "source": [
        "#Initialize all variables\n",
        "tf.global_variables_initializer().run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-kKuxWqXaIt"
      },
      "source": [
        "#object of the saver class which is actually used for saving and restoring variables to and from our checkpoints\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hukuhXUkXaIu"
      },
      "source": [
        "#with FileWriter,we save summary and events to the event file\n",
        "writer = tf.summary.FileWriter('projections', sess.graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJi6Fwy1XaIu"
      },
      "source": [
        "# Initialize the projectors and add the embeddings\n",
        "config = projector.ProjectorConfig()\n",
        "embed= config.embeddings.add()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6heCQG89XaIv"
      },
      "source": [
        "#specify our tensor_name as embedding and metadata_path to the metadata.tsv file\n",
        "embed.tensor_name = 'embedding'\n",
        "embed.metadata_path = 'metadata.tsv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYAjKpa6XaIv",
        "outputId": "c924f6fa-cc25-476c-fed9-680b3f20232d"
      },
      "source": [
        "#save the model\n",
        "projector.visualize_embeddings(writer, config)\n",
        "\n",
        "saver.save(sess, 'projections/model.ckpt', global_step=max_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'projections/model.ckpt-161017'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT8nr5DbXaIw"
      },
      "source": [
        "Open a terminal window and type the following command\n",
        "\n",
        "tensorboard --logdir=projections --port=8000\n",
        "\n",
        "If the tensorboard does not work for you try providing the absolute path for projections and re-run the above command\n",
        "\n",
        "If youve done everything right until you will get a link in your terminal through which you can access the tensorboard. Click on the link or copy paste it in your browser. You should see something similar to this.\n",
        "![TensorBoard-1](https://github.com/practical-nlp/practical-nlp/blob/master/Ch3/Images/TensorBoard-1.png?raw=1)\n",
        "<br>\n",
        "In the top right corner near \"INACTIVE\" click the dropdown arrow. And select PROJECTIONS from te dropdown menu\n",
        "![TensorBoard-2](https://github.com/practical-nlp/practical-nlp/blob/master/Ch3/Images/TensorBoard-2.png?raw=1)\n",
        "<br>\n",
        "Wait for a few seconds for it to load. You can now see your embeddings there are a lot of setting you can play around and experiment with.\n",
        "![TensorBoard-3](https://github.com/practical-nlp/practical-nlp/blob/master/Ch3/Images/TensorBoard-3.png?raw=1)\n",
        "<br>\n",
        "Output when we search for a specific word in this case \"human\" and isolate only those points\n",
        "![TensorBoard-4](https://github.com/practical-nlp/practical-nlp/blob/master/Ch3/Images/TensorBoard-4.png?raw=1)"
      ]
    }
  ]
}