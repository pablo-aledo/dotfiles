{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment winning code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZQ5LkjxHd1g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "e2c785df-9dc9-4087-e990-e365aadd6595"
      },
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package libfuse2:amd64.\n",
            "(Reading database ... 22280 files and directories currently installed.)\n",
            "Preparing to unpack .../libfuse2_2.9.7-1ubuntu1_amd64.deb ...\n",
            "Unpacking libfuse2:amd64 (2.9.7-1ubuntu1) ...\n",
            "Selecting previously unselected package fuse.\n",
            "Preparing to unpack .../fuse_2.9.7-1ubuntu1_amd64.deb ...\n",
            "Unpacking fuse (2.9.7-1ubuntu1) ...\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.0-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.0-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up libfuse2:amd64 (2.9.7-1ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Setting up fuse (2.9.7-1ubuntu1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.0-0ubuntu1~ubuntu18.04.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF_CaavYHy2l",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "outputId": "2bd59795-dd6e-43bf-cf50-1c4a27c9152f"
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-165bbd0b-e87d-492b-a9d1-1bce86187b5d\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-165bbd0b-e87d-492b-a9d1-1bce86187b5d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87XIRdaItRU5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "828ccabd-b20f-460e-d49f-e3bbe94ea893"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json                 dataset1_final_eval.csv    dataset2_interim_eval.csv\n",
            "data_processed.csv       dataset1_interim_eval.csv  interim.csv\n",
            "data_processed_test.csv  dataset2.csv               interim_results.csv\n",
            "dataset1.csv             dataset2_final_eval.csv    \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX-TReo5YRnq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "aabe0612-e590-49ac-dcdb-b62c59b86574"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import re\n",
        "import os\n",
        "print(os.listdir(\"../\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['bin', 'home', 'sys', 'boot', 'srv', 'sbin', 'tmp', 'lib', 'var', 'lib64', 'media', 'usr', 'opt', 'proc', 'dev', 'etc', 'run', 'root', 'mnt', 'content', '.dockerenv', 'datalab', 'colabtools', 'tools']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Bwjt3fwd1lC"
      },
      "source": [
        "def analyze_tweet(tweet):\n",
        "    result = {}\n",
        "    result['MENTIONS'] = tweet.count('USER_MENTION')\n",
        "    result['URLS'] = tweet.count('URL')\n",
        "    result['POS_EMOS'] = tweet.count('EMO_POS')\n",
        "    result['NEG_EMOS'] = tweet.count('EMO_NEG')\n",
        "    tweet = tweet.replace('USER_MENTION', '').replace(\n",
        "        'URL', '')\n",
        "    words = tweet.split()\n",
        "    result['WORDS'] = len(words)\n",
        "    bigrams = get_bigrams(words)\n",
        "    result['BIGRAMS'] = len(bigrams)\n",
        "    return result, words, bigrams\n",
        "\n",
        "\n",
        "def get_bigrams(tweet_words):\n",
        "    bigrams = []\n",
        "    num_words = len(tweet_words)\n",
        "    for i in range(num_words - 1):\n",
        "        bigrams.append((tweet_words[i], tweet_words[i + 1]))\n",
        "    return bigrams\n",
        "\n",
        "\n",
        "def get_bigram_freqdist(bigrams):\n",
        "    freq_dict = {}\n",
        "    for bigram in bigrams:\n",
        "        if freq_dict.get(bigram):\n",
        "            freq_dict[bigram] += 1\n",
        "        else:\n",
        "            freq_dict[bigram] = 1\n",
        "    counter = Counter(freq_dict)\n",
        "    return counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzuWvt4Hd3DC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "51db505c-787b-4ba4-c6a4-4bed4c36f027"
      },
      "source": [
        "import sys\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "\n",
        "def preprocess_word(word):\n",
        "    # Remove punctuation\n",
        "    word = word.strip('\\'\"?!,.():;')\n",
        "    # Convert more than 2 letter repetitions to 2 letter\n",
        "    # funnnnny --> funny\n",
        "    word = re.sub(r'(.)\\1+', r'\\1\\1', word)\n",
        "    # Remove - & '\n",
        "    word = re.sub(r'(-|\\')', '', word)\n",
        "    return word\n",
        "\n",
        "\n",
        "def is_valid_word(word):\n",
        "    return (re.search(r'^[a-zA-Z][a-z0-9A-Z\\._]*$', word) is not None)\n",
        "\n",
        "def handle_emojis(tweet):\n",
        "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
        "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMO_POS ', tweet)\n",
        "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
        "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMO_POS ', tweet)\n",
        "    # Love -- <3, :*\n",
        "    tweet = re.sub(r'(<3|:\\*)', ' EMO_POS ', tweet)\n",
        "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
        "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMO_POS ', tweet)\n",
        "    # Sad -- :-(, : (, :(, ):, )-:\n",
        "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', tweet)\n",
        "    # Cry -- :,(, :'(, :\"(\n",
        "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMO_NEG ', tweet)\n",
        "    return tweet\n",
        "\n",
        "def preprocess_tweet(tweet):\n",
        "    processed_tweet = []\n",
        "    # Convert to lower case\n",
        "    tweet = tweet.lower()\n",
        "    # Replaces URLs with the word URL\n",
        "    tweet = re.sub(r'((www\\.[\\S]+)|(https?://[\\S]+))', ' URL ', tweet)\n",
        "    # Replace @handle with the word USER_MENTION\n",
        "    tweet = re.sub(r'@[\\S]+', 'USER_MENTION', tweet)\n",
        "    # Replaces #hashtag with hashtag\n",
        "    tweet = re.sub(r'#(\\S+)', r' \\1 ', tweet)\n",
        "    # Remove RT (retweet)\n",
        "    tweet = re.sub(r'\\brt\\b', '', tweet)\n",
        "    # Replace 2+ dots with space\n",
        "    tweet = re.sub(r'\\.{2,}', ' ', tweet)\n",
        "    # Strip space, \" and ' from tweet\n",
        "    tweet = tweet.strip(' \"\\'')\n",
        "    # Replace emojis with either EMO_POS or EMO_NEG\n",
        "    tweet = handle_emojis(tweet)\n",
        "    # Replace multiple spaces with a single space\n",
        "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
        "    words = tweet.split()\n",
        "\n",
        "    for word in words:\n",
        "        word = preprocess_word(word)\n",
        "        if is_valid_word(word):\n",
        "            if use_stemmer:\n",
        "                word = str(porter_stemmer.stem(word))\n",
        "            processed_tweet.append(word)\n",
        "\n",
        "    return ' '.join(processed_tweet)\n",
        "  \n",
        "  \n",
        "def preprocess_csv(csv_file_name, processed_file_name, test_file=False):\n",
        "    save_to_file = open(processed_file_name, 'w')\n",
        "\n",
        "    with open(csv_file_name, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            tweet_id = line[:line.find(',')]\n",
        "            if not test_file:\n",
        "                line = line[1 + line.find(','):]\n",
        "                positive = int(line[:line.find(',')])\n",
        "            line = line[1 + line.find(','):]\n",
        "            tweet = line\n",
        "            processed_tweet = preprocess_tweet(tweet)\n",
        "            if not test_file:\n",
        "                save_to_file.write('%s,%d,%s\\n' %\n",
        "                                   (tweet_id, positive, processed_tweet))\n",
        "            else:\n",
        "                save_to_file.write('%s,%s\\n' %\n",
        "                                   (tweet_id, processed_tweet))\n",
        "            \n",
        "    save_to_file.close()\n",
        "    print( '\\nSaved processed tweets to: %s' % processed_file_name)\n",
        "    return processed_file_name\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "   \n",
        "    use_stemmer = False\n",
        "    csv_file_name = 'dataset2.csv'\n",
        "    processed_file_name = 'data_processed.csv'\n",
        "    preprocess_csv(csv_file_name, processed_file_name, test_file=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Saved processed tweets to: data_processed.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iMhRwjW8RAJ"
      },
      "source": [
        "x = pd.read_csv('data_processed.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Nb-oWB99zAJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "2b5c5b98-da20-4b01-eb7f-b6633dd85780"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "   \n",
        "    use_stemmer = False\n",
        "    csv_file_name = 'dataset2_interim_eval.csv'\n",
        "    processed_file_name = 'data_processed_test.csv'\n",
        "    preprocess_csv(csv_file_name, processed_file_name, test_file=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Saved processed tweets to: data_processed_test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRbKiapfCiEh"
      },
      "source": [
        "train_df = pd.read_csv('data_processed.csv')\n",
        "test_df = pd.read_csv('data_processed_test.csv')\n",
        "train_df['sentiment'] = pd.read_csv('dataset2.csv')['sentiment']\n",
        "train_df.columns=['a','text','sentiment']\n",
        "test_df.columns=['a','text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJuQ_6hZAVOo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "a6aae35e-6c16-4185-ed4f-5d2795bb2a87"
      },
      "source": [
        "print(len(train_df))\n",
        "train_df.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3791\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a</th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>my hair is blue</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>im sweating my forthcoming trip to e3 if i can...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>USER_MENTION im sorry i dont understand your l...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>website2018 my cupcakes</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>reading buyology before bedtime great premise ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   a                                               text  sentiment\n",
              "0  0                                    my hair is blue          1\n",
              "1  1  im sweating my forthcoming trip to e3 if i can...          0\n",
              "2  2  USER_MENTION im sorry i dont understand your l...          0\n",
              "3  3                            website2018 my cupcakes          1\n",
              "4  4  reading buyology before bedtime great premise ...          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEr9NFVNCh5v"
      },
      "source": [
        "def feat(data):\n",
        "    ment=[]\n",
        "    url=[]\n",
        "    pe=[]\n",
        "    ne=[]\n",
        "    bg=[]\n",
        "    for i in range(len(data)):\n",
        "        ment.append(analyze_tweet(data['text'].values[i])[0]['MENTIONS'])\n",
        "        url.append(analyze_tweet(data['text'].values[i])[0]['URLS'])\n",
        "        pe.append(analyze_tweet(data['text'].values[i])[0]['POS_EMOS'])\n",
        "        ne.append(analyze_tweet(data['text'].values[i])[0]['NEG_EMOS'])\n",
        "        bg.append(analyze_tweet(data['text'].values[i])[0]['BIGRAMS'])\n",
        "    data['ment']=ment\n",
        "    data['url']=url\n",
        "    data['pe']=pe\n",
        "    data['ne']=ne\n",
        "    data['bg']=bg\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S9cGeae2oMO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVBq6dqmCIi4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "f2d583e3-e661-4841-ecb4-7c6a20b44aee"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D,GRU\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "\n",
        "embed_size = 500 \n",
        "max_features = 90000 \n",
        "maxlen = 100 \n",
        "\n",
        "train_df=feat(train_df)\n",
        "test_df=feat(test_df)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(train_df.text))\n",
        "train_X = tokenizer.texts_to_sequences(train_df.text)\n",
        "test_X = tokenizer.texts_to_sequences(test_df.text)\n",
        "\n",
        "## Pad the sentences \n",
        "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
        "test_X = pad_sequences(test_X, maxlen=maxlen)\n",
        "train_X = pd.concat([pd.DataFrame(train_X),train_df[['ment','pe','ne','url','bg']].reset_index(drop=True)],axis=1)\n",
        "test_X = pd.concat([pd.DataFrame(test_X),train_df[['ment','pe','ne','url','bg']].reset_index(drop=True)],axis=1)\n",
        "## Get the sentiment values\n",
        "train_y = train_df['sentiment'].values\n",
        "\n",
        "train_x,val_X,train_Y, val_Y = train_test_split(train_X,train_y\n",
        "                                    , test_size=0.1, random_state=2018)\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf = RandomForestClassifier()\n",
        "clf = xgb.XGBClassifier()\n",
        "\n",
        "clf.fit(train_x,train_Y)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(val_Y,clf.predict(val_X)))\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(train_df.text))\n",
        "train_X = tokenizer.texts_to_sequences(train_df.text)\n",
        "test_X = tokenizer.texts_to_sequences(test_df.text)\n",
        "\n",
        "## Pad the sentences \n",
        "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
        "test_X = pad_sequences(test_X, maxlen=maxlen)\n",
        "train_X = pd.concat([pd.DataFrame(train_X),train_df[['ment','pe','ne','url','bg']].reset_index(drop=True)],axis=1)\n",
        "test_X = pd.concat([pd.DataFrame(test_X),train_df[['ment','pe','ne','url','bg']].reset_index(drop=True)],axis=1)\n",
        "## Get the sentiment values\n",
        "train_y = train_df['sentiment'].values\n",
        "\n",
        "train_x,val_X,train_Y, val_Y = train_test_split(train_X,train_y\n",
        "                                    , test_size=0.1, random_state=2018)\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf = RandomForestClassifier()\n",
        "clf = xgb.XGBClassifier()\n",
        "clf.fit(train_x,train_Y)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(val_Y,clf.predict(val_X)))\n",
        "\n",
        "\n",
        "\n",
        "train_df.head()\n",
        "\n",
        "new_train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=2018)\n",
        "\n",
        "## fill up the missing values\n",
        "train_X = train_df[\"text\"].fillna(\"_na_\").values\n",
        "new_train_X = new_train_df[\"text\"].fillna(\"_na_\").values\n",
        "val_X = val_df[\"text\"].fillna(\"_na_\").values\n",
        "test_X = test_df[\"text\"].fillna(\"_na_\").values\n",
        "\n",
        "## Tokenize the sentences\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(train_X))\n",
        "train_X = tokenizer.texts_to_sequences(train_X)\n",
        "new_train_X = tokenizer.texts_to_sequences(new_train_X)\n",
        "val_X = tokenizer.texts_to_sequences(val_X)\n",
        "test_X = tokenizer.texts_to_sequences(test_X)\n",
        "\n",
        "## Pad the sentences \n",
        "new_train_X = pad_sequences(new_train_X, maxlen=maxlen)\n",
        "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
        "val_X = pad_sequences(val_X, maxlen=maxlen)\n",
        "test_X = pad_sequences(test_X, maxlen=maxlen)\n",
        "\n",
        "## Get the sentiment values\n",
        "new_train_y = new_train_df['sentiment'].values\n",
        "train_y = train_df['sentiment'].values\n",
        "val_y = val_df['sentiment'].values\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf = RandomForestClassifier()\n",
        "\n",
        "clf.fit(new_train_X,new_train_y)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(val_y,clf.predict(val_X)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.6421052631578947\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.6421052631578947\n",
            "0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpHXBcOeyt9U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "0e988053-553a-4183-e605-0aa787fdf4da"
      },
      "source": [
        "\n",
        "def get_model():\n",
        "    embed_size = 128\n",
        "    inp = Input(shape=(maxlen, ))\n",
        "    x = Embedding(max_features, embed_size)(inp)\n",
        "    x = Bidirectional(LSTM(50, return_sequences=True))(x)\n",
        "    x = GlobalMaxPool1D()(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(50, activation=\"relu\")(x)    \n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "model = get_model()\n",
        "\n",
        "model.fit(new_train_X, new_train_y, batch_size=512, epochs=10, validation_data=(val_X, val_y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3411 samples, validate on 380 samples\n",
            "Epoch 1/10\n",
            "3411/3411 [==============================] - 23s 7ms/step - loss: 0.6910 - acc: 0.5432 - val_loss: 0.6889 - val_acc: 0.5474\n",
            "Epoch 2/10\n",
            "3411/3411 [==============================] - 16s 5ms/step - loss: 0.6856 - acc: 0.5558 - val_loss: 0.6851 - val_acc: 0.5474\n",
            "Epoch 3/10\n",
            "3411/3411 [==============================] - 16s 5ms/step - loss: 0.6804 - acc: 0.5558 - val_loss: 0.6767 - val_acc: 0.5474\n",
            "Epoch 4/10\n",
            "3411/3411 [==============================] - 16s 5ms/step - loss: 0.6661 - acc: 0.5576 - val_loss: 0.6576 - val_acc: 0.5500\n",
            "Epoch 5/10\n",
            "3411/3411 [==============================] - 16s 5ms/step - loss: 0.6301 - acc: 0.6201 - val_loss: 0.6093 - val_acc: 0.6868\n",
            "Epoch 6/10\n",
            "3411/3411 [==============================] - 16s 5ms/step - loss: 0.5237 - acc: 0.7593 - val_loss: 0.6820 - val_acc: 0.7289\n",
            "Epoch 7/10\n",
            "3411/3411 [==============================] - 16s 5ms/step - loss: 0.4391 - acc: 0.8481 - val_loss: 0.6194 - val_acc: 0.6921\n",
            "Epoch 8/10\n",
            "3411/3411 [==============================] - 16s 5ms/step - loss: 0.5145 - acc: 0.7356 - val_loss: 0.6979 - val_acc: 0.6947\n",
            "Epoch 9/10\n",
            "3411/3411 [==============================] - 16s 5ms/step - loss: 0.4461 - acc: 0.8229 - val_loss: 0.7246 - val_acc: 0.7132\n",
            "Epoch 10/10\n",
            "3411/3411 [==============================] - 16s 5ms/step - loss: 0.3525 - acc: 0.8745 - val_loss: 0.6780 - val_acc: 0.7079\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe1a84a1cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPGb6KA_uTQi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "fcdb07cc-2a37-4d8d-da66-7a52a30af291"
      },
      "source": [
        "model.fit(train_X, train_y, batch_size=512, epochs=15)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "3791/3791 [==============================] - 18s 5ms/step - loss: 0.3445 - acc: 0.8805\n",
            "Epoch 2/15\n",
            "3791/3791 [==============================] - 17s 5ms/step - loss: 0.3053 - acc: 0.9013\n",
            "Epoch 3/15\n",
            "3791/3791 [==============================] - 17s 5ms/step - loss: 0.2755 - acc: 0.9164\n",
            "Epoch 4/15\n",
            "3791/3791 [==============================] - 17s 5ms/step - loss: 0.2589 - acc: 0.9246\n",
            "Epoch 5/15\n",
            "3791/3791 [==============================] - 17s 5ms/step - loss: 0.2370 - acc: 0.9309\n",
            "Epoch 6/15\n",
            "3791/3791 [==============================] - 17s 5ms/step - loss: 0.2738 - acc: 0.9172\n",
            "Epoch 7/15\n",
            "3791/3791 [==============================] - 18s 5ms/step - loss: 0.4543 - acc: 0.7940\n",
            "Epoch 8/15\n",
            "3791/3791 [==============================] - 17s 5ms/step - loss: 0.3460 - acc: 0.9011\n",
            "Epoch 9/15\n",
            "3791/3791 [==============================] - 17s 4ms/step - loss: 0.2810 - acc: 0.9119\n",
            "Epoch 10/15\n",
            "3791/3791 [==============================] - 17s 4ms/step - loss: 0.2371 - acc: 0.9153\n",
            "Epoch 11/15\n",
            "3791/3791 [==============================] - 16s 4ms/step - loss: 0.1909 - acc: 0.9264\n",
            "Epoch 12/15\n",
            "3791/3791 [==============================] - 17s 5ms/step - loss: 0.1512 - acc: 0.9459\n",
            "Epoch 13/15\n",
            "3791/3791 [==============================] - 18s 5ms/step - loss: 0.1142 - acc: 0.9607\n",
            "Epoch 14/15\n",
            "3791/3791 [==============================] - 18s 5ms/step - loss: 0.0963 - acc: 0.9689\n",
            "Epoch 15/15\n",
            "3791/3791 [==============================] - 17s 4ms/step - loss: 0.0747 - acc: 0.9757\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe1a7039438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MqFKB31Chw4"
      },
      "source": [
        "pred = model.predict(test_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT8COEwQv1xb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "a58ca713-6bdf-460e-a447-405e07d4a2bb"
      },
      "source": [
        "interim = pd.read_csv('dataset2_interim_eval.csv')\n",
        "interim = interim.drop('text', axis = 1)\n",
        "interim.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id\n",
              "0   0\n",
              "1   1\n",
              "2   2\n",
              "3   3\n",
              "4   4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3PaecwpwiEf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "e52d118d-0a60-4a7f-bc37-04566b5950ec"
      },
      "source": [
        "interim['label'] = pred\n",
        "interim.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.098074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.994175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.995501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.994286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.998858</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id     label\n",
              "0   0  0.098074\n",
              "1   1  0.994175\n",
              "2   2  0.995501\n",
              "3   3  0.994286\n",
              "4   4  0.998858"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d4njdc4w_uZ"
      },
      "source": [
        "interim.loc[interim.label > 0.5 , 'label'] = 1\n",
        "interim.loc[interim.label < 0.5 , 'label'] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOXMw0oQ01Mn"
      },
      "source": [
        "interim.to_csv('interim_results.csv',index=None)\n",
        "from google.colab import files\n",
        "files.download('interim_results.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUHlifpyYEeN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHD9itDB1pVr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzT0oGADYlde"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bgHA37dYlmN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejv6mPq6YlwH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayp3e8WVZbry"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Osmu0i1dZbzG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz7af2CKbjUW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXziFv0JZb4Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}