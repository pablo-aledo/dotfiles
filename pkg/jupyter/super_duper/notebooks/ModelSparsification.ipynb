{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "colab": {
      "name": "ModelSparsification.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqkDISRLHru5"
      },
      "source": [
        "## How to sparsify a Pytorch model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDgV_m0KHru-"
      },
      "source": [
        "from transformers import RobertaConfig\n",
        "from transformers import RobertaForMaskedLM\n",
        "from pytorch_block_sparse import BlockSparseModelPatcher\n",
        "import re\n",
        "\n",
        "config = RobertaConfig(\n",
        "    vocab_size=52_000,\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1,\n",
        ")\n",
        "\n",
        "model = RobertaForMaskedLM(config=config).cuda()\n",
        "\n",
        "# =>84 million parameters\n",
        "print(f\"Initial model parameters count={model.num_parameters()}\")\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skMo5pw2HrvA"
      },
      "source": [
        "# Create a model patcher\n",
        "mp = BlockSparseModelPatcher()\n",
        "\n",
        "# Show names that can be used: this returns a list of all names in the network that are patchable.\n",
        "# These names are escaped to be used as regexps in mp.add_pattern()\n",
        "patchables = mp.get_patchable_layers(model)\n",
        "\n",
        "dedup_layers = []\n",
        "\n",
        "# Pretty print the regexps: replace layer number with regexp matching numbers, and dedup them\n",
        "# This is a bit specific to Roberta, but should work for most transformers, it's just for ease of reading.\n",
        "for patchable in patchables:\n",
        "    r = patchable[\"regexp\"]\n",
        "    r = re.sub(r'[0-9]+', '[0-9]+', r)\n",
        "    if r not in dedup_layers:\n",
        "        dedup_layers.append(r)\n",
        "        layer = patchable['layer']\n",
        "        print(f\"{r}\\n   => {layer.in_features}x{layer.out_features}, bias={layer.bias is not None}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUUzJFosHrvB"
      },
      "source": [
        "\n",
        "\n",
        "# Selecting some layers to sparsify.\n",
        "# This is the \"artful\" part, as some parts are more prone to be sparsified, other may impact model precision too much.\n",
        "\n",
        "# Match layers using regexp (we escape the ., just because, it's more correct, but it does not change anything here)\n",
        "# the [0-9]+ match any layer number.\n",
        "# We setup a density of 0.5 on these layers, you can test other layers / densities .\n",
        "mp.add_pattern(\"roberta\\.encoder\\.layer\\.[0-9]+\\.intermediate\\.dense\", {\"density\":0.5})\n",
        "mp.add_pattern(\"roberta\\.encoder\\.layer\\.[0-9]+\\.output\\.dense\", {\"density\":0.5})\n",
        "mp.add_pattern(\"roberta\\.encoder\\.layer\\.[0-9]+\\.attention\\.output\\.dense\", {\"density\":0.5})\n",
        "mp.patch_model(model)\n",
        "\n",
        "print(f\"Final model parameters count={model.num_parameters()}\")\n",
        "\n",
        "# => 68 million parameters instead of 84 million parameters (embeddings are taking a lof space in Roberta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHSszZ8IHrvC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}