{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Toxic Comment Classification (Kaggle) - Simple Baseline.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_PTaklxgTzA"
      },
      "source": [
        "#Toxic Comment Classification (Kaggle) - Simple Baseline\n",
        "\n",
        "Simple baseline model for the [Kaggle Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/rules) that uses Glove embeddings and a bidirectional GRU with Tensorflow 2.0\n",
        "\n",
        "Likely ways to further improve the model:\n",
        "\n",
        "\n",
        "*   Use an embedding method that takes into account the context of the sentence e.g. BERT\n",
        "*   Use optimisation tricks like a circular learning rate schedule\n",
        "*   Try using an attention-based architecture\n",
        "*   Try different forms of pre-processing, e.g. removing stop words like \"the\"\n",
        "*   Increase the allowed max length of sequence\n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOaCpe6keHze",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "outputId": "3e2469e9-7d9b-451e-8869-a64d7b0f6021"
      },
      "source": [
        "!pip install tensorflow==2.0 -q\n",
        "!pip install tensorflow-gpu==2.0 -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 86.3MB 101kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 36.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 450kB 53.1MB/s \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtSMpWrdeDdX"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "SEED = 0\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOkCkR0PZNSo"
      },
      "source": [
        "#1. Download and Prepare the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrrP4AhpiWga"
      },
      "source": [
        "#Download the data\n",
        "#Replace these lines with paths to the data for the competition which can be found here: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data\n",
        "DRIVE_FOLDER = '/content/drive/My Drive/Toxic_Comment_Classification_Kaggle/'\n",
        "train = pd.read_csv(DRIVE_FOLDER + 'train.csv')\n",
        "test = pd.read_csv(DRIVE_FOLDER + 'test.csv')\n",
        "print(\"Loaded training data, shape: \", train.shape)\n",
        "print(\"Loaded test data, shape: \", test.shape)\n",
        "print(train.head().to_string())\n",
        "\n",
        "#Check for null values\n",
        "print(train.isnull().any())\n",
        "print(test.isnull().any())\n",
        "\n",
        "#Split into features and dependent variables \n",
        "train_X = train[\"comment_text\"].values\n",
        "test_X = test[\"comment_text\"].values\n",
        "train_y = train.iloc[:, 2:].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRuIVaQri8N9"
      },
      "source": [
        "from keras.preprocessing import text, sequence\n",
        "max_num_words = 30000\n",
        "tokenizer = text.Tokenizer(num_words=max_num_words)\n",
        "tokenizer.fit_on_texts(list(train_X) + list(test_X) )\n",
        "\n",
        "train_X = tokenizer.texts_to_sequences(train_X)\n",
        "test_X = tokenizer.texts_to_sequences(test_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSrPGGpdfdx9"
      },
      "source": [
        "Now we explore the distribution of sentence lengths so we can understand what max sentence length to set and therefore how much padding to do"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lkfdm27KfcVX"
      },
      "source": [
        "import seaborn as sns\n",
        "sentence_lengths = [len(sentence) for sentence in train_X]\n",
        "sns.distplot(sentence_lengths);\n",
        "\n",
        "max_length = 400\n",
        "train_X = sequence.pad_sequences(train_X, maxlen=max_length)\n",
        "test_X = sequence.pad_sequences(test_X, maxlen=max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM9emExchn2Q"
      },
      "source": [
        "Now we load a dictionary containing Glove embeddings of the form: {word: embedding}.  Then we create an embedding matrix that maps the words in our vocabulary to their embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XIpU0FBfrkt"
      },
      "source": [
        "#Replace this line with a path to the glove embeddings file which you can download here: https://www.kaggle.com/watts2/glove6b50dtxt\n",
        "EMBEDDING_FILE = DRIVE_FOLDER + 'glove.6B.50d.txt'   \n",
        "\n",
        "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
        "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "max_number_words = 30000\n",
        "embedding_dimension = 50\n",
        "number_words = min(max_number_words, len(word_index))\n",
        "embedding_matrix = np.zeros((number_words, embedding_dimension))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_number_words: continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjBOLsIdZSe0"
      },
      "source": [
        "#2. Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1dlXquIiuQo"
      },
      "source": [
        "First we create our model object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa2De8Nlivx4"
      },
      "source": [
        "from tensorflow.keras import Model, activations\n",
        "from tensorflow.keras.layers import Dense, Concatenate, GRU, LSTM, SpatialDropout1D, \\\n",
        "Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, Embedding\n",
        "\n",
        "gru_hidden_size = 40\n",
        "dropout_rate = 0.1\n",
        "\n",
        "class gru_model(Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    Model.__init__(self)\n",
        "    self.gru = Bidirectional(GRU(units=gru_hidden_size, return_sequences=True))\n",
        "    #We use spatial dropout instead of dropout because the different dimensions of an embedding are likely to be highly correlated and so it is a more effective method of regularisation to drop whole embedding\n",
        "    #vectors at a time rather than only dropping parts of embedding vectors\n",
        "    self.spatial_dropout = SpatialDropout1D(dropout_rate) \n",
        "    self.global_avg_pooling = GlobalAveragePooling1D()\n",
        "    self.global_max_pooling = GlobalMaxPooling1D()\n",
        "    self.embedding = Embedding(max_number_words, embedding_dimension, input_length=max_length, weights=[embedding_matrix])\n",
        "    self.fc_layer = Dense(6, activation=\"sigmoid\")\n",
        "  \n",
        "  def call(self, x, training=True):\n",
        "    \"\"\"Forward pass for the network. Note that it expects input data in the form (batch, seq length, features)\"\"\"\n",
        "    x = self.embedding(x)\n",
        "    if training:\n",
        "      x = self.spatial_dropout(x)\n",
        "    x = self.gru(x)\n",
        "    avg_pool = self.global_avg_pooling(x)\n",
        "    max_pool = self.global_max_pooling(x)\n",
        "    x = Concatenate(axis=1)([avg_pool, max_pool])\n",
        "    x = self.fc_layer(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NOF_iP3m212"
      },
      "source": [
        "Then we create a ROC-AUC evaluation callback because it is this crtieria that the Kaggle competition is judged on"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sogwA_fgmzu0"
      },
      "source": [
        "from keras.callbacks import Callback\n",
        "from sklearn.metrics import roc_auc_score\n",
        "                                          \n",
        "class ROCAUCEvaluation(Callback):\n",
        "    def __init__(self, validation_data=(), interval=1):\n",
        "        super(Callback, self).__init__()\n",
        "\n",
        "        self.interval = interval\n",
        "        self.X_val, self.y_val = validation_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if epoch % self.interval == 0:\n",
        "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
        "            score = roc_auc_score(self.y_val, y_pred)\n",
        "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRUTmuhwZbL5"
      },
      "source": [
        "Then we compile and train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2PH-o282Z9E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "53ef1c92-bb95-4345-e0f9-4e590926ea59"
      },
      "source": [
        "batch_size = 32\n",
        "epochs = 1\n",
        "model = gru_model()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])                            \n",
        "tr_X, val_X, tr_y, val_y = train_test_split(train_X, train_y, train_size=0.95, random_state=SEED)\n",
        "rocauc = ROCAUCEvaluation(validation_data=(val_X, val_y), interval=1)\n",
        "hist = model.fit(tr_X, tr_y, batch_size=batch_size, epochs=epochs, validation_data=(val_X, val_y), callbacks=[rocauc]) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 151592 samples, validate on 7979 samples\n",
            "151584/151592 [============================>.] - ETA: 0s - loss: 0.0334 - accuracy: 0.9869\n",
            " ROC-AUC - epoch: 1 - score: 0.987150 \n",
            "\n",
            "151592/151592 [==============================] - 668s 4ms/sample - loss: 0.0334 - accuracy: 0.9869 - val_loss: 0.0410 - val_accuracy: 0.9848\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN6vuziL_ksN"
      },
      "source": [
        "**Our validation set ROC-AUC is 0.987. A similar score on the test set would put us in the top 1% of Kaggle submissions as of 9/10/19**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1Y-Q8V4rTYP"
      },
      "source": [
        "#How to create a submission csv file for Kaggle\n",
        "submission = pd.read_csv(DRIVE_FOLDER + \"sample_submission.csv\")\n",
        "y_pred = model.predict(test_X, batch_size=1024)\n",
        "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}