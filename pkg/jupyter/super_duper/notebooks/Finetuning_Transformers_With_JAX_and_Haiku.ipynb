{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Finetuning Transformers With JAX and Haiku",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a1dfb4e2c66045fe9ca03c2e286c6c7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1eff62a3d03640c88cb343d5d4f44b08",
              "IPY_MODEL_2a2c5330b4b0493baec3b7e6cb4e06f5"
            ],
            "layout": "IPY_MODEL_de51a3139a3a439aab4ff1982ae66420"
          }
        },
        "de51a3139a3a439aab4ff1982ae66420": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1eff62a3d03640c88cb343d5d4f44b08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Dl Completed...: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9789dce5bc9c4d06bc53e4cf0da589c5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05741a1dae274b258403af0cd4d4d7b7",
            "value": 1
          }
        },
        "2a2c5330b4b0493baec3b7e6cb4e06f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e989f52105aa4f1b991f6dd40f53c832",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_625f6d3e02c4447aa60a552868b2fc5a",
            "value": " 1/1 [00:02&lt;00:00,  2.89s/ url]"
          }
        },
        "05741a1dae274b258403af0cd4d4d7b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "9789dce5bc9c4d06bc53e4cf0da589c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "625f6d3e02c4447aa60a552868b2fc5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e989f52105aa4f1b991f6dd40f53c832": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73b2e459fa70491faf94b9e20356dbd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3c8174ca9134202a8d0ae03c720c221",
              "IPY_MODEL_1041d4f787ca4151a9605bf94d62f172"
            ],
            "layout": "IPY_MODEL_648a9eb195e64b28b796945c5028d07b"
          }
        },
        "648a9eb195e64b28b796945c5028d07b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3c8174ca9134202a8d0ae03c720c221": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Dl Size...: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ef5c3ed70b7444d8d290ffd8bfa9756",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e3cd8a193de842a0bf38935a1ce5e59c",
            "value": 1
          }
        },
        "1041d4f787ca4151a9605bf94d62f172": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce15e79abf4f416a89bb82a15448702e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d53077358f6546f9a81a9afdedfa30bf",
            "value": " 80/80 [00:02&lt;00:00, 27.92 MiB/s]"
          }
        },
        "e3cd8a193de842a0bf38935a1ce5e59c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "4ef5c3ed70b7444d8d290ffd8bfa9756": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d53077358f6546f9a81a9afdedfa30bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce15e79abf4f416a89bb82a15448702e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81915e2c10164fa089b22e12ebe6c3c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_365ef5427fbe470f8bc5053ff606b9bd",
              "IPY_MODEL_4e46d4f4a2a84987830aaf6149c6cdf3"
            ],
            "layout": "IPY_MODEL_17504e201c1b44dda3a216d3d0cc1bd0"
          }
        },
        "17504e201c1b44dda3a216d3d0cc1bd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "365ef5427fbe470f8bc5053ff606b9bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b918d1bf2b034e37a324cd413859bb19",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_13244c77718e4af8981c3da023f589cc",
            "value": 1
          }
        },
        "4e46d4f4a2a84987830aaf6149c6cdf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c25352db456e4ea8ba4adafd7d972891",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ce34e994a68b488ca0db6f3bb9705ffe",
            "value": " 25000/0 [00:14&lt;00:00, 2984.21 examples/s]"
          }
        },
        "13244c77718e4af8981c3da023f589cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "b918d1bf2b034e37a324cd413859bb19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce34e994a68b488ca0db6f3bb9705ffe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c25352db456e4ea8ba4adafd7d972891": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0fbd28140c84cec92ab52a2ba569244": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d34a4eb5c9cb41858fdfb764b76cddf8",
              "IPY_MODEL_900f3def1d954c9abbf14847a5662b6f"
            ],
            "layout": "IPY_MODEL_70700350bee940a5ac1fbbde921eec5d"
          }
        },
        "70700350bee940a5ac1fbbde921eec5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d34a4eb5c9cb41858fdfb764b76cddf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": " 63%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8de773c70de74e8c87740083379e760f",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6eefbeeeb02d47dbaeaab0bbfbece380",
            "value": 15855
          }
        },
        "900f3def1d954c9abbf14847a5662b6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77f0e167e42445e4930361ec2329d841",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fa9b5a99bb5b408d94e174a358a7215d",
            "value": " 15855/25000 [00:00&lt;00:00, 158549.40 examples/s]"
          }
        },
        "6eefbeeeb02d47dbaeaab0bbfbece380": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "8de773c70de74e8c87740083379e760f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa9b5a99bb5b408d94e174a358a7215d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77f0e167e42445e4930361ec2329d841": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a15821d41b434e9eb2aead09df23a2ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_28999991a2f24f76938484c2d893a995",
              "IPY_MODEL_78f11223e69f437a9654a5df4fe94cd1"
            ],
            "layout": "IPY_MODEL_4be8a0cabdb2451290ecb23217113642"
          }
        },
        "4be8a0cabdb2451290ecb23217113642": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28999991a2f24f76938484c2d893a995": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da5d466cc7bc41d095db288c14e2eeaa",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f277e8b686b84d7cab3f47ef1e88ebcb",
            "value": 1
          }
        },
        "78f11223e69f437a9654a5df4fe94cd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d933fd7e28344448bd17c3404e83533",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_eb3b9625ea754ade9b57a2a0ee1e4f6a",
            "value": " 25000/0 [00:14&lt;00:00, 2972.10 examples/s]"
          }
        },
        "f277e8b686b84d7cab3f47ef1e88ebcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "da5d466cc7bc41d095db288c14e2eeaa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb3b9625ea754ade9b57a2a0ee1e4f6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d933fd7e28344448bd17c3404e83533": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ff0e3e23e374acba5a91730b550f0fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f5710bd9075c4ad99af11e4ee60f6bb7",
              "IPY_MODEL_823aa6fe99de4d70a6ad21f5c98ce939"
            ],
            "layout": "IPY_MODEL_8a916437595f47cfadce738fa30c79c9"
          }
        },
        "8a916437595f47cfadce738fa30c79c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5710bd9075c4ad99af11e4ee60f6bb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": " 62%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b3aedd820c548b0880d143a9426883b",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3bfdb19c65244ded9c2b7830fdc87b9f",
            "value": 15414
          }
        },
        "823aa6fe99de4d70a6ad21f5c98ce939": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e8f9af6995146ef84c499c62f5dc79e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9279278c52974d959b70e4a6484f729f",
            "value": " 15414/25000 [00:00&lt;00:00, 154137.57 examples/s]"
          }
        },
        "3bfdb19c65244ded9c2b7830fdc87b9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "0b3aedd820c548b0880d143a9426883b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9279278c52974d959b70e4a6484f729f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e8f9af6995146ef84c499c62f5dc79e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea7e26935e1a4167993b3deabbdc93a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf8fd96442024255943393d50ff4cd5a",
              "IPY_MODEL_e4b3487b04d140b783c59b502ce869df"
            ],
            "layout": "IPY_MODEL_66845f8544c347a4aa4345ebe80e6f8b"
          }
        },
        "66845f8544c347a4aa4345ebe80e6f8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf8fd96442024255943393d50ff4cd5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_819d17e082a64683ba84f8df091ae9fe",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6a6eb763583849a6aecf4b766b66a48a",
            "value": 1
          }
        },
        "e4b3487b04d140b783c59b502ce869df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b46b146f7ed410885e2a44cff25a878",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_cea390c704464466b3db39486c75c0a7",
            "value": " 50000/0 [00:20&lt;00:00, 3014.71 examples/s]"
          }
        },
        "6a6eb763583849a6aecf4b766b66a48a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "819d17e082a64683ba84f8df091ae9fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cea390c704464466b3db39486c75c0a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b46b146f7ed410885e2a44cff25a878": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "268033ab11ec4d2ab94a42680912e585": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a7158a080973484a98d8b58e307f5da6",
              "IPY_MODEL_9d94fbcf8a044070be64b18ec3baac4c"
            ],
            "layout": "IPY_MODEL_060c0360b5954c058c857a6b9840eafd"
          }
        },
        "060c0360b5954c058c857a6b9840eafd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7158a080973484a98d8b58e307f5da6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": " 67%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77ff9aae35f145628e09f3dde794481f",
            "max": 50000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c231b5a87d55401e9427e72943cc4db1",
            "value": 33266
          }
        },
        "9d94fbcf8a044070be64b18ec3baac4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c8055a5280a41e8ad3ec085ced8a856",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1e291ddfae9640e5bd81c1ad1c5b7843",
            "value": " 33266/50000 [00:00&lt;00:00, 106813.71 examples/s]"
          }
        },
        "c231b5a87d55401e9427e72943cc4db1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "77ff9aae35f145628e09f3dde794481f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e291ddfae9640e5bd81c1ad1c5b7843": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c8055a5280a41e8ad3ec085ced8a856": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2XMaLX1mNhk"
      },
      "source": [
        "Just last month [DeepMind open-sourced Haiku](https://github.com/deepmind/dm-haiku), the JAX version of their tensorflow [neural network library Sonnet](https://github.com/deepmind/sonnet).  Today we'll be walking through a port of the RoBERTa pre-trained model to JAX + Haiku, then finetuning the model to solve a downstream task! \n",
        "\n",
        "If you're unfamiliar with JAX, need a primer on JAX fundamentals, or simply want to know why you should care, check out my previous post -- [A First Look at JAX](https://www.pragmatic.ml/first-look-at-jax/) or wander on over to the [official JAX documentation](https://jax.readthedocs.io/en/latest/).\n",
        "\n",
        "Some brief notes about the format of this blog post:\n",
        "    - This post will be code-oriented and show code examples first before providing commentary. \n",
        "    - We're going to be working in a top-down fashion, so we'll lay out broad strokes and then fill in the detail.\n",
        "    - Along the way I'll add notes on potential JAX / Haiku stumbling blocks and rough edges that you may find useful if you're just getting started with either.\n",
        "\n",
        "If you'd like to run the code as you read along I've also made this walkthrough available as a Colab notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7BDW9r5uQC5"
      },
      "source": [
        "Without further ado, let's explore Haiku!\n",
        "\n",
        "The Haiku README provides an excellent short description of Haiku's role in the JAX ecosystem:\n",
        "\n",
        "> * Haiku is a simple neural network library for JAX that enables users to use familiar object-oriented programming \n",
        "models while allowing full access to JAX's pure function transformations.\n",
        "> * Haiku provides two core tools: a module abstraction, hk.Module, and a simple function transformation, hk.transform.\n",
        "> * hk.Modules are Python objects that hold references to their own parameters, other modules, and methods that apply functions on user inputs.\n",
        "> * hk.transform turns functions that use these object-oriented, functionally \"impure\" modules into pure functions that can be used with jax.jit, jax.grad, jax.pmap, etc.\n",
        "\n",
        "In addition to those two key abstractions, Haiku provides a collection of common neural network components like convolutions, pooling operations, RNN / GRU / LSTM units, batch norm and other normalization methods, padding operations, and a suite of a initializers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYbj4qH_uRYe"
      },
      "source": [
        "!pip install git+https://github.com/deepmind/dm-haiku transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_d-yf46lTeo"
      },
      "source": [
        "import haiku as hk\n",
        "\n",
        "class Transformer(hk.Module):\n",
        "\n",
        "    def __init__(self, config, *args, **kwargs):\n",
        "        super().__init__(name=\"Transformer\")\n",
        "        self.config = config\n",
        "    \n",
        "    def __call__(self, token_ids):\n",
        "        x = Embedding(config)(token_ids)\n",
        "        for layer_num, layer in enumerate(range(config.n_layers)):\n",
        "            x = TransformerBlock(config, layer_num=layer_num)(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LTWi6cDtJmb"
      },
      "source": [
        "First, a comment on how Haiku modules are typically structured.  The `__init__` function is where we manage module configuration -- storing values like the number of layers in our transformer, our hidden dimension size, and other parameters. In order for Haiku to function properly, the `__init__()` of all `hk.Modules` must call `super().__init__()`, optionally with a `name`. \n",
        "\n",
        "Note that if we wanted to we could have moved some of this logic around, constructing our TokenEmbedding object in the `__init__` method for instance. Although nothing prevents us from doing this, there are times when we don't have enough information to construct all our modules in the `__init__` method because our configuration is dependent on some property of our input.  Because of this, child `hk.Modules` are often constructed in the same method that is called when the module is applied to an input. It's also simply convenient to have the full declaration of module settings in line with the application of the module to an input, to prevent hopping back and forth between methods when reading code.\n",
        "\n",
        "In our example above, we use the `__call__` method as our application method for brevity but we could equivalently use `forward` or any other method name. An `hk.Module` can also expose more than one application method if desirable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDl06c3bVDcn"
      },
      "source": [
        "import haiku as hk\n",
        "\n",
        "class AlternateTransformer(hk.Module):\n",
        "    \"\"\"\n",
        "    An equally valid implementation\n",
        "    \"\"\"\n",
        "    def __init__(self, config, *args, **kwargs):\n",
        "        super().__init__(name=\"Transformer\")\n",
        "        self.config = config\n",
        "        self.embedder = Embedding(config)\n",
        "    \n",
        "    def embed(self, tokens):\n",
        "        return self.embedder(tokens)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        x = self.embedder(tokens)\n",
        "        for layer in config.n_layers:\n",
        "            x = TransformerBlock(config)(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SYsJBfqVJdH"
      },
      "source": [
        "If you're interested in more detail on how `hk.Module` and `hk.transform` function, I'd recommend Sabrina Mielke's recent article, [\"From PyTorch to JAX: towards neural net frameworks that purify stateful code\"](https://sjmielke.com/jax-purify.htm), which builds up the motivation for the design decisions that Haiku made.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hre0n9grY50o"
      },
      "source": [
        "If we had already implemented the `Embedding` and `TransformerBlock` modules, we could convert our new `hk.Module` to a JAX compatible function through the use of `hk.transform`. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPEc5qq3ZUJN"
      },
      "source": [
        "# We'll fill our our config later\n",
        "config = {'max_length': 512}\n",
        "\n",
        "def features(tokens):\n",
        "    transformer = Transformer(config)\n",
        "    return transformer(tokens)\n",
        "\n",
        "features_fn = hk.transform(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSvAcLL8azAk"
      },
      "source": [
        "Neat!  Let's write our `Embedding` module and plug in some pre-trained model weights from HuggingFace so we can start executing code. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJP4q2debYHX"
      },
      "source": [
        "from transformers import RobertaModel\n",
        "import jax.numpy as jnp\n",
        "\n",
        "\n",
        "class Embedding(hk.Module):\n",
        "    \"\"\"\n",
        "    Embeds tokens and positions into an array of shape [n_batch, n_seq, n_hidden]\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "    def __call__(self, token_ids, training=False):\n",
        "        \"\"\"\n",
        "        token_ids: ints of shape (batch, n_seq)\n",
        "        \"\"\"\n",
        "        word_embeddings = self.config['pretrained']['embeddings/word_embeddings']\n",
        "        \n",
        "        # We have to flatten our tokens before passing them to the hk.Embed module,\n",
        "        # as arrays with more than one dimension are interpreted as multi-dimensional indexes\n",
        "        flat_token_ids = jnp.reshape(token_ids, [token_ids.shape[0] * token_ids.shape[1]])\n",
        "        flat_token_embeddings = hk.Embed(\n",
        "            vocab_size=word_embeddings.shape[0],\n",
        "            embed_dim=word_embeddings.shape[1],\n",
        "            \n",
        "            # Here we're using hk.initializers.Constant to supply pre-trained embeddings\n",
        "            # to our hk.Embed module\n",
        "            w_init=hk.initializers.Constant(self.config['pretrained']['embeddings/word_embeddings'])\n",
        "        )(flat_token_ids)\n",
        "        \n",
        "        # After we've embedded our token IDs, we reshape to recover our batch dimension\n",
        "        token_embeddings = jnp.reshape(\n",
        "            flat_token_embeddings, \n",
        "            [token_ids.shape[0], token_ids.shape[1], word_embeddings.shape[1]]\n",
        "        )\n",
        "        \n",
        "        # Combine our token embeddings with a set of learned positional embeddings\n",
        "        embeddings = token_embeddings + PositionEmbeddings(self.config)()\n",
        "        embeddings = hk.LayerNorm(\n",
        "            axis=-1, \n",
        "            create_scale=True,\n",
        "            create_offset=True,\n",
        "            \n",
        "            # The layer norm parameters are also pretrained, so we have to take care to \n",
        "            # use a constant initializer for these as well\n",
        "            scale_init=hk.initializers.Constant(\n",
        "                self.config['pretrained']['embeddings/LayerNorm/gamma']\n",
        "            ),\n",
        "            offset_init=hk.initializers.Constant(\n",
        "                self.config['pretrained']['embeddings/LayerNorm/beta']\n",
        "            )\n",
        "        )(embeddings)\n",
        "        \n",
        "        # Dropout is will be applied later when we finetune our Roberta implementation \n",
        "        # to solve a classification task. For now we'll set `training` to False.\n",
        "        if training:\n",
        "            embeddings = hk.dropout(\n",
        "                # Haiku magic -- we'll explicitly provide a RNG key to haiku later to make this function\n",
        "                hk.next_rng_key(), \n",
        "                rate=self.config['embed_dropout_rate'], \n",
        "                x=embeddings\n",
        "            )\n",
        "        \n",
        "        return embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2Rm8LL3cE2d"
      },
      "source": [
        "Our Embedding module is pretty straightforward:\n",
        "\n",
        "*   Using `hk.Embed`, we perform a lookup to get the vector that corresponds to each of our token IDs.\n",
        "    * We initialize this to the pre-trained embedding matrix we downloaded.\n",
        "*   We add `PositionEmbeddings()` -- a `hk.Module` we have yet to define.  Assuming a fixed sequence length, this is nothing more than a static matrix that we broadcast to each sequence in our batch.\n",
        "*   We normalize our embeddings using layer norm, applying the scales and offsets learned during pre-training.\n",
        "*   Finally, we optionally apply dropout to our embeddings at train time.\n",
        "    * Note the hk.next_rng_key() feature. With vanilla JAX, we would have to make sure to pass our `PRNGKey` around to every module that needs it.  A handy feature of Haiku is that your `PRNGKey` is exposed via the `hk.next_rng_key` utility with the context of `hk.transform`. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuXwMMNjcEP_"
      },
      "source": [
        "\n",
        "class PositionEmbeddings(hk.Module):\n",
        "    \"\"\"\n",
        "    A position embedding of shape [n_seq, n_hidden]\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        # The Roberta position embeddings are offset by 2\n",
        "        self.offset = 2\n",
        "\n",
        "    def __call__(self):\n",
        "        pretrained_position_embedding = self.config['pretrained']['embeddings/position_embeddings']\n",
        "        position_weights = hk.get_parameter(\n",
        "            \"position_embeddings\", \n",
        "            pretrained_position_embedding.shape,\n",
        "            init=hk.initializers.Constant(pretrained_position_embedding)\n",
        "        )\n",
        "        \n",
        "        return position_weights[self.offset:self.offset + self.config['max_length']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w83Tfx2efQl"
      },
      "source": [
        "The `PositionEmbeddings` module is also pretty straightforward -- we simply slice the pre-trained position embedding matrix to the desired length and return it. However, our `PositionEmbeddings` has introduced a new key haiku function! \n",
        "\n",
        "```\n",
        "hk.get_parameter(name, shape, dtype, init)\n",
        "```\n",
        "\n",
        "The `hk.get_parameter` function is how Haiku keeps track of parameter state for us.  The module we're in and the `name` argument passed to `hk.get_parameter` register this new parameter in a Haiku-managed parameter store. If we were writing something similar with vanilla JAX, we would have to keep track of these parameters manually. Later on we'll see how `hk.transform` allows us to retrieve the values of our parameters using `init()`. \n",
        "\n",
        "[Extended docs for `hk.get_parameter`](https://dm-haiku.readthedocs.io/en/latest/api.html#haiku.get_parameter) are available in the [official Haiku documentation](https://dm-haiku.readthedocs.io/en/latest/).\n",
        "\n",
        "Now that you're familiar with `hk.get_parameter`, let's load in some pre-trained model weights so we can test out what we've put together so far."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4hScrweZlnM"
      },
      "source": [
        "from io import BytesIO\n",
        "from functools import lru_cache\n",
        "\n",
        "import joblib\n",
        "import requests\n",
        "\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "\n",
        "# We'll make use of these again later as a means to check our implementation\n",
        "huggingface_roberta = RobertaModel.from_pretrained('roberta-base', output_hidden_states=True)\n",
        "huggingface_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# Some light postprocessing to make the parameter keys a bit more concise\n",
        "def postprocess_key(key):\n",
        "    return key.replace('model/featurizer/bert/', '').replace(':0', '').replace('self/', '')\n",
        "\n",
        "\n",
        "# Cache the downloaded file to go easy on the tubes \n",
        "@lru_cache()\n",
        "def get_pretrained_weights():\n",
        "    # We'll use the weight dictionary from the Roberta encoder at https://github.com/IndicoDataSolutions/finetune\n",
        "    remote_url = \"https://bendropbox.s3.amazonaws.com/roberta/roberta-model-sm-v2.jl\"\n",
        "    weights = joblib.load(BytesIO(requests.get(remote_url).content))\n",
        "\n",
        "    weights = {\n",
        "        postprocess_key(key): value\n",
        "        for key, value in weights.items()\n",
        "    }\n",
        "    \n",
        "    # We use huggingface's word embedding matrix because their token IDs mapping varies slightly from the \n",
        "    # format used in our joblib file above\n",
        "    weights['embeddings/word_embeddings'] = (\n",
        "        huggingface_roberta.get_input_embeddings().weight.detach().numpy()\n",
        "    )\n",
        "    return weights\n",
        "\n",
        "\n",
        "class Scope(object):\n",
        "    \"\"\"\n",
        "    A tiny utility to help make looking up into our dictionary cleaner.\n",
        "    There's no haiku magic here.\n",
        "    \"\"\"\n",
        "    def __init__(self, weights, prefix):\n",
        "        self.weights = weights\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        return self.weights[self.prefix + key]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPr8IW9Lkf1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "51170d10-aa46-4775-efae-311b14fe4722"
      },
      "source": [
        "pretrained = get_pretrained_weights()\n",
        "print([k for k in pretrained.keys() if 'embedding' in k])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['embeddings/word_embeddings', 'embeddings/LayerNorm/beta', 'embeddings/LayerNorm/gamma', 'embeddings/token_type_embeddings', 'embeddings/position_embeddings']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HedW0FSl56u"
      },
      "source": [
        "Looks like we have the weights we need for our `Embedding` module, but we're still missing a way to go from text to numeric token IDs in our word embedding matrix.  Let's load in the pre-written tokenizer from huggingface to save ourselves some pain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfdSTXulmQHR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1373027a-8b23-40f8-8ab8-4d772030e158"
      },
      "source": [
        "sample_text = \"This was a lot less painful than re-implementing a tokenizer\"\n",
        "encoded = huggingface_tokenizer.batch_encode_plus(\n",
        "    [sample_text, sample_text],\n",
        "    pad_to_max_length=True,\n",
        "    max_length=config['max_length']\n",
        ")\n",
        "sample_tokens = encoded['input_ids']\n",
        "print(sample_tokens[0][:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 152, 21, 10, 319, 540, 8661, 87, 769, 12, 757, 40224, 154, 10, 19233, 6315, 2, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLbi5eXLnsUM"
      },
      "source": [
        "Looks good!  We've passed the `pad_to_max_length` and `max_length` arguments so that the `huggingface_tokenizer` can handle padding out sequences to a constant length for us -- and it's clearly working as the sea of 1's above show us. Token ID 1 corresponds to the padding token used by RoBERTa.\n",
        "\n",
        "Now we have all the necessary ingredients to test out our embedding layer, let's `hk.transform` the embedding operation and take things for a spin."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5XQBdLfn_r2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ed3db3c5-00e4-498b-bea0-b4cec15d16d5"
      },
      "source": [
        "from jax import jit\n",
        "from jax.random import PRNGKey\n",
        "import numpy as np\n",
        "\n",
        "config = {\n",
        "    'pretrained': pretrained,\n",
        "    'max_length': 512,\n",
        "    'embed_dropout_rate': 0.1\n",
        "}\n",
        "\n",
        "def embed_fn(tokens, training=False):\n",
        "    embedding = Embedding(config)(tokens)\n",
        "    return embedding\n",
        "\n",
        "rng = PRNGKey(42)\n",
        "embed = hk.transform(embed_fn, apply_rng=True)\n",
        "sample_tokens = np.asarray(sample_tokens)\n",
        "params = embed.init(rng, sample_tokens, training=False)\n",
        "embedded_tokens = jit(embed.apply)(params, rng, sample_tokens)\n",
        "print(embedded_tokens.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 512, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNkJ5Ruqy3f3"
      },
      "source": [
        "\n",
        "With that, we've successfully executed our first snippet of code using Haiku!\n",
        "\n",
        "First we wrote `embed_fn` to instantiate an instance of our `Embedding` module and call it.  This is necessary to wrap up all the requisite state for the embedding function for haiku.\n",
        "\n",
        "If you accidentally try to instantiate a `hk.Module` outside of a `hk.transform` context you'll receive a helpful reminder that this isn't permitted. Haiku imposes this constraint so it can \"purify\" our stateful `hk.Module` and convert it into a pure function that functions with JAX.\n",
        "\n",
        "```\n",
        "embedding = Embedding(config)\n",
        "```\n",
        "\n",
        "```\n",
        "ValueError: All `hk.Module`s must be initialized inside an `hk.transform`.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljzB0vhx3L-R"
      },
      "source": [
        "You've probably also noticed that we're using two methods we haven't yet spoken about -- `init()` and `apply()`.  \n",
        "\n",
        "```\n",
        "params = embed.init(rng, sample_tokens, training=False)\n",
        "```\n",
        "The `init` method is a haiku utility that gathers up all the parameters of our custom haiku modules for us and consolidates them into a `frozendict`. The `init()` method is also responsible for initializing any unitialized parameters, which is why we pass a source of randomness (our `rng`) as the first argument.\n",
        "\n",
        "Let's inspect our `params` variable:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vytGcmMS3q9v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f4f6ac02-5f76-4d5a-8a66-06dc12f3f303"
      },
      "source": [
        "print({key: type(value) for key, value in params.items()})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'embedding/embed': haiku._src.data_structures.frozendict, 'embedding/layer_norm': haiku._src.data_structures.frozendict, 'embedding/position_embeddings': haiku._src.data_structures.frozendict}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLGZ-4Zb3_Zh"
      },
      "source": [
        "When we use a `hk.Module` within another `hk.Module`, it gets placed in a subdictionary.  But if we drill down into our frozendict we'll eventually hit the bottom and uncover an `np.ndarray` -- the current state of the weights of our model.  If you're familiar with tensorflow's concept of a variable scope, this should feel familiar. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsyBSKQx4hwZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9481a121-9a26-46e9-8a1b-116f37e1a233"
      },
      "source": [
        "print({key: type(value) for key, value in params['embedding/layer_norm'].items()})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'offset': <class 'numpy.ndarray'>, 'scale': <class 'numpy.ndarray'>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNyITBR24kkm"
      },
      "source": [
        "It's worth noting, however, that we need to use haiku's `hk.get_parameter` construct (or a `hk.Module`) for the parameters to be tracked automatically.  If we try to use a simple `jnp.ndarray` within the context of `hk.transform` it won't be tracked as a trainable parameter. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0judoXS443VS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e965ce79-53e2-4425-ed72-eb16e27957b4"
      },
      "source": [
        "import numpy as np\n",
        "import jax\n",
        "\n",
        "def linear_fn(x):\n",
        "    w = jax.random.normal(hk.next_rng_key(), (10, 10))\n",
        "    b = jnp.zeros(10)\n",
        "    return np.dot(w, x) + b\n",
        "\n",
        "linear = hk.transform(linear_fn)\n",
        "params = linear.init(PRNGKey(0), np.random.rand(10))\n",
        "print(params.keys())\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KeysOnlyKeysView([])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ec4Z-UcJJKu"
      },
      "source": [
        "The second new method is `.apply()`:\n",
        "\n",
        "```\n",
        "embedded_tokens = embed.apply(params, rng, sample_tokens)\n",
        "```\n",
        "\n",
        "The `apply()` method injects the parameters of our model (and if we've passed `apply_rng` to `hk.transform`, our pseudo-random number generator) to the embed function so that we have all the necessary state to compute the output of our function.  This pattern is how `hk.transform` is able to turn a stateful `hk.Module` class into a JAX compatible operation -- `init()` and `apply()` are natural counterparts. Our `init()` method extracts the problematic state from our module, and the second passes that state back into the pure `apply()` method.  Because `apply()` is functionally pure, we're totally free to compose it with any of the standard JAX operations like `jit()` and `grad()`!\n",
        "\n",
        "```\n",
        "embedded_tokens = jit(embed.apply)(params, rng, sample_tokens)\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8WC81XVMqoA"
      },
      "source": [
        "In all honestly, that's about all there is to working with Haiku!  That's part of the beauty of it -- Haiku aims to be a library, not a framework -- providing a suite of utilities that complement JAX but doing it's best not to get in your way by requiring custom formats or imposing problematic constraints.\n",
        "\n",
        "With the critical pieces of the haiku API behind us, let's continue implementing our transformer!  Next up -- the `TransformerBlock` module.\n",
        "\n",
        "![alt text](https://i.imgur.com/UrXOmY7.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXOt56z-Rowl"
      },
      "source": [
        "class TransformerBlock(hk.Module):\n",
        "\n",
        "    def __init__(self, config, layer_num):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.n = layer_num\n",
        "\n",
        "    def __call__(self, x, mask, training=False):\n",
        "        scope = Scope(self.config['pretrained'], f'encoder/layer_{self.n}/')\n",
        "        # Feed our input through a multi-head attention operation\n",
        "        attention_output = MultiHeadAttention(self.config, self.n)(x, mask, training=training)\n",
        "\n",
        "        # Add a residual connection with the input to the layer\n",
        "        residual = attention_output + x\n",
        "\n",
        "        # Apply layer norm to the combined output\n",
        "        attention_output = hk.LayerNorm(\n",
        "            axis=-1,\n",
        "            create_scale=True,\n",
        "            create_offset=True,\n",
        "            scale_init=hk.initializers.Constant(scope['attention/output/LayerNorm/gamma']),\n",
        "            offset_init=hk.initializers.Constant(scope['attention/output/LayerNorm/beta']),\n",
        "        )(residual)\n",
        "\n",
        "        # Project out to a larger dim, apply a gelu, and then project back down to our hidden dim\n",
        "        mlp_output = TransformerMLP(self.config, self.n)(attention_output, training=training)\n",
        "\n",
        "        # Residual connection to the output of the attention operation\n",
        "        output_residual = mlp_output + attention_output\n",
        "\n",
        "        # Apply another LayerNorm\n",
        "        layer_output = hk.LayerNorm(\n",
        "            axis=-1,\n",
        "            create_scale=True,\n",
        "            create_offset=True,\n",
        "            scale_init=hk.initializers.Constant(scope['output/LayerNorm/gamma']),\n",
        "            offset_init=hk.initializers.Constant(scope['output/LayerNorm/beta']),\n",
        "        )(output_residual) \n",
        "        return layer_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmiwUBmH8sx3"
      },
      "source": [
        "We feed the inputs to each transformer block through it's signature self-attention layer, then add in residuals and apply layer normalization.\n",
        "We then feed the attention outputs through a 2 layer MLP, apply the residuals from the self-attention output, and apply a second layer normalization step.\n",
        "\n",
        "Let's define our MultiHeadAttention layer next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsusUYin4sQW"
      },
      "source": [
        "![Multi-Head Self-Attention](https://i.imgur.com/FvjWVvX.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzLCMjzOZX-W"
      },
      "source": [
        "class MultiHeadAttention(hk.Module):\n",
        "\n",
        "    def __init__(self, config, layer_num):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.n = layer_num\n",
        "\n",
        "    def _split_into_heads(self, x):\n",
        "        return jnp.reshape(\n",
        "            x, \n",
        "            [\n",
        "                x.shape[0],\n",
        "                x.shape[1],\n",
        "                self.config['n_heads'],\n",
        "                x.shape[2] // self.config['n_heads']\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def __call__(self, x, mask, training=False):\n",
        "        \"\"\"\n",
        "        x: tensor of shape (batch, seq, n_hidden)\n",
        "        mask: tensor of shape (batch, seq)\n",
        "        \"\"\"\n",
        "        scope = Scope(self.config['pretrained'], f'encoder/layer_{self.n}/attention/')\n",
        "        \n",
        "        # Project to queries, keys, and values\n",
        "        # Shapes are all [batch, sequence_length, hidden_size]\n",
        "        queries = hk.Linear(\n",
        "            output_size=self.config['hidden_size'],\n",
        "            w_init=hk.initializers.Constant(scope['query/kernel']),\n",
        "            b_init=hk.initializers.Constant(scope['query/bias'])\n",
        "        )(x)\n",
        "        keys = hk.Linear(\n",
        "            output_size=self.config['hidden_size'],\n",
        "            w_init=hk.initializers.Constant(scope['key/kernel']),\n",
        "            b_init=hk.initializers.Constant(scope['key/bias'])\n",
        "        )(x)\n",
        "        values = hk.Linear(\n",
        "            output_size=self.config['hidden_size'],\n",
        "            w_init=hk.initializers.Constant(scope['value/kernel']),\n",
        "            b_init=hk.initializers.Constant(scope['value/bias'])\n",
        "        )(x)\n",
        "        \n",
        "        # Reshape our hidden state to group into heads\n",
        "        # New shape are [batch, sequence_length, n_heads, size_per_head]\n",
        "        queries = self._split_into_heads(queries)\n",
        "        keys = self._split_into_heads(keys)\n",
        "        values = self._split_into_heads(values)\n",
        "        \n",
        "\n",
        "        # Compute per head attention weights \n",
        "        # b: batch\n",
        "        # s: source sequence\n",
        "        # t: target sequence\n",
        "        # n: number of heads\n",
        "        # h: per-head hidden state\n",
        "        \n",
        "        # Note -- we could also write this with jnp.reshape and jnp.matmul, but I'm becoming\n",
        "        # a fan of how concise opting to use einsum notation for this kind of operation is.\n",
        "        # For more info, see https://rockt.github.io/2018/04/30/einsum and \n",
        "        # https://obilaniu6266h16.wordpress.com/2016/02/04/einstein-summation-in-numpy/\n",
        "        attention_logits = jnp.einsum('bsnh,btnh->bnst', queries, keys) / np.sqrt(queries.shape[-1])\n",
        "        # Add logits of mask tokens with a large negative number to prevent attending to those terms.\n",
        "        attention_logits += jnp.reshape(mask * -2**32, [mask.shape[0], 1, 1, mask.shape[1]])\n",
        "        attention_weights = jax.nn.softmax(attention_logits, axis=-1)\n",
        "        per_head_attention_output = jnp.einsum('btnh,bnst->bsnh', values, attention_weights)\n",
        "        attention_output = jnp.reshape(\n",
        "            per_head_attention_output, \n",
        "            [\n",
        "                per_head_attention_output.shape[0],\n",
        "                per_head_attention_output.shape[1],\n",
        "                per_head_attention_output.shape[2] * per_head_attention_output.shape[3]\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Apply dense layer to output of attention operation\n",
        "        attention_output = hk.Linear(\n",
        "            output_size=self.config['hidden_size'],\n",
        "            w_init=hk.initializers.Constant(scope['output/dense/kernel']),\n",
        "            b_init=hk.initializers.Constant(scope['output/dense/bias'])\n",
        "        )(attention_output)\n",
        "\n",
        "        # Apply dropout at training time\n",
        "        if training:\n",
        "            attention_output = hk.dropout(\n",
        "                rng=hk.next_rng_key(),\n",
        "                rate=self.config['attention_drop_rate'],\n",
        "                x=attention_output\n",
        "            )\n",
        "\n",
        "        return attention_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldz2kmfp8sx7"
      },
      "source": [
        "\n",
        "We project our hidden state out to key, query, and value tensors of the same dimensions as the input hidden state. We then reshape the last dimension of our matrix to group neighboring activations into N heads. We dot our queries and keys to produce a measure of agreement that we'll use as an attention logit, divide by the square root of our head size as a way to prevent the attention distribution from being too sharp, and then apply our softmax to produce our attention weights.  Our then use our attention weights in conjunction with our values to produce our new hidden state, and reshape our matrices to re-combine the heads.  Finally, we apply a linear projection to our attention outputs and optionally apply dropout at training time.\n",
        "\n",
        "We won't spent too much time on the details here, as the intent of this blog post is to highlight using JAX and Haiku rather than devote too much time to an explanation of self-attention.\n",
        "\n",
        "If you'd like a more indepth refresher on the self-attention operation, I'd recommend:\n",
        "\n",
        "- [Alex Rush's Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html#attention)\n",
        "- [Jay Alammar's Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBXoQyZe_r-d"
      },
      "source": [
        "![alt text](https://i.imgur.com/4hDiOaa.pngps://)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAHS3EpcaHQ1"
      },
      "source": [
        "\n",
        "def gelu(x):\n",
        "    \"\"\"\n",
        "    We use this in place of jax.nn.relu because the approximation used \n",
        "    produces a non-trivial difference in the output state\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + jax.scipy.special.erf(x / jnp.sqrt(2.0)))\n",
        "\n",
        "\n",
        "class TransformerMLP(hk.Module):\n",
        "\n",
        "    def __init__(self, config, layer_num):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.n = layer_num\n",
        "\n",
        "    def __call__(self, x, training=False):\n",
        "        # Project out to higher dim\n",
        "        scope = Scope(self.config['pretrained'], f'encoder/layer_{self.n}/')\n",
        "        intermediate_output = hk.Linear(\n",
        "            output_size=self.config['intermediate_size'],\n",
        "            w_init=hk.initializers.Constant(scope['intermediate/dense/kernel']),\n",
        "            b_init=hk.initializers.Constant(scope['intermediate/dense/bias'])\n",
        "        )(x)\n",
        "\n",
        "        # Apply gelu nonlinearity\n",
        "        intermediate_output = gelu(intermediate_output)\n",
        "\n",
        "        # Project back down to hidden size\n",
        "        output = hk.Linear(\n",
        "            output_size=self.config['hidden_size'],\n",
        "            w_init=hk.initializers.Constant(scope['output/dense/kernel']),\n",
        "            b_init=hk.initializers.Constant(scope['output/dense/bias']),\n",
        "        )(intermediate_output)\n",
        "\n",
        "        # Apply dropout at training time\n",
        "        if training:\n",
        "            output = hk.dropout(\n",
        "                rng=hk.next_rng_key(), \n",
        "                rate=self.config['fully_connected_drop_rate'],\n",
        "                x=output\n",
        "            )\n",
        "\n",
        "        return output\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-8HJFjN8syA"
      },
      "source": [
        "The final component of our pre-trained RoBERTa model is the Transformer MLP block.  The linear MLP block contains a linear up-projection from our hidden size to a larger intermediate hidden representation, the application of a single gaussian error linear unit (GELU), and a linear projection back down to our hidden size. As per usual, we optionally apply dropout at training time. \n",
        "\n",
        "In the code block below, we wrap up all our previous work, embedding our input token IDs with our `Embedding` module and applying 12 layers of our `TransformerBlock`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83ikp-ZeAF6h"
      },
      "source": [
        "import haiku as hk\n",
        "\n",
        "class RobertaFeaturizer(hk.Module):\n",
        "    def __init__(self, config, *args, **kwargs):\n",
        "        super().__init__(name=\"Transformer\")\n",
        "        self.config = config\n",
        "    \n",
        "    def __call__(self, token_ids, training=False):\n",
        "        x = Embedding(self.config)(token_ids, training=training)\n",
        "        mask = (token_ids == self.config['mask_id']).astype(jnp.float32)\n",
        "        for layer_num, layer in enumerate(range(config['n_layers'])):\n",
        "            x = TransformerBlock(config, layer_num=layer_num)(x, mask, training=training)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCSgLNxR8syD"
      },
      "source": [
        "With that final `hk.Module` complete, we'll populate the config object we've been referencing through our `hk.Module`'s and apply `hk.transform` to produce a pure function. Even without attaching a classification head to the pre-trained base, we already have features we could use for purposes of computing textual similarities or similar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRwclaIt-t6K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "68ef8f9c-0d2f-431d-c71b-e7a3d6344768"
      },
      "source": [
        "from jax import jit\n",
        "from jax.random import PRNGKey\n",
        "\n",
        "config = {\n",
        "    'pretrained': pretrained,\n",
        "    'max_length': 512,\n",
        "    'embed_dropout_rate': 0.1,\n",
        "    'fully_connected_drop_rate': 0.1,\n",
        "    'attention_drop_rate': 0.1,\n",
        "    'hidden_size': 768,\n",
        "    'intermediate_size': 3072,\n",
        "    'n_heads': 12,\n",
        "    'n_layers': 12,\n",
        "    'mask_id': 1,\n",
        "    'weight_stddev': 0.02,\n",
        "\n",
        "    # For use later in finetuning\n",
        "    'n_classes': 2,\n",
        "    'classifier_drop_rate': 0.1,\n",
        "    'learning_rate': 1e-5,\n",
        "    'max_grad_norm': 1.0,\n",
        "    'l2': 0.1,\n",
        "    'n_epochs': 5,\n",
        "    'batch_size': 4\n",
        "}\n",
        "\n",
        "def featurizer_fn(tokens, training=False):\n",
        "    contextual_embeddings = RobertaFeaturizer(config)(tokens, training=training)\n",
        "    return contextual_embeddings\n",
        "\n",
        "rng = PRNGKey(42)\n",
        "roberta = hk.transform(featurizer_fn, apply_rng=True)\n",
        "sample_tokens = np.asarray(sample_tokens)\n",
        "params = roberta.init(rng, sample_tokens, training=False)\n",
        "contextual_embedding = jit(roberta.apply)(params, rng, sample_tokens)\n",
        "print(contextual_embedding.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 512, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAPxOHn88XXz"
      },
      "source": [
        "Let's check to make sure our implementation matches up with a known functional implementation -- we'll opt to use the hugging face model we instantiated earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8qbpwGzEPA1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "27db8534-2ad3-46a0-d8f4-958ad7d12b98"
      },
      "source": [
        "import torch\n",
        "batch_token_ids = torch.tensor(huggingface_tokenizer.encode(sample_text)).unsqueeze(0)\n",
        "huggingface_output_state, huggingface_pooled_state, _ = huggingface_roberta.forward(batch_token_ids)\n",
        "print(np.allclose(\n",
        "    huggingface_output_state.detach().numpy(), \n",
        "    contextual_embedding[:1, :batch_token_ids.size()[1]], \n",
        "    atol=1e-3\n",
        "))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIAdWiaJ8qBg"
      },
      "source": [
        "Great!  The contextual embeddings line up, so our implementation is correct. \n",
        "\n",
        "Admittedly, the first time I ran this things didn't go quite so smoothly -- I missed the subtle difference in the two `gelu` implementations and the difference in outputs was enough to make this check fail.  We can't directly inspect the intermediate outputs of our functions if we have things wrapped in a `jit` call like our example above, but if you remove the `jit` call to `roberta.apply`, it's trivial to add print statements to our implementation to track activations at intermediate points in our network and compare to the Hugging Face implementation.  This is a common gotcha if you're new to JAX, and I recommend reading [JAX core developer Matthew Johnson's response to a github issue](https://github.com/google/jax/issues/196#issuecomment-451671635) on the topic of printing within `jit` if you're curious why this limitation exists.\n",
        "\n",
        "Now that our featurizer is implemented, let's wrap this up to use for downstream classification tasks! \n",
        "This is as easy as slicing off the hidden state of the first token and applying a linear projection.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx2vIBdvCsNQ"
      },
      "source": [
        "class RobertaClassifier(hk.Module):\n",
        "\n",
        "    def __init__(self, config, *args, **kwargs):\n",
        "        super().__init__(name=\"Transformer\")\n",
        "        self.config = config\n",
        "    \n",
        "    def __call__(self, token_ids, training=False):\n",
        "        sequence_features = RobertaFeaturizer(self.config)(token_ids=token_ids, training=training)\n",
        "        \n",
        "        # Our classifier representation is just the output state of our first token\n",
        "        clf_state = sequence_features[:,0,:]\n",
        "        \n",
        "        if training:\n",
        "            clf_state = hk.dropout(\n",
        "                rng=hk.next_rng_key(),\n",
        "                rate=self.config['classifier_drop_rate'],\n",
        "                x=clf_state\n",
        "            )\n",
        "        \n",
        "        # We project down from our hidden dimension to n_classes and use this as our softmax logits\n",
        "        clf_logits = hk.Linear(\n",
        "            output_size=self.config['n_classes'],\n",
        "            w_init=hk.initializers.TruncatedNormal(self.config['weight_stddev'])\n",
        "        )(clf_state)\n",
        "        \n",
        "        return clf_logits\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT8njBXlBZJ1"
      },
      "source": [
        "Let's plug in a real dataset to try it out.  As much as I dislike the trope of testing text classifiers on sentiment analysis, we'll be using the IMDB Sentiment dataset from tensorflow datasets because it's already packaged up neatly for us. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS-SWhz8CnpI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571,
          "referenced_widgets": [
            "a1dfb4e2c66045fe9ca03c2e286c6c7b",
            "de51a3139a3a439aab4ff1982ae66420",
            "1eff62a3d03640c88cb343d5d4f44b08",
            "2a2c5330b4b0493baec3b7e6cb4e06f5",
            "05741a1dae274b258403af0cd4d4d7b7",
            "9789dce5bc9c4d06bc53e4cf0da589c5",
            "625f6d3e02c4447aa60a552868b2fc5a",
            "e989f52105aa4f1b991f6dd40f53c832",
            "73b2e459fa70491faf94b9e20356dbd5",
            "648a9eb195e64b28b796945c5028d07b",
            "b3c8174ca9134202a8d0ae03c720c221",
            "1041d4f787ca4151a9605bf94d62f172",
            "e3cd8a193de842a0bf38935a1ce5e59c",
            "4ef5c3ed70b7444d8d290ffd8bfa9756",
            "d53077358f6546f9a81a9afdedfa30bf",
            "ce15e79abf4f416a89bb82a15448702e",
            "81915e2c10164fa089b22e12ebe6c3c1",
            "17504e201c1b44dda3a216d3d0cc1bd0",
            "365ef5427fbe470f8bc5053ff606b9bd",
            "4e46d4f4a2a84987830aaf6149c6cdf3",
            "13244c77718e4af8981c3da023f589cc",
            "b918d1bf2b034e37a324cd413859bb19",
            "ce34e994a68b488ca0db6f3bb9705ffe",
            "c25352db456e4ea8ba4adafd7d972891",
            "b0fbd28140c84cec92ab52a2ba569244",
            "70700350bee940a5ac1fbbde921eec5d",
            "d34a4eb5c9cb41858fdfb764b76cddf8",
            "900f3def1d954c9abbf14847a5662b6f",
            "6eefbeeeb02d47dbaeaab0bbfbece380",
            "8de773c70de74e8c87740083379e760f",
            "fa9b5a99bb5b408d94e174a358a7215d",
            "77f0e167e42445e4930361ec2329d841",
            "a15821d41b434e9eb2aead09df23a2ed",
            "4be8a0cabdb2451290ecb23217113642",
            "28999991a2f24f76938484c2d893a995",
            "78f11223e69f437a9654a5df4fe94cd1",
            "f277e8b686b84d7cab3f47ef1e88ebcb",
            "da5d466cc7bc41d095db288c14e2eeaa",
            "eb3b9625ea754ade9b57a2a0ee1e4f6a",
            "0d933fd7e28344448bd17c3404e83533",
            "9ff0e3e23e374acba5a91730b550f0fd",
            "8a916437595f47cfadce738fa30c79c9",
            "f5710bd9075c4ad99af11e4ee60f6bb7",
            "823aa6fe99de4d70a6ad21f5c98ce939",
            "3bfdb19c65244ded9c2b7830fdc87b9f",
            "0b3aedd820c548b0880d143a9426883b",
            "9279278c52974d959b70e4a6484f729f",
            "1e8f9af6995146ef84c499c62f5dc79e",
            "ea7e26935e1a4167993b3deabbdc93a4",
            "66845f8544c347a4aa4345ebe80e6f8b",
            "cf8fd96442024255943393d50ff4cd5a",
            "e4b3487b04d140b783c59b502ce869df",
            "6a6eb763583849a6aecf4b766b66a48a",
            "819d17e082a64683ba84f8df091ae9fe",
            "cea390c704464466b3db39486c75c0a7",
            "4b46b146f7ed410885e2a44cff25a878",
            "268033ab11ec4d2ab94a42680912e585",
            "060c0360b5954c058c857a6b9840eafd",
            "a7158a080973484a98d8b58e307f5da6",
            "9d94fbcf8a044070be64b18ec3baac4c",
            "c231b5a87d55401e9427e72943cc4db1",
            "77ff9aae35f145628e09f3dde794481f",
            "1e291ddfae9640e5bd81c1ad1c5b7843",
            "1c8055a5280a41e8ad3ec085ced8a856"
          ]
        },
        "outputId": "3f547004-413e-4aab-d89e-cceb7bd7cb2a"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "\n",
        "def load_dataset(split, training, batch_size, n_epochs=1, n_examples=None):\n",
        "    \"\"\"Loads the dataset as a generator of batches.\"\"\"\n",
        "    ds = tfds.load(\"imdb_reviews\", split=f\"{split}[:{n_examples}]\").cache().repeat(n_epochs)\n",
        "    if training:\n",
        "        ds = ds.shuffle(10 * batch_size, seed=0)\n",
        "    ds = ds.batch(batch_size)\n",
        "    return tfds.as_numpy(ds)\n",
        "\n",
        "n_examples = 25000\n",
        "train = load_dataset(\"train\", training=True, batch_size=4, n_epochs=config['n_epochs'], n_examples=n_examples)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split train[:25000], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/m/.local/lib/python3.6/site-packages/tensorflow_core/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/m/.local/lib/python3.6/site-packages/tensorflow_core/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fSjd-MZCwVz"
      },
      "source": [
        "We'll add in an `encode_batch` utility to make calling the huggingface tokenizer more concise, transformer our new `RobertaClassifier` module into a pure function, and initialize our model state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4MMySIgC6rh"
      },
      "source": [
        "from jax.experimental import optix\n",
        "\n",
        "def roberta_classification_fn(batch_token_ids, training):\n",
        "    model = RobertaClassifier(config)(\n",
        "        jnp.asarray(batch_token_ids), \n",
        "        training=training\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def encode_batch(batch_text):\n",
        "    # Accept either utf-8 encoded bytes or unicode\n",
        "    batch_text = [\n",
        "        text.decode('utf-8') if isinstance(text, bytes) else text \n",
        "        for text in batch_text\n",
        "    ]\n",
        "    \n",
        "    # Use huggingface's tokenizer to convert from raw text to integer token ids\n",
        "    token_ids = huggingface_tokenizer.batch_encode_plus(\n",
        "        batch_text, \n",
        "        pad_to_max_length=True, \n",
        "        max_length=config['max_length'],\n",
        "    )['input_ids']\n",
        "    return np.asarray(token_ids)\n",
        "\n",
        "\n",
        "# Purify our RobertaClassifier through the use of hk.transform and initialize our classifier\n",
        "rng = jax.random.PRNGKey(42)\n",
        "roberta_classifier = hk.transform(roberta_classification_fn, apply_rng=True)\n",
        "params = roberta_classifier.init(\n",
        "    rng, \n",
        "    batch_token_ids=encode_batch(['Sample text', 'Sample text']), \n",
        "    training=True\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13FkGz158syX"
      },
      "source": [
        "Next we jit compile some functions that use our roberta classifier for computing the loss, measuring model accuracy, and computing gradient updates.\n",
        "Because passed `apply_rng=True` to the call to `hk.transform`, the first arguments to `roberta_classifier.apply` are `params` and `rng`.  After the positional haiku arguments we supply the rest of the arguments our transformed roberta_classifier function expects.\n",
        "\n",
        "Note that our `update` function calls our `loss` function -- so although we didn't decorate our `loss` function with `@jax.jit` directly we'll still reap the benefits when we call `update`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0OrGss88syY"
      },
      "source": [
        "def loss(params, rng, batch_token_ids, batch_labels):\n",
        "    logits = roberta_classifier.apply(params, rng, batch_token_ids, training=True)\n",
        "    labels = hk.one_hot(batch_labels, config['n_classes'])\n",
        "    softmax_xent = -jnp.sum(labels * jax.nn.log_softmax(logits))\n",
        "    softmax_xent /= labels.shape[0]\n",
        "    return softmax_xent\n",
        "\n",
        "@jax.jit\n",
        "def accuracy(params, rng, batch_token_ids, batch_labels):\n",
        "    predictions = roberta_classifier.apply(params, rng, batch_token_ids, training=False)\n",
        "    return jnp.mean(jnp.argmax(predictions, axis=-1) == batch_labels)\n",
        "\n",
        "@jax.jit\n",
        "def update(params, rng, opt_state, batch_token_ids, batch_labels):\n",
        "    batch_loss, grads = jax.value_and_grad(loss)(params, rng, batch_token_ids, batch_labels)\n",
        "    updates, opt_state = opt.update(grads, opt_state)\n",
        "    new_params = optix.apply_updates(params, updates)\n",
        "    return new_params, opt_state, batch_loss\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9RzrI8e8syc"
      },
      "source": [
        "Finetuning transformers for downstream tasks requires a few tricks for reliable performance -- we'll use a linear warmup + decay along with gradient clipping for our optimizers. JAX exposes 2 different sets of optimization utilities in `jax.experimental.optimizers` and `jax.experimental.optix` respectively. For finetuning RoBERTa we'll be using the latter, as it includes learning rate schedule utilities through `optix.scale_by_schedule` as well as a utility for gradient clipping with `optix.clip_by_global_norm`. We can apply our bag of optimization tricks in combination with a vanilla adam optimizer using `optix.chain` as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-uWRAsMIS4L"
      },
      "source": [
        "def make_lr_schedule(warmup_percentage, total_steps):\n",
        "    def lr_schedule(step):\n",
        "        percent_complete = step / total_steps\n",
        "        before_peak = jax.lax.convert_element_type(\n",
        "            (percent_complete <= warmup_percentage),\n",
        "            np.float32\n",
        "        )\n",
        "        scale = (\n",
        "            (before_peak * (percent_complete / warmup_percentage) + (1 - before_peak))\n",
        "            * (1 - percent_complete)\n",
        "        )\n",
        "        return scale\n",
        "    return lr_schedule\n",
        "\n",
        "\n",
        "total_steps = config['n_epochs'] * (n_examples // config['batch_size'])\n",
        "lr_schedule = make_lr_schedule(warmup_percentage=0.1, total_steps=total_steps)\n",
        "opt = optix.chain(\n",
        "    optix.clip_by_global_norm(config['max_grad_norm']),\n",
        "    optix.adam(learning_rate=config['learning_rate']),\n",
        "    optix.scale_by_schedule(lr_schedule)\n",
        ")\n",
        "opt_state = opt.init(params)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRcUMlnr8syf"
      },
      "source": [
        "Below, we throw together one final utility before writing our training loop -- a short convenience function to print how our train and test accuracy change over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rglzrhxC8syg"
      },
      "source": [
        "def measure_current_performance(params, n_examples=None, splits=('train', 'test')):\n",
        "    # Load our training evaluation and test evaluation splits \n",
        "    if 'train' in splits:\n",
        "        train_eval = load_dataset(\"train\", training=False, batch_size=25, n_examples=n_examples)\n",
        "        # Compute mean train accuracy\n",
        "        train_accuracy = np.mean([\n",
        "            accuracy(\n",
        "                params, \n",
        "                rng, \n",
        "                encode_batch(train_eval_batch['text']), \n",
        "                train_eval_batch['label']\n",
        "            )\n",
        "            for train_eval_batch in train_eval\n",
        "        ])\n",
        "        print(f\"\\t Train acc: {train_accuracy:.3f}\")\n",
        "    \n",
        "    if 'test' in splits:\n",
        "        test_eval = load_dataset(\"test\", training=False, batch_size=25, n_examples=n_examples)\n",
        "        # Compute mean test accuracy\n",
        "        test_accuracy = np.mean([\n",
        "            accuracy(\n",
        "                params, \n",
        "                rng,\n",
        "                encode_batch(test_eval_batch['text']), \n",
        "                test_eval_batch['label'],\n",
        "            )\n",
        "            for test_eval_batch in test_eval\n",
        "        ])\n",
        "        print(f\"\\t Test accuracy: {test_accuracy:.3f}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFqwpsBB8syj"
      },
      "source": [
        "In our training loop, we simply pull batches of examples from our training data iterator and call the update function to modify the state of our parameters.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8QKBe7J8syj",
        "outputId": "4dbce28d-288d-4616-bf69-db0b755b16ef"
      },
      "source": [
        "for step, train_batch in enumerate(train):\n",
        "    if step % 100 == 0:\n",
        "        print(f\"[Step {step}]\")\n",
        "    if step % 1000 == 0 and step != 0:\n",
        "        measure_current_performance(params, n_examples=100)\n",
        "\n",
        "    # Perform adam update\n",
        "    next_batch = next(train)\n",
        "    batch_token_ids = encode_batch(next_batch['text'])\n",
        "    batch_labels = next_batch['label']\n",
        "    params, opt_state, batch_loss = update(\n",
        "        params, rng, opt_state, batch_token_ids, batch_labels\n",
        "    )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Step 0]\n",
            "[Step 100]\n",
            "[Step 200]\n",
            "[Step 300]\n",
            "[Step 400]\n",
            "[Step 500]\n",
            "[Step 600]\n",
            "[Step 700]\n",
            "[Step 800]\n",
            "[Step 900]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split train[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Step 1000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split test[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t Train acc: 0.950\n",
            "\t Test accuracy: 0.870\n",
            "[Step 1100]\n",
            "[Step 1200]\n",
            "[Step 1300]\n",
            "[Step 1400]\n",
            "[Step 1500]\n",
            "[Step 1600]\n",
            "[Step 1700]\n",
            "[Step 1800]\n",
            "[Step 1900]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split train[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Step 2000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split test[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t Train acc: 0.940\n",
            "\t Test accuracy: 0.880\n",
            "[Step 2100]\n",
            "[Step 2200]\n",
            "[Step 2300]\n",
            "[Step 2400]\n",
            "[Step 2500]\n",
            "[Step 2600]\n",
            "[Step 2700]\n",
            "[Step 2800]\n",
            "[Step 2900]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split train[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Step 3000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split test[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t Train acc: 0.960\n",
            "\t Test accuracy: 0.880\n",
            "[Step 3100]\n",
            "[Step 3200]\n",
            "[Step 3300]\n",
            "[Step 3400]\n",
            "[Step 3500]\n",
            "[Step 3600]\n",
            "[Step 3700]\n",
            "[Step 3800]\n",
            "[Step 3900]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split train[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Step 4000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split test[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t Train acc: 0.970\n",
            "\t Test accuracy: 0.890\n",
            "[Step 4100]\n",
            "[Step 4200]\n",
            "[Step 4300]\n",
            "[Step 4400]\n",
            "[Step 4500]\n",
            "[Step 4600]\n",
            "[Step 4700]\n",
            "[Step 4800]\n",
            "[Step 4900]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split train[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Step 5000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split test[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t Train acc: 0.970\n",
            "\t Test accuracy: 0.870\n",
            "[Step 5100]\n",
            "[Step 5200]\n",
            "[Step 5300]\n",
            "[Step 5400]\n",
            "[Step 5500]\n",
            "[Step 5600]\n",
            "[Step 5700]\n",
            "[Step 5800]\n",
            "[Step 5900]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split train[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Step 6000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split test[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t Train acc: 0.970\n",
            "\t Test accuracy: 0.900\n",
            "[Step 6100]\n",
            "[Step 6200]\n",
            "[Step 6300]\n",
            "[Step 6400]\n",
            "[Step 6500]\n",
            "[Step 6600]\n",
            "[Step 6700]\n",
            "[Step 6800]\n",
            "[Step 6900]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split train[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Step 7000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split test[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t Train acc: 0.960\n",
            "\t Test accuracy: 0.920\n",
            "[Step 7100]\n",
            "[Step 7200]\n",
            "[Step 7300]\n",
            "[Step 7400]\n",
            "[Step 7500]\n",
            "[Step 7600]\n",
            "[Step 7700]\n",
            "[Step 7800]\n",
            "[Step 7900]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split train[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Step 8000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split test[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t Train acc: 0.980\n",
            "\t Test accuracy: 0.890\n",
            "[Step 8100]\n",
            "[Step 8200]\n",
            "[Step 8300]\n",
            "[Step 8400]\n",
            "[Step 8500]\n",
            "[Step 8600]\n",
            "[Step 8700]\n",
            "[Step 8800]\n",
            "[Step 8900]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split train[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Step 9000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split test[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t Train acc: 0.980\n",
            "\t Test accuracy: 0.910\n",
            "[Step 9100]\n",
            "[Step 9200]\n",
            "[Step 9300]\n",
            "[Step 9400]\n",
            "[Step 9500]\n",
            "[Step 9600]\n",
            "[Step 9700]\n",
            "[Step 9800]\n",
            "[Step 9900]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split train[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Step 10000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split test[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t Train acc: 0.980\n",
            "\t Test accuracy: 0.900\n",
            "[Step 10100]\n",
            "[Step 10200]\n",
            "[Step 10300]\n",
            "[Step 10400]\n",
            "[Step 10500]\n",
            "[Step 10600]\n",
            "[Step 10700]\n",
            "[Step 10800]\n",
            "[Step 10900]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split train[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Step 11000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split test[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t Train acc: 0.980\n",
            "\t Test accuracy: 0.900\n",
            "[Step 11100]\n",
            "[Step 11200]\n",
            "[Step 11300]\n",
            "[Step 11400]\n",
            "[Step 11500]\n",
            "[Step 11600]\n",
            "[Step 11700]\n",
            "[Step 11800]\n",
            "[Step 11900]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split train[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Step 12000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split test[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t Train acc: 0.990\n",
            "\t Test accuracy: 0.920\n",
            "[Step 12100]\n",
            "[Step 12200]\n",
            "[Step 12300]\n",
            "[Step 12400]\n",
            "[Step 12500]\n",
            "[Step 12600]\n",
            "[Step 12700]\n",
            "[Step 12800]\n",
            "[Step 12900]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split train[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Step 13000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split test[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t Train acc: 0.980\n",
            "\t Test accuracy: 0.920\n",
            "[Step 13100]\n",
            "[Step 13200]\n",
            "[Step 13300]\n",
            "[Step 13400]\n",
            "[Step 13500]\n",
            "[Step 13600]\n",
            "[Step 13700]\n",
            "[Step 13800]\n",
            "[Step 13900]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split train[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Step 14000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split test[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t Train acc: 0.970\n",
            "\t Test accuracy: 0.880\n",
            "[Step 14100]\n",
            "[Step 14200]\n",
            "[Step 14300]\n",
            "[Step 14400]\n",
            "[Step 14500]\n",
            "[Step 14600]\n",
            "[Step 14700]\n",
            "[Step 14800]\n",
            "[Step 14900]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split train[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Step 15000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split test[:100], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t Train acc: 0.990\n",
            "\t Test accuracy: 0.900\n",
            "[Step 15100]\n",
            "[Step 15200]\n",
            "[Step 15300]\n",
            "[Step 15400]\n",
            "[Step 15500]\n",
            "[Step 15600]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdKkud6u8syn"
      },
      "source": [
        "When all is said and done, we achieve a respectable test accuracy of 0.xx on the IMDB review dataset. Not too shabby!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLHm04RcIyIB",
        "outputId": "856553d0-3e6f-4634-a2c1-5eb78941088b"
      },
      "source": [
        "measure_current_performance(params, n_examples=25000, splits='test')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset imdb_reviews (/home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split test[:25000], from /home/m/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t Test accuracy: 0.944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY9PEiep8syq"
      },
      "source": [
        ""
      ]
    }
  ]
}