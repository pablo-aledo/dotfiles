{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colabRNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwEWNRf8ZEOi"
      },
      "source": [
        "# Train your own text generator using a recurrent neural network!\n",
        "by [Mohamed Abdulaziz](https://www.mohamedabdulaziz.com/)\n",
        "\n",
        "Train either a bidirectional or normal LSTM recurrent neural network to generate text using any dataset.  **No need to write any code. Just upload your text file and click run!**\n",
        "\n",
        "You can create modern neural network architectures which use modern techniques such as skip-embedding and attention weighting. It trains and generate text at the character-level. It also uses the CuDNN implementation when trained on GPUs which significantly improves training time when compared to the usual implementation of LSTMs.\n",
        "\n",
        "You can generate text after your model has finished training as well! Also you can continue training a pre-trained model if it needs more accuracy.\n",
        "\n",
        "**The best part is that all of the training is conducted on a free GPU courtesy of Colaboratory!**\n",
        "\n",
        "For more information about the code used in this demo please check this github link.[github link](https://github.com/demmojo/colabrnn)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rimi-EESvGVP"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "Ensure you are running this in Google Chrome. \n",
        "\n",
        "Next, copy the notebook to your Google Drive to keep it as well as save your changes. \n",
        "\n",
        "Also, make sure you are using the GPU runtime type by clicking **Runtime** in the toolbar above and then **Change runtime type**. Check if the hardware accelerator is set to GPU.\n",
        "\n",
        "That's it! Run the next code cells!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At7H0K4Qe7SY"
      },
      "source": [
        "!git clone https://github.com/demmojo/colabrnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBoHbZnTfl3a"
      },
      "source": [
        "The above code clones the github project on the Colaboratory VM. Next we will install the dependencies and import the necessary packages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EobyzRjoK_c5"
      },
      "source": [
        "Note: If you get a **Failed to assign a backend** message that means that no free GPUs are available. You can connect and train using CPUs but that will be much slower. Otherwise, try again later to connect to a server with a GPU. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrzkCSSEcjsy"
      },
      "source": [
        "!pip install keras==2.2\n",
        "\n",
        "from google.colab import files\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7cqqjbhsi3v"
      },
      "source": [
        "# What would you like to do?\n",
        "\n",
        "If you are training a new model we first need to upload a text file. Then colabrnn will use that to train and generate original text! \n",
        "\n",
        "## Either train a new model\n",
        "\n",
        "Run the cell below and click ***Choose Files*** and select your files from your local computer. (Ideally, your text files should be quite large >1mb).\n",
        "\n",
        "Please note that the uploaded file is stored on the Colaboratory VM and** only you** have access to it.\n",
        "\n",
        "After uploading the file run the next cell to start the training process! You can see generated text as the training process goes on to see how your model is learning.\n",
        "\n",
        "If you prefer to change some parameters please do so before running the cell.\n",
        "\n",
        "## Or continue training a pre-trained model\n",
        "\n",
        "Run the cell below and upload the weight, vocabulary and config files as well as the text file.\n",
        "\n",
        "After uploading the necessary model files (weight, vocabulary and config files) as well as the text file you can retrain your old model. \n",
        "\n",
        "Change the ***train_new_model*** variable to **False**. Then check whether the file names are correct before running the cell below.\n",
        "\n",
        "## Or generate text with a pre-trained model\n",
        "\n",
        "After uploading the necessary model files (weight, vocabulary and config files) as well as the text file skip to the section: **Generate text using your trained model**.\n",
        "\n",
        "Check whether the file names are correct before running the cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t36tbB1hwooE"
      },
      "source": [
        "!mkdir models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR2vf0nG_e2V"
      },
      "source": [
        "uploaded = files.upload()\n",
        "all_files = [(name, os.path.getmtime(name)) for name in os.listdir()]\n",
        "latest_uploaded_file = sorted(all_files, key=lambda x: -x[1])[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWMaIiGK2Cq9"
      },
      "source": [
        "##Neural Network Architecture\n",
        "\n",
        "The default neural network model takes in an input of up to 60 characters, converts each character to a 100 dimension character embedding vector, which then feeds those into a 128-cell bidirectional long-short-term-memory (LSTM) recurrent layer. The output of that layer is then fed into four other 128-cell LSTMs. \n",
        "\n",
        "All of the six layers are fed into an attention layer whose function is to weigh the most significant temporal features and then average them. Since the embedding and the first LSTM layer are skip-connected to the attention layer, the model updates can be backpropogated more easily thereby preventing the vanishing gradient problem. The output of the attention layer is mapped to probabilities of different characters. \n",
        "\n",
        "Bidirectional LSTMs train two instead of one LSTMs on the input sequence. The first on the input sequence as-is and the second on a reversed copy of the input sequence. This can provide additional context to the network and result in faster and even fuller learning on the problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rQ4lFinceov"
      },
      "source": [
        "from colabrnn.rnn import CharGen\n",
        "from colabrnn.rnn.train_model import train\n",
        "\n",
        "train_new_model = True\n",
        "model_name = 'mevlana'\n",
        "\n",
        "if train_new_model:  # Create a new neural network model and train it\n",
        "    char_gen = CharGen(name=model_name,\n",
        "                       bidirectional=True,  # Boolean. Train using a bidirectional LSTM or unidirectional LSTM. See this coursera video for more information: https://www.coursera.org/lecture/nlp-sequence-models/bidirectional-rnn-fyXnn\n",
        "                       rnn_size=128,  # Number of neurons in each layer of your neural network (default 128)\n",
        "                       rnn_layers=5,  # Number of layers in your neural network (default 5)       \n",
        "                       embedding_dims=100,  # Size of the embedding layer (default 100)\n",
        "                       input_length=60  # Number of characters considered for prediction (default 60)\n",
        "                      )\n",
        "    train(text_filepath=latest_uploaded_file,\n",
        "          chargen=char_gen,\n",
        "          gen_text_length=500,  # Number of characters to be generated. Average number of characters in a word is approximately 5. (default 500)\n",
        "          num_epochs=10,  # One epoch is when an entire dataset is passed forward and backward through the neural network only once (default 10)\n",
        "          batch_size=512,  # Total number of training examples present in a single batch. More is faster but there are memory constraints. If you are experiencing insufficient memory issues reduce this number. (default 512)\n",
        "          train_new_model=train_new_model\n",
        "         )  \n",
        "\n",
        "    print(char_gen.model.summary())\n",
        "else:  # Continue training an old model\n",
        "    text_filename = './colabrnn/datasets/nazim.txt'  # specify correct filename if you are retraining an old model\n",
        "    char_gen = CharGen(name=model_name,\n",
        "                      weights_filepath='./models/weights-improved-05-1.48.hdf5',  # specify correct filename if you are retraining an old model\n",
        "                      vocab_filepath='nazim_vocabulary.json',  # specify correct filename if you are retraining an old model\n",
        "                      config_filepath='nazim_config.json')  # specify correct filename if you are retraining an old model\n",
        "    \n",
        "    train(text_filename, char_gen, train_new_model=False, num_epochs=10)  # change num_epochs to specify number of epochs to continue training\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u1ea4ADtIF4"
      },
      "source": [
        "##A little note on temperature\n",
        "\n",
        "Temperature is a hyperparameter of LSTMs (and neural networks generally) used to control the randomness of predictions by scaling the logits before applying softmax. For example, in TensorFlow’s Magenta implementation of LSTMs, temperature represents how much to divide the logits by before computing the softmax.\n",
        "\n",
        "When the temperature is 1, we compute the softmax directly on the logits (the unscaled output of earlier layers), and using a temperature of 0.6 the model computes the softmax on logits0.6, resulting in a larger value. Performing softmax on larger values makes the LSTM more confident (less input is needed to activate the output layer) but also more conservative in its samples (it is less likely to sample from unlikely candidates). Using a higher temperature produces a softer probability distribution over the classes, and makes the RNN more “easily excited” by samples, resulting in more diversity and also more mistakes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QizcSnyvzGdR"
      },
      "source": [
        "## Save the model files\n",
        "\n",
        "Run the cell below to save the model files locally. You can upload them again later to retrain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmmApybl3Btl"
      },
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "files.download('{}_vocabulary.json'.format(model_name))\n",
        "files.download('{}_config.json'.format(model_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaJvAP1mO7Gc"
      },
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "for root, dirs, filenames in os.walk(\"./models\"):\n",
        "    for filename in filenames:\n",
        "      print(filename)\n",
        "      files.download('./models/{}'.format(filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HTTjr1rzYV9"
      },
      "source": [
        "## Generate text using your trained model!\n",
        "\n",
        "Run the cell below to generate samples of your trained model. \n",
        "\n",
        "You can specify the starting text  for the model by changing the ***prefix***  variable to use as the beginning of the generated text.\n",
        "\n",
        "You can also define how long you want your generated text to be by ***change gen_text_length*** variable.\n",
        "\n",
        "Have fun!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_ioCslUzp21"
      },
      "source": [
        "from colabrnn.rnn import CharGen\n",
        "\n",
        "char_gen = CharGen(weights_filepath='colab_weights.hdf5',  # specify correct filename \n",
        "                   vocab_filepath='colab_vocabulary.json',  # specify correct filename\n",
        "                   config_filepath='colab_config.json')  # specify correct filename \n",
        "\n",
        "char_gen.generate(gen_text_length=500, prefix='To be or not to be,')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICV_FdOe6kvi"
      },
      "source": [
        "If at any time you would like to list the contents of the current directory use the following command:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "910T4lEU6yhm"
      },
      "source": [
        "If you would like to restart or reset the Colaboratory VM you can run the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRe7rzwzrUus"
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEqZBUMp7J3f"
      },
      "source": [
        "### If you have any questions, suggestions or would like to share your project, please contact me [here](https://www.mohamedabdulaziz.com/#contact).\n",
        "\n",
        "### You can also check out my other projects on [Github](https://github.com/demmojo)."
      ]
    }
  ]
}