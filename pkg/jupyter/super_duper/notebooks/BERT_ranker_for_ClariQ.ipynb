{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT-ranker_for_ClariQ.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj5cB8x9R7Lp"
      },
      "source": [
        "## Installing transformer-rankers and dependencies\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndWAB7rQRg0J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ec804a7-0ed5-41bc-8517-311284caefe4"
      },
      "source": [
        "!pip install git+https://github.com/Guzpenha/transformer_rankers.git\n",
        "!wget https://raw.githubusercontent.com/Guzpenha/transformer_rankers/master/requirements.txt\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/Guzpenha/transformer_rankers.git\n",
            "  Cloning https://github.com/Guzpenha/transformer_rankers.git to /tmp/pip-req-build-_7lci577\n",
            "  Running command git clone -q https://github.com/Guzpenha/transformer_rankers.git /tmp/pip-req-build-_7lci577\n",
            "Requirement already satisfied (use --upgrade to upgrade): transformer-rankers==0.0.1 from git+https://github.com/Guzpenha/transformer_rankers.git in /usr/local/lib/python3.6/dist-packages\n",
            "Building wheels for collected packages: transformer-rankers\n",
            "  Building wheel for transformer-rankers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformer-rankers: filename=transformer_rankers-0.0.1-cp36-none-any.whl size=49634 sha256=3b64a324804a8e4de1214835eb56822d920fa61cea283408cb7eac6a7bbbd14c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-eyb3kofw/wheels/8d/17/81/c0402e419a193812e5b02d153d777fcae6cfcecf519e3884cb\n",
            "Successfully built transformer-rankers\n",
            "--2020-12-15 18:31:53--  https://raw.githubusercontent.com/Guzpenha/transformer_rankers/master/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 239 [text/plain]\n",
            "Saving to: ‘requirements.txt.2’\n",
            "\n",
            "requirements.txt.2  100%[===================>]     239  --.-KB/s    in 0s      \n",
            "\n",
            "2020-12-15 18:31:53 (17.0 MB/s) - ‘requirements.txt.2’ saved [239/239]\n",
            "\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (5.5.0)\n",
            "Requirement already satisfied: numpy==1.18.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (1.18.5)\n",
            "Requirement already satisfied: pandas==1.0.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (1.0.4)\n",
            "Requirement already satisfied: pytrec-eval==0.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (0.4)\n",
            "Requirement already satisfied: sacred==0.8.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.8.1)\n",
            "Requirement already satisfied: torch==1.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (1.5.0)\n",
            "Requirement already satisfied: tqdm==4.46.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (4.46.1)\n",
            "Requirement already satisfied: transformers==3.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (3.1.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (1.4.1)\n",
            "Requirement already satisfied: sentence-transformers==0.2.6.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (0.2.6.1)\n",
            "Requirement already satisfied: faiss-cpu==1.6.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 11)) (1.6.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 12)) (0.8)\n",
            "Requirement already satisfied: pyserini==0.9.3.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 13)) (0.9.3.1)\n",
            "Requirement already satisfied: wget==3.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 14)) (3.2)\n",
            "Requirement already satisfied: py7zr==0.9.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 15)) (0.9.5)\n",
            "Requirement already satisfied: wandb==0.9.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 16)) (0.9.7)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->-r requirements.txt (line 1)) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->-r requirements.txt (line 1)) (50.3.2)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->-r requirements.txt (line 1)) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->-r requirements.txt (line 1)) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->-r requirements.txt (line 1)) (0.8.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->-r requirements.txt (line 1)) (4.3.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->-r requirements.txt (line 1)) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->-r requirements.txt (line 1)) (1.0.18)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas==1.0.4->-r requirements.txt (line 3)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas==1.0.4->-r requirements.txt (line 3)) (2018.9)\n",
            "Requirement already satisfied: GitPython in /usr/local/lib/python3.6/dist-packages (from sacred==0.8.1->-r requirements.txt (line 5)) (3.1.11)\n",
            "Requirement already satisfied: docopt<1.0,>=0.3 in /usr/local/lib/python3.6/dist-packages (from sacred==0.8.1->-r requirements.txt (line 5)) (0.6.2)\n",
            "Requirement already satisfied: munch<3.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from sacred==0.8.1->-r requirements.txt (line 5)) (2.5.0)\n",
            "Requirement already satisfied: wrapt<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from sacred==0.8.1->-r requirements.txt (line 5)) (1.12.1)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.6/dist-packages (from sacred==0.8.1->-r requirements.txt (line 5)) (0.4.4)\n",
            "Requirement already satisfied: jsonpickle<2.0,>=1.2 in /usr/local/lib/python3.6/dist-packages (from sacred==0.8.1->-r requirements.txt (line 5)) (1.4.2)\n",
            "Requirement already satisfied: packaging>=18.0 in /usr/local/lib/python3.6/dist-packages (from sacred==0.8.1->-r requirements.txt (line 5)) (20.7)\n",
            "Requirement already satisfied: py-cpuinfo>=4.0 in /usr/local/lib/python3.6/dist-packages (from sacred==0.8.1->-r requirements.txt (line 5)) (7.0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.0->-r requirements.txt (line 6)) (0.16.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0->-r requirements.txt (line 8)) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0->-r requirements.txt (line 8)) (0.8.1rc2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0->-r requirements.txt (line 8)) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0->-r requirements.txt (line 8)) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0->-r requirements.txt (line 8)) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0->-r requirements.txt (line 8)) (0.1.94)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.2.6.1->-r requirements.txt (line 10)) (0.22.2.post1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.2.6.1->-r requirements.txt (line 10)) (3.2.5)\n",
            "Requirement already satisfied: pyjnius in /usr/local/lib/python3.6/dist-packages (from pyserini==0.9.3.1->-r requirements.txt (line 13)) (1.3.0)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from pyserini==0.9.3.1->-r requirements.txt (line 13)) (0.29.21)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.6/dist-packages (from py7zr==0.9.5->-r requirements.txt (line 15)) (3.9.9)\n",
            "Requirement already satisfied: texttable in /usr/local/lib/python3.6/dist-packages (from py7zr==0.9.5->-r requirements.txt (line 15)) (1.6.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from py7zr==0.9.5->-r requirements.txt (line 15)) (3.1.1)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.7->-r requirements.txt (line 16)) (0.19.5)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.7->-r requirements.txt (line 16)) (5.0.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.7->-r requirements.txt (line 16)) (1.15.0)\n",
            "Requirement already satisfied: gql==0.2.0 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.7->-r requirements.txt (line 16)) (0.2.0)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.7->-r requirements.txt (line 16)) (3.5.4)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.7->-r requirements.txt (line 16)) (3.13)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.7->-r requirements.txt (line 16)) (0.4.0)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.7->-r requirements.txt (line 16)) (7.1.2)\n",
            "Requirement already satisfied: watchdog>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.7->-r requirements.txt (line 16)) (1.0.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.7->-r requirements.txt (line 16)) (5.4.8)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.7->-r requirements.txt (line 16)) (7.352.0)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from wandb==0.9.7->-r requirements.txt (line 16)) (1.0.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->-r requirements.txt (line 1)) (0.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->-r requirements.txt (line 1)) (0.2.5)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from GitPython->sacred==0.8.1->-r requirements.txt (line 5)) (4.0.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=18.0->sacred==0.8.1->-r requirements.txt (line 5)) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.1.0->-r requirements.txt (line 8)) (0.17.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.1.0->-r requirements.txt (line 8)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.1.0->-r requirements.txt (line 8)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.1.0->-r requirements.txt (line 8)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.1.0->-r requirements.txt (line 8)) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->py7zr==0.9.5->-r requirements.txt (line 15)) (3.4.0)\n",
            "Requirement already satisfied: graphql-core<2,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb==0.9.7->-r requirements.txt (line 16)) (1.1)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb==0.9.7->-r requirements.txt (line 16)) (2.3)\n",
            "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->GitPython->sacred==0.8.1->-r requirements.txt (line 5)) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1AfQvDVjLJU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "191288d3-809a-45df-83bf-d927b0569f6b"
      },
      "source": [
        "#Install Anserini which is also a requirement for part of transformer-rankers (BM25 Negative Samplers)\n",
        "!apt-get install maven -qq\n",
        "!git clone --recurse-submodules https://github.com/castorini/anserini.git\n",
        "!cd anserini; mvn clean package appassembler:assemble -DskipTests -Dmaven.javadoc.skip=true\n",
        "!ls anserini/target/appassembler/bin/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'anserini' already exists and is not an empty directory.\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$1 (file:/usr/share/maven/lib/guice.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)\n",
            "WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$1\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "[\u001b[1;34mINFO\u001b[m] Scanning for projects...\n",
            "[\u001b[1;34mINFO\u001b[m] \n",
            "[\u001b[1;34mINFO\u001b[m] \u001b[1m------------------------< \u001b[0;36mio.anserini:anserini\u001b[0;1m >------------------------\u001b[m\n",
            "[\u001b[1;34mINFO\u001b[m] \u001b[1mBuilding Anserini 0.10.1-SNAPSHOT\u001b[m\n",
            "[\u001b[1;34mINFO\u001b[m] \u001b[1m--------------------------------[ jar ]---------------------------------\u001b[m\n",
            "[\u001b[1;34mINFO\u001b[m] \n",
            "[\u001b[1;34mINFO\u001b[m] \u001b[1m--- \u001b[0;32mmaven-clean-plugin:2.5:clean\u001b[m \u001b[1m(default-clean)\u001b[m @ \u001b[36manserini\u001b[0;1m ---\u001b[m\n",
            "[\u001b[1;34mINFO\u001b[m] Deleting /content/anserini/target\n",
            "[\u001b[1;34mINFO\u001b[m] \n",
            "[\u001b[1;34mINFO\u001b[m] \u001b[1m--- \u001b[0;32mjacoco-maven-plugin:0.8.2:prepare-agent\u001b[m \u001b[1m(default)\u001b[m @ \u001b[36manserini\u001b[0;1m ---\u001b[m\n",
            "[\u001b[1;34mINFO\u001b[m] argLine set to -javaagent:/root/.m2/repository/org/jacoco/org.jacoco.agent/0.8.2/org.jacoco.agent-0.8.2-runtime.jar=destfile=/content/anserini/target/jacoco.exec\n",
            "[\u001b[1;34mINFO\u001b[m] \n",
            "[\u001b[1;34mINFO\u001b[m] \u001b[1m--- \u001b[0;32mmaven-resources-plugin:2.6:resources\u001b[m \u001b[1m(default-resources)\u001b[m @ \u001b[36manserini\u001b[0;1m ---\u001b[m\n",
            "[\u001b[1;34mINFO\u001b[m] Using 'UTF-8' encoding to copy filtered resources.\n",
            "[\u001b[1;34mINFO\u001b[m] Copying 233 resources\n",
            "[\u001b[1;34mINFO\u001b[m] \n",
            "[\u001b[1;34mINFO\u001b[m] \u001b[1m--- \u001b[0;32mmaven-compiler-plugin:3.8.1:compile\u001b[m \u001b[1m(default-compile)\u001b[m @ \u001b[36manserini\u001b[0;1m ---\u001b[m\n",
            "[\u001b[1;34mINFO\u001b[m] Changes detected - recompiling the module!\n",
            "[\u001b[1;34mINFO\u001b[m] Compiling 153 source files to /content/anserini/target/classes\n",
            "[\u001b[1;33mWARNING\u001b[m] /content/anserini/src/main/java/io/anserini/index/IndexCollection.java:[180,31] sha1() in com.google.common.hash.Hashing has been deprecated\n",
            "[\u001b[1;33mWARNING\u001b[m] /content/anserini/src/main/java/io/anserini/index/IndexCollection.java:[289,31] sha1() in com.google.common.hash.Hashing has been deprecated\n",
            "[\u001b[1;33mWARNING\u001b[m] /content/anserini/src/main/java/io/anserini/index/IndexCollection.java:[464,31] sha1() in com.google.common.hash.Hashing has been deprecated\n",
            "[\u001b[1;34mINFO\u001b[m] \n",
            "[\u001b[1;34mINFO\u001b[m] \u001b[1m--- \u001b[0;32mmaven-resources-plugin:2.6:testResources\u001b[m \u001b[1m(default-testResources)\u001b[m @ \u001b[36manserini\u001b[0;1m ---\u001b[m\n",
            "[\u001b[1;34mINFO\u001b[m] Using 'UTF-8' encoding to copy filtered resources.\n",
            "[\u001b[1;34mINFO\u001b[m] Copying 51 resources\n",
            "[\u001b[1;34mINFO\u001b[m] \n",
            "[\u001b[1;34mINFO\u001b[m] \u001b[1m--- \u001b[0;32mmaven-compiler-plugin:3.8.1:testCompile\u001b[m \u001b[1m(default-testCompile)\u001b[m @ \u001b[36manserini\u001b[0;1m ---\u001b[m\n",
            "[\u001b[1;34mINFO\u001b[m] Changes detected - recompiling the module!\n",
            "[\u001b[1;34mINFO\u001b[m] Compiling 106 source files to /content/anserini/target/test-classes\n",
            "[\u001b[1;33mWARNING\u001b[m] /content/anserini/src/test/java/io/anserini/doc/GenerateRegressionDocsTest.java:[47,7] org.apache.commons.lang3.text.StrSubstitutor in org.apache.commons.lang3.text has been deprecated\n",
            "[\u001b[1;33mWARNING\u001b[m] /content/anserini/src/test/java/io/anserini/doc/GenerateRegressionDocsTest.java:[47,32] org.apache.commons.lang3.text.StrSubstitutor in org.apache.commons.lang3.text has been deprecated\n",
            "[\u001b[1;33mWARNING\u001b[m] /content/anserini/src/test/java/io/anserini/doc/JDIQ2018EffectivenessDocsTest.java:[122,5] org.apache.commons.lang3.text.StrSubstitutor in org.apache.commons.lang3.text has been deprecated\n",
            "[\u001b[1;33mWARNING\u001b[m] /content/anserini/src/test/java/io/anserini/doc/JDIQ2018EffectivenessDocsTest.java:[122,30] org.apache.commons.lang3.text.StrSubstitutor in org.apache.commons.lang3.text has been deprecated\n",
            "[\u001b[1;34mINFO\u001b[m] \n",
            "[\u001b[1;34mINFO\u001b[m] \u001b[1m--- \u001b[0;32mmaven-surefire-plugin:2.12.4:test\u001b[m \u001b[1m(default-test)\u001b[m @ \u001b[36manserini\u001b[0;1m ---\u001b[m\n",
            "[\u001b[1;34mINFO\u001b[m] Tests are skipped.\n",
            "[\u001b[1;34mINFO\u001b[m] \n",
            "[\u001b[1;34mINFO\u001b[m] \u001b[1m--- \u001b[0;32mjacoco-maven-plugin:0.8.2:report\u001b[m \u001b[1m(report)\u001b[m @ \u001b[36manserini\u001b[0;1m ---\u001b[m\n",
            "[\u001b[1;34mINFO\u001b[m] Skipping JaCoCo execution due to missing execution data file.\n",
            "[\u001b[1;34mINFO\u001b[m] \n",
            "[\u001b[1;34mINFO\u001b[m] \u001b[1m--- \u001b[0;32mmaven-jar-plugin:2.4:jar\u001b[m \u001b[1m(default-jar)\u001b[m @ \u001b[36manserini\u001b[0;1m ---\u001b[m\n",
            "[\u001b[1;34mINFO\u001b[m] Building jar: /content/anserini/target/anserini-0.10.1-SNAPSHOT.jar\n",
            "[\u001b[1;34mINFO\u001b[m] \n",
            "[\u001b[1;34mINFO\u001b[m] \u001b[1m--- \u001b[0;32mmaven-javadoc-plugin:3.1.0:jar\u001b[m \u001b[1m(attach-javadocs)\u001b[m @ \u001b[36manserini\u001b[0;1m ---\u001b[m\n",
            "[\u001b[1;34mINFO\u001b[m] Skipping javadoc generation\n",
            "[\u001b[1;34mINFO\u001b[m] \n",
            "[\u001b[1;34mINFO\u001b[m] \u001b[1m--- \u001b[0;32mmaven-shade-plugin:3.2.1:shade\u001b[m \u001b[1m(default)\u001b[m @ \u001b[36manserini\u001b[0;1m ---\u001b[m\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.lucene:lucene-core:jar:8.3.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.solr:solr-solrj:jar:8.3.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including commons-io:commons-io:jar:2.5 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including io.netty:netty-buffer:jar:4.1.29.Final in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including io.netty:netty-codec:jar:4.1.29.Final in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including io.netty:netty-common:jar:4.1.29.Final in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including io.netty:netty-handler:jar:4.1.29.Final in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including io.netty:netty-resolver:jar:4.1.29.Final in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including io.netty:netty-transport:jar:4.1.29.Final in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including io.netty:netty-transport-native-epoll:jar:4.1.29.Final in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including io.netty:netty-transport-native-unix-common:jar:4.1.29.Final in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.commons:commons-math3:jar:3.6.1 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.httpcomponents:httpclient:jar:4.5.6 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.httpcomponents:httpcore:jar:4.4.10 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.httpcomponents:httpmime:jar:4.5.6 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.zookeeper:zookeeper:jar:3.5.5 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.zookeeper:zookeeper-jute:jar:3.5.5 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.codehaus.woodstox:stax2-api:jar:3.1.4 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.codehaus.woodstox:woodstox-core-asl:jar:4.4.1 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.eclipse.jetty:jetty-alpn-client:jar:9.4.19.v20190610 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.eclipse.jetty:jetty-alpn-java-client:jar:9.4.19.v20190610 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.eclipse.jetty:jetty-client:jar:9.4.19.v20190610 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.eclipse.jetty:jetty-http:jar:9.4.19.v20190610 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.eclipse.jetty:jetty-io:jar:9.4.19.v20190610 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.eclipse.jetty:jetty-util:jar:9.4.19.v20190610 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.eclipse.jetty.http2:http2-client:jar:9.4.19.v20190610 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.eclipse.jetty.http2:http2-common:jar:9.4.19.v20190610 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.eclipse.jetty.http2:http2-hpack:jar:9.4.19.v20190610 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.eclipse.jetty.http2:http2-http-client-transport:jar:9.4.19.v20190610 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.slf4j:jcl-over-slf4j:jar:1.7.24 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.slf4j:slf4j-api:jar:1.7.24 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.lucene:lucene-analyzers-common:jar:8.3.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.lucene:lucene-backward-codecs:jar:8.3.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.lucene:lucene-grouping:jar:8.3.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.lucene:lucene-highlighter:jar:8.3.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.lucene:lucene-join:jar:8.3.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.lucene:lucene-memory:jar:8.3.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.lucene:lucene-misc:jar:8.3.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.lucene:lucene-queries:jar:8.3.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.lucene:lucene-queryparser:jar:8.3.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.lucene:lucene-sandbox:jar:8.3.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.lucene:lucene-spatial-extras:jar:8.3.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.lucene:lucene-spatial3d:jar:8.3.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.lucene:lucene-suggest:jar:8.3.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including com.carrotsearch:hppc:jar:0.8.1 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including com.fasterxml.jackson.core:jackson-annotations:jar:2.9.9 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including com.fasterxml.jackson.dataformat:jackson-dataformat-smile:jar:2.9.9 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including com.google.guava:guava:jar:25.1-jre in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including com.tdunning:t-digest:jar:3.1 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including commons-codec:commons-codec:jar:1.11 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.commons:commons-lang3:jar:3.8.1 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.elasticsearch.client:elasticsearch-rest-high-level-client:jar:7.0.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.elasticsearch:elasticsearch:jar:7.0.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.elasticsearch:elasticsearch-core:jar:7.0.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.elasticsearch:elasticsearch-secure-sm:jar:7.0.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.elasticsearch:elasticsearch-x-content:jar:7.0.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including com.fasterxml.jackson.dataformat:jackson-dataformat-cbor:jar:2.8.11 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.elasticsearch:elasticsearch-geo:jar:7.0.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.lucene:lucene-spatial:jar:8.0.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.elasticsearch:elasticsearch-cli:jar:7.0.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including net.sf.jopt-simple:jopt-simple:jar:5.0.2 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including joda-time:joda-time:jar:2.10.1 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.hdrhistogram:HdrHistogram:jar:2.1.9 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.elasticsearch:jna:jar:4.5.1 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.elasticsearch.client:elasticsearch-rest-client:jar:7.0.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.httpcomponents:httpasyncclient:jar:4.1.4 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.httpcomponents:httpcore-nio:jar:4.4.11 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including commons-logging:commons-logging:jar:1.1.3 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.elasticsearch.plugin:parent-join-client:jar:7.0.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.elasticsearch.plugin:aggs-matrix-stats-client:jar:7.0.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.elasticsearch.plugin:rank-eval-client:jar:7.0.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.elasticsearch.plugin:lang-mustache-client:jar:7.0.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including com.github.spullara.mustache.java:compiler:jar:0.9.3 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.tukaani:xz:jar:1.5 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including com.fasterxml.jackson.core:jackson-core:jar:2.10.1 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including com.fasterxml.jackson.core:jackson-databind:jar:2.10.1 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including com.fasterxml.jackson.datatype:jackson-datatype-jdk8:jar:2.10.1 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:jar:2.10.1 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.yaml:snakeyaml:jar:1.24 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.jsoup:jsoup:jar:1.8.3 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including args4j:args4j:jar:2.32 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.logging.log4j:log4j-api:jar:2.13.3 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.logging.log4j:log4j-core:jar:2.13.3 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.slf4j:slf4j-simple:jar:1.7.29 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including io.anserini:anserini-fastutil:jar:6.5.6 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.wikiclean:wikiclean:jar:1.1 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.ant:ant:jar:1.9.15 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.ant:ant-launcher:jar:1.9.15 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.openrdf.sesame:sesame-rio-ntriples:jar:4.1.2 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.openrdf.sesame:sesame-model:jar:4.1.2 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.openrdf.sesame:sesame-util:jar:4.1.2 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.openrdf.sesame:sesame-rio-api:jar:4.1.2 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.openrdf.sesame:sesame-rio-datatypes:jar:4.1.2 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.openrdf.sesame:sesame-rio-languages:jar:4.1.2 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including com.twitter.twittertext:twitter-text:jar:2.0.10 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including com.google.code.findbugs:jsr305:jar:2.0.1 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including com.github.TREMA-UNH:trec-car-tools-java:jar:13 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including co.nstant.in:cbor:jar:0.7 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.jetbrains:annotations-java5:jar:20.1.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.commons:commons-pool2:jar:2.6.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.wikidata.wdtk:wdtk-dumpfiles:jar:0.10.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.wikidata.wdtk:wdtk-datamodel:jar:0.10.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.wikidata.wdtk:wdtk-util:jar:0.10.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.commons:commons-compress:jar:1.18 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.wikidata.wdtk:wdtk-storage:jar:0.10.0 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.mockito:mockito-all:jar:1.10.19 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.jbibtex:jbibtex:jar:1.0.17 in the shaded jar.\n",
            "[\u001b[1;34mINFO\u001b[m] Including org.apache.commons:commons-csv:jar:1.8 in the shaded jar.\n",
            "[\u001b[1;33mWARNING\u001b[m] Discovered module-info.class. Shading will break its strong encapsulation.\n",
            "[\u001b[1;33mWARNING\u001b[m] Discovered module-info.class. Shading will break its strong encapsulation.\n",
            "[\u001b[1;33mWARNING\u001b[m] Discovered module-info.class. Shading will break its strong encapsulation.\n",
            "[\u001b[1;33mWARNING\u001b[m] Discovered module-info.class. Shading will break its strong encapsulation.\n",
            "[\u001b[1;33mWARNING\u001b[m] jcl-over-slf4j-1.7.24.jar, commons-logging-1.1.3.jar define 6 overlapping classes: \n",
            "[\u001b[1;33mWARNING\u001b[m]   - org.apache.commons.logging.impl.SimpleLog$1\n",
            "[\u001b[1;33mWARNING\u001b[m]   - org.apache.commons.logging.Log\n",
            "[\u001b[1;33mWARNING\u001b[m]   - org.apache.commons.logging.impl.SimpleLog\n",
            "[\u001b[1;33mWARNING\u001b[m]   - org.apache.commons.logging.LogConfigurationException\n",
            "[\u001b[1;33mWARNING\u001b[m]   - org.apache.commons.logging.impl.NoOpLog\n",
            "[\u001b[1;33mWARNING\u001b[m]   - org.apache.commons.logging.LogFactory\n",
            "[\u001b[1;33mWARNING\u001b[m] maven-shade-plugin has detected that some class files are\n",
            "[\u001b[1;33mWARNING\u001b[m] present in two or more JARs. When this happens, only one\n",
            "[\u001b[1;33mWARNING\u001b[m] single version of the class is copied to the uber jar.\n",
            "[\u001b[1;33mWARNING\u001b[m] Usually this is not harmful and you can skip these warnings,\n",
            "[\u001b[1;33mWARNING\u001b[m] otherwise try to manually exclude artifacts based on\n",
            "[\u001b[1;33mWARNING\u001b[m] mvn dependency:tree -Ddetail=true and the above output.\n",
            "[\u001b[1;33mWARNING\u001b[m] See http://maven.apache.org/plugins/maven-shade-plugin/\n",
            "[\u001b[1;34mINFO\u001b[m] Attaching shaded artifact.\n",
            "[\u001b[1;34mINFO\u001b[m] \n",
            "[\u001b[1;34mINFO\u001b[m] \u001b[1m--- \u001b[0;32mappassembler-maven-plugin:2.0.0:assemble\u001b[m \u001b[1m(default-cli)\u001b[m @ \u001b[36manserini\u001b[0;1m ---\u001b[m\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/lucene/lucene-core/8.3.0/lucene-core-8.3.0.jar to /content/anserini/target/appassembler/repo/lucene-core-8.3.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/solr/solr-solrj/8.3.0/solr-solrj-8.3.0.jar to /content/anserini/target/appassembler/repo/solr-solrj-8.3.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/commons-io/commons-io/2.5/commons-io-2.5.jar to /content/anserini/target/appassembler/repo/commons-io-2.5.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/io/netty/netty-buffer/4.1.29.Final/netty-buffer-4.1.29.Final.jar to /content/anserini/target/appassembler/repo/netty-buffer-4.1.29.Final.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/io/netty/netty-codec/4.1.29.Final/netty-codec-4.1.29.Final.jar to /content/anserini/target/appassembler/repo/netty-codec-4.1.29.Final.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/io/netty/netty-common/4.1.29.Final/netty-common-4.1.29.Final.jar to /content/anserini/target/appassembler/repo/netty-common-4.1.29.Final.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/io/netty/netty-handler/4.1.29.Final/netty-handler-4.1.29.Final.jar to /content/anserini/target/appassembler/repo/netty-handler-4.1.29.Final.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/io/netty/netty-resolver/4.1.29.Final/netty-resolver-4.1.29.Final.jar to /content/anserini/target/appassembler/repo/netty-resolver-4.1.29.Final.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/io/netty/netty-transport/4.1.29.Final/netty-transport-4.1.29.Final.jar to /content/anserini/target/appassembler/repo/netty-transport-4.1.29.Final.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/io/netty/netty-transport-native-epoll/4.1.29.Final/netty-transport-native-epoll-4.1.29.Final.jar to /content/anserini/target/appassembler/repo/netty-transport-native-epoll-4.1.29.Final.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/io/netty/netty-transport-native-unix-common/4.1.29.Final/netty-transport-native-unix-common-4.1.29.Final.jar to /content/anserini/target/appassembler/repo/netty-transport-native-unix-common-4.1.29.Final.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/commons/commons-math3/3.6.1/commons-math3-3.6.1.jar to /content/anserini/target/appassembler/repo/commons-math3-3.6.1.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar to /content/anserini/target/appassembler/repo/httpclient-4.5.6.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/httpcomponents/httpcore/4.4.10/httpcore-4.4.10.jar to /content/anserini/target/appassembler/repo/httpcore-4.4.10.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/httpcomponents/httpmime/4.5.6/httpmime-4.5.6.jar to /content/anserini/target/appassembler/repo/httpmime-4.5.6.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/zookeeper/zookeeper/3.5.5/zookeeper-3.5.5.jar to /content/anserini/target/appassembler/repo/zookeeper-3.5.5.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.5.5/zookeeper-jute-3.5.5.jar to /content/anserini/target/appassembler/repo/zookeeper-jute-3.5.5.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/codehaus/woodstox/stax2-api/3.1.4/stax2-api-3.1.4.jar to /content/anserini/target/appassembler/repo/stax2-api-3.1.4.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/codehaus/woodstox/woodstox-core-asl/4.4.1/woodstox-core-asl-4.4.1.jar to /content/anserini/target/appassembler/repo/woodstox-core-asl-4.4.1.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/eclipse/jetty/jetty-alpn-client/9.4.19.v20190610/jetty-alpn-client-9.4.19.v20190610.jar to /content/anserini/target/appassembler/repo/jetty-alpn-client-9.4.19.v20190610.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/eclipse/jetty/jetty-alpn-java-client/9.4.19.v20190610/jetty-alpn-java-client-9.4.19.v20190610.jar to /content/anserini/target/appassembler/repo/jetty-alpn-java-client-9.4.19.v20190610.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/eclipse/jetty/jetty-client/9.4.19.v20190610/jetty-client-9.4.19.v20190610.jar to /content/anserini/target/appassembler/repo/jetty-client-9.4.19.v20190610.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/eclipse/jetty/jetty-http/9.4.19.v20190610/jetty-http-9.4.19.v20190610.jar to /content/anserini/target/appassembler/repo/jetty-http-9.4.19.v20190610.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/eclipse/jetty/jetty-io/9.4.19.v20190610/jetty-io-9.4.19.v20190610.jar to /content/anserini/target/appassembler/repo/jetty-io-9.4.19.v20190610.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/eclipse/jetty/jetty-util/9.4.19.v20190610/jetty-util-9.4.19.v20190610.jar to /content/anserini/target/appassembler/repo/jetty-util-9.4.19.v20190610.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/eclipse/jetty/http2/http2-client/9.4.19.v20190610/http2-client-9.4.19.v20190610.jar to /content/anserini/target/appassembler/repo/http2-client-9.4.19.v20190610.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/eclipse/jetty/http2/http2-common/9.4.19.v20190610/http2-common-9.4.19.v20190610.jar to /content/anserini/target/appassembler/repo/http2-common-9.4.19.v20190610.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/eclipse/jetty/http2/http2-hpack/9.4.19.v20190610/http2-hpack-9.4.19.v20190610.jar to /content/anserini/target/appassembler/repo/http2-hpack-9.4.19.v20190610.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/eclipse/jetty/http2/http2-http-client-transport/9.4.19.v20190610/http2-http-client-transport-9.4.19.v20190610.jar to /content/anserini/target/appassembler/repo/http2-http-client-transport-9.4.19.v20190610.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.24/jcl-over-slf4j-1.7.24.jar to /content/anserini/target/appassembler/repo/jcl-over-slf4j-1.7.24.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/slf4j/slf4j-api/1.7.24/slf4j-api-1.7.24.jar to /content/anserini/target/appassembler/repo/slf4j-api-1.7.24.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/lucene/lucene-analyzers-common/8.3.0/lucene-analyzers-common-8.3.0.jar to /content/anserini/target/appassembler/repo/lucene-analyzers-common-8.3.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/lucene/lucene-backward-codecs/8.3.0/lucene-backward-codecs-8.3.0.jar to /content/anserini/target/appassembler/repo/lucene-backward-codecs-8.3.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/lucene/lucene-grouping/8.3.0/lucene-grouping-8.3.0.jar to /content/anserini/target/appassembler/repo/lucene-grouping-8.3.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/lucene/lucene-highlighter/8.3.0/lucene-highlighter-8.3.0.jar to /content/anserini/target/appassembler/repo/lucene-highlighter-8.3.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/lucene/lucene-join/8.3.0/lucene-join-8.3.0.jar to /content/anserini/target/appassembler/repo/lucene-join-8.3.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/lucene/lucene-memory/8.3.0/lucene-memory-8.3.0.jar to /content/anserini/target/appassembler/repo/lucene-memory-8.3.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/lucene/lucene-misc/8.3.0/lucene-misc-8.3.0.jar to /content/anserini/target/appassembler/repo/lucene-misc-8.3.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/lucene/lucene-queries/8.3.0/lucene-queries-8.3.0.jar to /content/anserini/target/appassembler/repo/lucene-queries-8.3.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/lucene/lucene-queryparser/8.3.0/lucene-queryparser-8.3.0.jar to /content/anserini/target/appassembler/repo/lucene-queryparser-8.3.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/lucene/lucene-sandbox/8.3.0/lucene-sandbox-8.3.0.jar to /content/anserini/target/appassembler/repo/lucene-sandbox-8.3.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/lucene/lucene-spatial-extras/8.3.0/lucene-spatial-extras-8.3.0.jar to /content/anserini/target/appassembler/repo/lucene-spatial-extras-8.3.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/lucene/lucene-spatial3d/8.3.0/lucene-spatial3d-8.3.0.jar to /content/anserini/target/appassembler/repo/lucene-spatial3d-8.3.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/lucene/lucene-suggest/8.3.0/lucene-suggest-8.3.0.jar to /content/anserini/target/appassembler/repo/lucene-suggest-8.3.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/com/carrotsearch/hppc/0.8.1/hppc-0.8.1.jar to /content/anserini/target/appassembler/repo/hppc-0.8.1.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.9.9/jackson-annotations-2.9.9.jar to /content/anserini/target/appassembler/repo/jackson-annotations-2.9.9.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-smile/2.9.9/jackson-dataformat-smile-2.9.9.jar to /content/anserini/target/appassembler/repo/jackson-dataformat-smile-2.9.9.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/com/google/guava/guava/25.1-jre/guava-25.1-jre.jar to /content/anserini/target/appassembler/repo/guava-25.1-jre.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/com/tdunning/t-digest/3.1/t-digest-3.1.jar to /content/anserini/target/appassembler/repo/t-digest-3.1.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/commons-codec/commons-codec/1.11/commons-codec-1.11.jar to /content/anserini/target/appassembler/repo/commons-codec-1.11.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/commons/commons-lang3/3.8.1/commons-lang3-3.8.1.jar to /content/anserini/target/appassembler/repo/commons-lang3-3.8.1.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/elasticsearch/client/elasticsearch-rest-high-level-client/7.0.0/elasticsearch-rest-high-level-client-7.0.0.jar to /content/anserini/target/appassembler/repo/elasticsearch-rest-high-level-client-7.0.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/elasticsearch/elasticsearch/7.0.0/elasticsearch-7.0.0.jar to /content/anserini/target/appassembler/repo/elasticsearch-7.0.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/elasticsearch/elasticsearch-core/7.0.0/elasticsearch-core-7.0.0.jar to /content/anserini/target/appassembler/repo/elasticsearch-core-7.0.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/elasticsearch/elasticsearch-secure-sm/7.0.0/elasticsearch-secure-sm-7.0.0.jar to /content/anserini/target/appassembler/repo/elasticsearch-secure-sm-7.0.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/elasticsearch/elasticsearch-x-content/7.0.0/elasticsearch-x-content-7.0.0.jar to /content/anserini/target/appassembler/repo/elasticsearch-x-content-7.0.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-cbor/2.8.11/jackson-dataformat-cbor-2.8.11.jar to /content/anserini/target/appassembler/repo/jackson-dataformat-cbor-2.8.11.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/elasticsearch/elasticsearch-geo/7.0.0/elasticsearch-geo-7.0.0.jar to /content/anserini/target/appassembler/repo/elasticsearch-geo-7.0.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/lucene/lucene-spatial/8.0.0/lucene-spatial-8.0.0.jar to /content/anserini/target/appassembler/repo/lucene-spatial-8.0.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/elasticsearch/elasticsearch-cli/7.0.0/elasticsearch-cli-7.0.0.jar to /content/anserini/target/appassembler/repo/elasticsearch-cli-7.0.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/net/sf/jopt-simple/jopt-simple/5.0.2/jopt-simple-5.0.2.jar to /content/anserini/target/appassembler/repo/jopt-simple-5.0.2.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/joda-time/joda-time/2.10.1/joda-time-2.10.1.jar to /content/anserini/target/appassembler/repo/joda-time-2.10.1.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/hdrhistogram/HdrHistogram/2.1.9/HdrHistogram-2.1.9.jar to /content/anserini/target/appassembler/repo/HdrHistogram-2.1.9.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/elasticsearch/jna/4.5.1/jna-4.5.1.jar to /content/anserini/target/appassembler/repo/jna-4.5.1.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/elasticsearch/client/elasticsearch-rest-client/7.0.0/elasticsearch-rest-client-7.0.0.jar to /content/anserini/target/appassembler/repo/elasticsearch-rest-client-7.0.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/httpcomponents/httpasyncclient/4.1.4/httpasyncclient-4.1.4.jar to /content/anserini/target/appassembler/repo/httpasyncclient-4.1.4.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/httpcomponents/httpcore-nio/4.4.11/httpcore-nio-4.4.11.jar to /content/anserini/target/appassembler/repo/httpcore-nio-4.4.11.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar to /content/anserini/target/appassembler/repo/commons-logging-1.1.3.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/elasticsearch/plugin/parent-join-client/7.0.0/parent-join-client-7.0.0.jar to /content/anserini/target/appassembler/repo/parent-join-client-7.0.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/elasticsearch/plugin/aggs-matrix-stats-client/7.0.0/aggs-matrix-stats-client-7.0.0.jar to /content/anserini/target/appassembler/repo/aggs-matrix-stats-client-7.0.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/elasticsearch/plugin/rank-eval-client/7.0.0/rank-eval-client-7.0.0.jar to /content/anserini/target/appassembler/repo/rank-eval-client-7.0.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/elasticsearch/plugin/lang-mustache-client/7.0.0/lang-mustache-client-7.0.0.jar to /content/anserini/target/appassembler/repo/lang-mustache-client-7.0.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/com/github/spullara/mustache/java/compiler/0.9.3/compiler-0.9.3.jar to /content/anserini/target/appassembler/repo/compiler-0.9.3.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar to /content/anserini/target/appassembler/repo/xz-1.5.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.10.1/jackson-core-2.10.1.jar to /content/anserini/target/appassembler/repo/jackson-core-2.10.1.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.10.1/jackson-databind-2.10.1.jar to /content/anserini/target/appassembler/repo/jackson-databind-2.10.1.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/com/fasterxml/jackson/datatype/jackson-datatype-jdk8/2.10.1/jackson-datatype-jdk8-2.10.1.jar to /content/anserini/target/appassembler/repo/jackson-datatype-jdk8-2.10.1.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-yaml/2.10.1/jackson-dataformat-yaml-2.10.1.jar to /content/anserini/target/appassembler/repo/jackson-dataformat-yaml-2.10.1.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/yaml/snakeyaml/1.24/snakeyaml-1.24.jar to /content/anserini/target/appassembler/repo/snakeyaml-1.24.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/jsoup/jsoup/1.8.3/jsoup-1.8.3.jar to /content/anserini/target/appassembler/repo/jsoup-1.8.3.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/args4j/args4j/2.32/args4j-2.32.jar to /content/anserini/target/appassembler/repo/args4j-2.32.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/logging/log4j/log4j-api/2.13.3/log4j-api-2.13.3.jar to /content/anserini/target/appassembler/repo/log4j-api-2.13.3.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/logging/log4j/log4j-core/2.13.3/log4j-core-2.13.3.jar to /content/anserini/target/appassembler/repo/log4j-core-2.13.3.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/slf4j/slf4j-simple/1.7.29/slf4j-simple-1.7.29.jar to /content/anserini/target/appassembler/repo/slf4j-simple-1.7.29.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/io/anserini/anserini-fastutil/6.5.6/anserini-fastutil-6.5.6.jar to /content/anserini/target/appassembler/repo/anserini-fastutil-6.5.6.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/wikiclean/wikiclean/1.1/wikiclean-1.1.jar to /content/anserini/target/appassembler/repo/wikiclean-1.1.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/ant/ant/1.9.15/ant-1.9.15.jar to /content/anserini/target/appassembler/repo/ant-1.9.15.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/ant/ant-launcher/1.9.15/ant-launcher-1.9.15.jar to /content/anserini/target/appassembler/repo/ant-launcher-1.9.15.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/openrdf/sesame/sesame-rio-ntriples/4.1.2/sesame-rio-ntriples-4.1.2.jar to /content/anserini/target/appassembler/repo/sesame-rio-ntriples-4.1.2.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/openrdf/sesame/sesame-model/4.1.2/sesame-model-4.1.2.jar to /content/anserini/target/appassembler/repo/sesame-model-4.1.2.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/openrdf/sesame/sesame-util/4.1.2/sesame-util-4.1.2.jar to /content/anserini/target/appassembler/repo/sesame-util-4.1.2.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/openrdf/sesame/sesame-rio-api/4.1.2/sesame-rio-api-4.1.2.jar to /content/anserini/target/appassembler/repo/sesame-rio-api-4.1.2.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/openrdf/sesame/sesame-rio-datatypes/4.1.2/sesame-rio-datatypes-4.1.2.jar to /content/anserini/target/appassembler/repo/sesame-rio-datatypes-4.1.2.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/openrdf/sesame/sesame-rio-languages/4.1.2/sesame-rio-languages-4.1.2.jar to /content/anserini/target/appassembler/repo/sesame-rio-languages-4.1.2.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/com/twitter/twittertext/twitter-text/2.0.10/twitter-text-2.0.10.jar to /content/anserini/target/appassembler/repo/twitter-text-2.0.10.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/com/google/code/findbugs/jsr305/2.0.1/jsr305-2.0.1.jar to /content/anserini/target/appassembler/repo/jsr305-2.0.1.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/com/github/TREMA-UNH/trec-car-tools-java/13/trec-car-tools-java-13.jar to /content/anserini/target/appassembler/repo/trec-car-tools-java-13.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/co/nstant/in/cbor/0.7/cbor-0.7.jar to /content/anserini/target/appassembler/repo/cbor-0.7.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/jetbrains/annotations-java5/20.1.0/annotations-java5-20.1.0.jar to /content/anserini/target/appassembler/repo/annotations-java5-20.1.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/commons/commons-pool2/2.6.0/commons-pool2-2.6.0.jar to /content/anserini/target/appassembler/repo/commons-pool2-2.6.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/wikidata/wdtk/wdtk-dumpfiles/0.10.0/wdtk-dumpfiles-0.10.0.jar to /content/anserini/target/appassembler/repo/wdtk-dumpfiles-0.10.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/wikidata/wdtk/wdtk-datamodel/0.10.0/wdtk-datamodel-0.10.0.jar to /content/anserini/target/appassembler/repo/wdtk-datamodel-0.10.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/wikidata/wdtk/wdtk-util/0.10.0/wdtk-util-0.10.0.jar to /content/anserini/target/appassembler/repo/wdtk-util-0.10.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/commons/commons-compress/1.18/commons-compress-1.18.jar to /content/anserini/target/appassembler/repo/commons-compress-1.18.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/wikidata/wdtk/wdtk-storage/0.10.0/wdtk-storage-0.10.0.jar to /content/anserini/target/appassembler/repo/wdtk-storage-0.10.0.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/mockito/mockito-all/1.10.19/mockito-all-1.10.19.jar to /content/anserini/target/appassembler/repo/mockito-all-1.10.19.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/jbibtex/jbibtex/1.0.17/jbibtex-1.0.17.jar to /content/anserini/target/appassembler/repo/jbibtex-1.0.17.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /root/.m2/repository/org/apache/commons/commons-csv/1.8/commons-csv-1.8.jar to /content/anserini/target/appassembler/repo/commons-csv-1.8.jar\n",
            "[\u001b[1;34mINFO\u001b[m] Installing artifact /content/anserini/target/anserini-0.10.1-SNAPSHOT.jar to /content/anserini/target/appassembler/repo/anserini-0.10.1-SNAPSHOT.jar\n",
            "[\u001b[1;34mINFO\u001b[m] \u001b[1m------------------------------------------------------------------------\u001b[m\n",
            "[\u001b[1;34mINFO\u001b[m] \u001b[1;32mBUILD SUCCESS\u001b[m\n",
            "[\u001b[1;34mINFO\u001b[m] \u001b[1m------------------------------------------------------------------------\u001b[m\n",
            "[\u001b[1;34mINFO\u001b[m] Total time:  29.916 s\n",
            "[\u001b[1;34mINFO\u001b[m] Finished at: 2020-12-15T18:32:30Z\n",
            "[\u001b[1;34mINFO\u001b[m] \u001b[1m------------------------------------------------------------------------\u001b[m\n",
            "ApproximateNearestNeighborEval\t      IndexCollection.bat\n",
            "ApproximateNearestNeighborEval.bat    IndexReaderUtils\n",
            "ApproximateNearestNeighborSearch      IndexReaderUtils.bat\n",
            "ApproximateNearestNeighborSearch.bat  IndexVectors\n",
            "DumpAnalyzedQueries\t\t      IndexVectors.bat\n",
            "DumpAnalyzedQueries.bat\t\t      SearchCollection\n",
            "ExtractAverageDocumentLength\t      SearchCollection.bat\n",
            "ExtractAverageDocumentLength.bat      SearchElastic\n",
            "ExtractDocumentLengths\t\t      SearchElastic.bat\n",
            "ExtractDocumentLengths.bat\t      SearchMsmarco\n",
            "ExtractNorms\t\t\t      SearchMsmarco.bat\n",
            "ExtractNorms.bat\t\t      SearchSolr\n",
            "ExtractTopDfTerms\t\t      SearchSolr.bat\n",
            "ExtractTopDfTerms.bat\t\t      SimpleSearcher\n",
            "FeatureExtractorCli\t\t      SimpleSearcher.bat\n",
            "FeatureExtractorCli.bat\t\t      SimpleTweetSearcher\n",
            "IndexCollection\t\t\t      SimpleTweetSearcher.bat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Oa0JWLcSAeF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "515d0f9e-dda3-4c23-99be-a8726d05347e"
      },
      "source": [
        "#Google colab with torch 1.5 doesnt see the GPU\n",
        "!pip install -I torch==1.4.0\n",
        "import torch\n",
        "torch.cuda.get_device_name(0)  #This should ouptut a GPU device name"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.4.0\n",
            "  Using cached https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "\u001b[31mERROR: torchvision 0.8.1+cu101 has requirement torch==1.7.0, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.5.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla P4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lG57GzJWSOPY"
      },
      "source": [
        "## Downloading ClariQ data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv2XbhwqSP7O"
      },
      "source": [
        "!mkdir data\n",
        "!mkdir data/clariq\n",
        "!cd data/clariq; wget https://github.com/aliannejadi/ClariQ/raw/master/data/dev.tsv\n",
        "!cd data/clariq; wget https://github.com/aliannejadi/ClariQ/raw/master/data/train.tsv\n",
        "!cd data/clariq; wget https://github.com/aliannejadi/ClariQ/raw/master/data/question_bank.tsv\n",
        "!mv data/clariq/train.tsv data/clariq/train_original.tsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p108XEnBrROX"
      },
      "source": [
        "!rm -rf data/clariq/anserini_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isQIZq0XSljz"
      },
      "source": [
        "## Preprocess ClariQ for transformer-rankers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylAtrf1lSoP-"
      },
      "source": [
        "import pandas as pd\n",
        "data_path = \"./data/\"\n",
        "\n",
        "train = pd.read_csv(data_path+\"clariq/train_original.tsv\", sep=\"\\t\")\n",
        "valid = pd.read_csv(data_path+\"clariq/dev.tsv\", sep=\"\\t\")\n",
        "\n",
        "train = train[[\"initial_request\", \"question\"]]\n",
        "train.columns = [\"query\", \"clarifying_question\"]\n",
        "train = train[~train[\"clarifying_question\"].isnull()]\n",
        "\n",
        "valid = valid[[\"initial_request\", \"question\"]]\n",
        "valid.columns = [\"query\", \"clarifying_question\"]\n",
        "valid = valid[~valid[\"clarifying_question\"].isnull()]\n",
        "\n",
        "train.to_csv(data_path+\"clariq/train.tsv\", sep=\"\\t\", index=False)\n",
        "valid.to_csv(data_path+\"clariq/valid.tsv\", sep=\"\\t\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAEKgEf6TNzz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9d15a588-5cf8-4c36-dba4-3c615a0d9ffd"
      },
      "source": [
        "# For transformer-rankers we only need a pandas DF with query (here the initial request) \n",
        "# and relevant documents (here the clarifying questions).\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>query</th>\n",
              "      <th>clarifying_question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Tell me about Obama family tree.</td>\n",
              "      <td>are you interested in seeing barack obamas family</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tell me about Obama family tree.</td>\n",
              "      <td>would you like to know barack obamas geneology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Tell me about Obama family tree.</td>\n",
              "      <td>would you like to know about obamas ancestors</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tell me about Obama family tree.</td>\n",
              "      <td>would you like to know who is currently alive ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Tell me about Obama family tree.</td>\n",
              "      <td>are you looking for biological information on ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              query                                clarifying_question\n",
              "0  Tell me about Obama family tree.  are you interested in seeing barack obamas family\n",
              "1  Tell me about Obama family tree.     would you like to know barack obamas geneology\n",
              "2  Tell me about Obama family tree.      would you like to know about obamas ancestors\n",
              "3  Tell me about Obama family tree.  would you like to know who is currently alive ...\n",
              "4  Tell me about Obama family tree.  are you looking for biological information on ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOeCaSHOVDLK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "15423ca2-7713-4ae0-c622-42bbe33a7b6b"
      },
      "source": [
        "# We will sample negative samples for training using the question bank\n",
        "question_bank = pd.read_csv(data_path+\"clariq/question_bank.tsv\", sep=\"\\t\")\n",
        "question_bank.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question_id</th>\n",
              "      <th>question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Q00001</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Q00002</td>\n",
              "      <td>a total cholesterol of 180 to 200 mgdl 10 to 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Q00003</td>\n",
              "      <td>about how many years experience do you want th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Q00004</td>\n",
              "      <td>according to anima the bible or what other source</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Q00005</td>\n",
              "      <td>ae you looking for examples of septic system d...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  question_id                                           question\n",
              "0      Q00001                                                NaN\n",
              "1      Q00002  a total cholesterol of 180 to 200 mgdl 10 to 1...\n",
              "2      Q00003  about how many years experience do you want th...\n",
              "3      Q00004  according to anima the bible or what other source\n",
              "4      Q00005  ae you looking for examples of septic system d..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "XboApa7AqKYx",
        "outputId": "fa597a5e-8d6c-4288-d90a-eff388ade84d"
      },
      "source": [
        "import wandb\n",
        "wandb.init()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/guz/uncategorized\" target=\"_blank\">https://app.wandb.ai/guz/uncategorized</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/guz/uncategorized/runs/28q3r7ea\" target=\"_blank\">https://app.wandb.ai/guz/uncategorized/runs/28q3r7ea</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.10.12 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-12-15 18:36:48,673 [INFO] system metrics and metadata threads started\n",
            "2020-12-15 18:36:48,674 [INFO] checking resume status, waiting at most 10 seconds\n",
            "2020-12-15 18:36:48,731 [INFO] resuming run from id: UnVuOnYxOjI4cTNyN2VhOnVuY2F0ZWdvcml6ZWQ6Z3V6\n",
            "2020-12-15 18:36:48,744 [INFO] upserting run before process can begin, waiting at most 10 seconds\n",
            "2020-12-15 18:36:48,799 [INFO] saving patches\n",
            "2020-12-15 18:36:48,800 [INFO] saving pip packages\n",
            "2020-12-15 18:36:48,803 [INFO] initializing streaming files api\n",
            "2020-12-15 18:36:48,809 [INFO] unblocking file change observer, beginning sync with W&B servers\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "W&B Run: https://app.wandb.ai/guz/uncategorized/runs/28q3r7ea"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "stream",
          "text": [
            "2020-12-15 18:36:48,828 [INFO] shutting down system stats and metadata service\n",
            "2020-12-15 18:36:49,394 [INFO] file/dir modified: /content/wandb/run-20201215_183648-28q3r7ea/config.yaml\n",
            "2020-12-15 18:36:49,466 [INFO] file/dir created: /content/wandb/run-20201215_183648-28q3r7ea/wandb-history.jsonl\n",
            "2020-12-15 18:36:49,468 [INFO] file/dir created: /content/wandb/run-20201215_183648-28q3r7ea/requirements.txt\n",
            "2020-12-15 18:36:49,476 [INFO] file/dir created: /content/wandb/run-20201215_183648-28q3r7ea/wandb-metadata.json\n",
            "2020-12-15 18:36:49,487 [INFO] file/dir created: /content/wandb/run-20201215_183648-28q3r7ea/wandb-summary.json\n",
            "2020-12-15 18:36:49,490 [INFO] file/dir created: /content/wandb/run-20201215_183648-28q3r7ea/wandb-events.jsonl\n",
            "2020-12-15 18:36:49,502 [INFO] file/dir created: /content/wandb/run-20201215_183648-28q3r7ea/code\n",
            "2020-12-15 18:36:49,675 [INFO] stopping streaming files and file change observer\n",
            "2020-12-15 18:36:50,394 [INFO] file/dir modified: /content/wandb/run-20201215_183648-28q3r7ea/wandb-metadata.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAILVs8MUL9k"
      },
      "source": [
        "## Training a transformer-ranker for ClariQ (RQ2)\n",
        "\n",
        "The problem is to retrieve the most relevant clarifying question for a given query. We will train a BERT-ranker using transformer-rankers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC9XL8YvTXoE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1af95d0f-f7b8-484a-c084-c2373c616b79"
      },
      "source": [
        "from transformer_rankers.trainers import transformer_trainer\n",
        "from transformer_rankers.datasets import dataset\n",
        "from transformer_rankers.negative_samplers import negative_sampling\n",
        "from transformer_rankers.eval import results_analyses_tools\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "logging.basicConfig(\n",
        "  level=logging.INFO,\n",
        "  format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
        "  handlers=[\n",
        "      logging.StreamHandler(sys.stdout)\n",
        "  ]\n",
        ")\n",
        "\n",
        "#The combination of query and question are not that big.\n",
        "max_seq_len = 50\n",
        "\n",
        "#Lets use an almost balanced amount of positive and negative samples during training.\n",
        "average_relevant_per_query = train.groupby(\"query\").count().mean().values[0]\n",
        "\n",
        "#Instantiate BM25 negative sampler.\n",
        "# ns_train = negative_sampling.BM25NegativeSamplerPyserini(list(question_bank[\"question\"].values[1:]), 9 , \n",
        "#                     \"/content/data/clariq/anserini_train/\", -1, \"./anserini/\")\n",
        "# ns_val = negative_sampling.BM25NegativeSamplerPyserini(list(question_bank[\"question\"].values[1:]), 9, \n",
        "#                     \"/content/data/clariq/anserini_train/\", -1, \"./anserini/\")\n",
        "\n",
        "# We could also use random sampling which does not require Anserini.\n",
        "ns_train = negative_sampling.RandomNegativeSampler(list(question_bank[\"question\"].values[1:]), int(average_relevant_per_query))\n",
        "ns_val = negative_sampling.RandomNegativeSampler(list(question_bank[\"question\"].values[1:]), int(average_relevant_per_query))\n",
        "\n",
        "#Create the loaders for the dataset, with the respective negative samplers\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "dataloader = dataset.QueryDocumentDataLoader(train_df=train,\n",
        "                    val_df=valid, test_df=valid,\n",
        "                    tokenizer=tokenizer, negative_sampler_train=ns_train,\n",
        "                    negative_sampler_val=ns_val, task_type='classification',\n",
        "                    train_batch_size=8, val_batch_size=8, max_seq_len=max_seq_len,\n",
        "                    sample_data=-1, cache_path=\"./data/clariq/\")\n",
        "\n",
        "train_loader, val_loader, test_loader = dataloader.\\\n",
        "  get_pytorch_dataloaders()\n",
        "\n",
        "#Use BERT (any model that has SequenceClassification class from HuggingFace would work here)\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "#Instantiate trainer that handles fitting.\n",
        "trainer = transformer_trainer.TransformerTrainer(model=model,\n",
        "  train_loader=train_loader,\n",
        "  val_loader=val_loader, test_loader=test_loader,\n",
        "  num_ns_eval=int(average_relevant_per_query), task_type=\"classification\", tokenizer=tokenizer,\n",
        "  validate_every_epochs=1, num_validation_batches=-1,\n",
        "  num_epochs=1, lr=5e-7, sacred_ex=None)\n",
        "\n",
        "#Train (our validation eval uses the NS sampling procedure)\n",
        "trainer.fit()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-15 18:36:59,320 [INFO] Train instances per batch 8\n",
            "2020-12-15 18:36:59,361 [INFO] Generating instances with signature pointwise_set_train_n_cand_docs_45_ns_sampler_RandomNS_seq_max_l_50_sample_-1_for_classification_using_BertTokenizer\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 187/187 [00:00<00:00, 7287.93it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-12-15 18:36:59,392 [INFO] Encoding examples using tokenizer.batch_encode_plus().\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-12-15 18:37:05,937 [INFO] Transforming examples to instances format.\n",
            "2020-12-15 18:37:05,997 [INFO] Set train Instance 0 query \n",
            "\n",
            "Child support in Indiana?[...]\n",
            "\n",
            "2020-12-15 18:37:05,998 [INFO] Set train Instance 0 document \n",
            "\n",
            "are you interested in indiana child support\n",
            "\n",
            "2020-12-15 18:37:05,999 [INFO] Set train Instance 0 features \n",
            "\n",
            "InputFeatures(input_ids=[101, 2775, 2490, 1999, 5242, 1029, 102, 2024, 2017, 4699, 1999, 5242, 2775, 2490, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
            "\n",
            "2020-12-15 18:37:06,000 [INFO] Set train Instance 1 query \n",
            "\n",
            "Child support in Indiana?[...]\n",
            "\n",
            "2020-12-15 18:37:06,002 [INFO] Set train Instance 1 document \n",
            "\n",
            "which counties law would you like clarification on\n",
            "\n",
            "2020-12-15 18:37:06,003 [INFO] Set train Instance 1 features \n",
            "\n",
            "InputFeatures(input_ids=[101, 2775, 2490, 1999, 5242, 1029, 102, 2029, 5721, 2375, 2052, 2017, 2066, 18856, 8486, 10803, 2006, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
            "\n",
            "2020-12-15 18:37:06,004 [INFO] Set train Instance 2 query \n",
            "\n",
            "Child support in Indiana?[...]\n",
            "\n",
            "2020-12-15 18:37:06,007 [INFO] Set train Instance 2 document \n",
            "\n",
            "would you like a list of forms for processing child support claims\n",
            "\n",
            "2020-12-15 18:37:06,009 [INFO] Set train Instance 2 features \n",
            "\n",
            "InputFeatures(input_ids=[101, 2775, 2490, 1999, 5242, 1029, 102, 2052, 2017, 2066, 1037, 2862, 1997, 3596, 2005, 6364, 2775, 2490, 4447, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0fLLW7wV3YE"
      },
      "source": [
        "## Evaluating with ClariQ evaluation scripts\n",
        "The above code uses the transformer-ranker's built-in evaluation. This means that we are only ranking from a set of K (int(average_relevant_per_query) in the example) candidate questions, a re-ranking scenario where all the positive examples are in the candidate list. RQ2 of ClariQ requires us to rank from the entire question_bank. Additionally it evaluates whether the clarifying questions helps for document retrieval."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4BSJT7vVT01"
      },
      "source": [
        "! git clone https://github.com/aliannejadi/ClariQ.git ClariQ-repo\n",
        "! pip install rank_bm25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rguln-2txb5o"
      },
      "source": [
        "### Re-rank BM25 with the fine-tuned BERT-ranker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnToD3YFAAXK"
      },
      "source": [
        "Lets first use the bm25 [example](https://colab.research.google.com/drive/1g_Sc9j5fYT1hiOxif6BVH5NHNt-icxtT#scrollTo=_7_2LTXoXqK7) from Mohammad to generate the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HgNLYQJ1PaC"
      },
      "source": [
        "rerank_top_k = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39S7b3jvAG5P"
      },
      "source": [
        "# Imports required packages, defines stem & tokenizez function\n",
        "import pandas as pd\n",
        "from rank_bm25 import BM25Okapi\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def stem_tokenize(text, remove_stopwords=True):\n",
        "  stemmer = PorterStemmer()\n",
        "  tokens = [word for sent in nltk.sent_tokenize(text) \\\n",
        "                                      for word in nltk.word_tokenize(sent)]\n",
        "  tokens = [word for word in tokens if word not in \\\n",
        "          nltk.corpus.stopwords.words('english')]\n",
        "  return [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "# Files paths\n",
        "request_file_path = './ClariQ-repo/data/dev.tsv'\n",
        "question_bank_path = './ClariQ-repo/data/question_bank.tsv'\n",
        "run_file_path = './ClariQ-repo/sample_runs/dev_bm25'\n",
        "\n",
        "# Reads files and build bm25 corpus (index)\n",
        "dev = pd.read_csv(request_file_path, sep='\\t')\n",
        "question_bank = pd.read_csv(question_bank_path, sep='\\t').fillna('')\n",
        "question_bank['tokenized_question_list'] = question_bank['question'].map(stem_tokenize)\n",
        "question_bank['tokenized_question_str'] = question_bank['tokenized_question_list'].map(lambda x: ' '.join(x))\n",
        "bm25_corpus = question_bank['tokenized_question_list'].tolist()\n",
        "bm25 = BM25Okapi(bm25_corpus)\n",
        "\n",
        "# Runs bm25 for every query and stores output in file.\n",
        "examples = []\n",
        "all_preds_bm25 = []\n",
        "with open(run_file_path, 'w') as fo:\n",
        "  for tid in dev['topic_id'].unique():\n",
        "    query = dev.loc[dev['topic_id']==tid, 'initial_request'].tolist()[0]\n",
        "    bm25_ranked_list = bm25.get_top_n(stem_tokenize(query, True), \n",
        "                                    bm25_corpus, \n",
        "                                    n=rerank_top_k)\n",
        "    bm25_q_list = [' '.join(sent) for sent in bm25_ranked_list]\n",
        "    docs = question_bank.set_index('tokenized_question_str').loc[bm25_q_list, 'question'].tolist()\n",
        "    preds = question_bank.set_index('tokenized_question_str').loc[bm25_q_list, 'question_id'].tolist()\n",
        "    all_preds_bm25.append(preds)\n",
        "    for doc in docs[:rerank_top_k]:\n",
        "      examples.append((query, doc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EELGiVqv6_6L"
      },
      "source": [
        "Now we are going to transform this dataset in the format required for our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHBszQljAT_i"
      },
      "source": [
        "from transformers.data.data_collator import default_data_collator\n",
        "from transformers.data.processors.utils import InputFeatures\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformer_rankers.utils import utils\n",
        "\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, features):\n",
        "        self.features = features\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "    def __getitem__(self, index):\n",
        "        return self.features[index]\n",
        "\n",
        "batch_encoding = tokenizer.batch_encode_plus(examples, \n",
        "                max_length=max_seq_len, pad_to_max_length=True)\n",
        "features = []\n",
        "for i in range(len(examples)):\n",
        "    inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
        "    feature = InputFeatures(**inputs, label=0)\n",
        "    features.append(feature)\n",
        "\n",
        "dataset = SimpleDataset(features)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=False, collate_fn=default_data_collator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXPpw7FTE2x2"
      },
      "source": [
        "Now we can run the trained model on this dataset and  save the predictions to a file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVGhE-yQAvH8"
      },
      "source": [
        "logits, _, softmax_output = trainer.predict(dataloader)\n",
        "softmax_output_by_query = utils.acumulate_list(softmax_output[0], rerank_top_k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVDAJuVEAwZk"
      },
      "source": [
        "import numpy as np\n",
        "run_file_path = './ClariQ-repo/sample_runs/dev_BERT-reranker'\n",
        "with open(run_file_path, 'w') as fo:\n",
        "  for tid_idx, tid in enumerate(dev['topic_id'].unique()):\n",
        "    document_scores = np.array(softmax_output_by_query[tid_idx])\n",
        "    top_k_scores_idx = (-document_scores).argsort()[:rerank_top_k]  \n",
        "    preds = np.array(all_preds_bm25[tid_idx])[top_k_scores_idx]\n",
        "    for i, qid in enumerate(preds):\n",
        "      fo.write('{} 0 {} {} {} BERT-reranker\\n'.format(tid, qid, i, len(preds)-i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy9GH-rDCAyx"
      },
      "source": [
        "# Report question relevance performance\n",
        "! python ./ClariQ-repo/src/clariq_eval_tool.py  --eval_task question_relevance\\\n",
        "                                                --data_dir ./ClariQ-repo/data/ \\\n",
        "                                                --experiment_type dev \\\n",
        "                                                --run_file {run_file_path} \\\n",
        "                                                --out_file {run_file_path}_question_relevance.eval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEiMf2pqB0XC"
      },
      "source": [
        "! python ./ClariQ-repo/src/clariq_eval_tool.py  --eval_task document_relevance\\\n",
        "                                                --data_dir ./ClariQ-repo/data/ \\\n",
        "                                                --experiment_type dev \\\n",
        "                                                --run_file {run_file_path} \\\n",
        "                                                --out_file {run_file_path}.eval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBFowAVmtQH4"
      },
      "source": [
        "### Full retrieval\n",
        "So let's first generate a dataset containing all combinations of dev queries \n",
        "and question_bank questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB2VwOPlc1ol"
      },
      "source": [
        "from transformers.data.data_collator import default_data_collator\n",
        "from transformers.data.processors.utils import InputFeatures\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, features):\n",
        "        self.features = features\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "    def __getitem__(self, index):\n",
        "        return self.features[index]\n",
        "\n",
        "#Lets not use the null document for no question.\n",
        "all_documents = list(question_bank[\"question\"].values[1:])\n",
        "examples = []\n",
        "for tid in dev['topic_id'].unique():\n",
        "    query = dev.loc[dev['topic_id']==tid, 'initial_request'].tolist()[0]\n",
        "    for doc in all_documents:\n",
        "      examples.append((query, doc))\n",
        "\n",
        "batch_encoding = tokenizer.batch_encode_plus(examples, \n",
        "                max_length=max_seq_len, pad_to_max_length=True)\n",
        "features = []\n",
        "for i in range(len(examples)):\n",
        "    inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
        "    feature = InputFeatures(**inputs, label=0)\n",
        "    features.append(feature)\n",
        "\n",
        "dataset = SimpleDataset(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddUbNbzGt73A"
      },
      "source": [
        "Now we have to make the predictions and acumulate the logits by the number of candidate documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLF1qpmRpBZL"
      },
      "source": [
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=False, collate_fn=default_data_collator)\n",
        "from transformer_rankers.utils import utils\n",
        "logits, _, softmax_output = trainer.predict(dataloader)\n",
        "softmax_output_by_query = utils.acumulate_list(softmax_output[0], len(all_documents))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QefshgrStfja"
      },
      "source": [
        "import numpy as np\n",
        "run_file_path = './ClariQ-repo/sample_runs/dev_BERT-ranker'\n",
        "all_doc_ids = np.array(question_bank[\"question_id\"].values[1:])\n",
        "with open(run_file_path, 'w') as fo:\n",
        "  for tid_idx, tid in enumerate(dev['topic_id'].unique()):\n",
        "    all_documents_scores = np.array(softmax_output_by_query[tid_idx])\n",
        "    top_30_scores_idx = (-all_documents_scores).argsort()[:30]  \n",
        "    preds = all_doc_ids[top_30_scores_idx]\n",
        "    for i, qid in enumerate(preds):    \n",
        "      fo.write('{} 0 {} {} {} BERT-ranker\\n'.format(tid, qid, i, len(preds)-i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NlCqE7XwSh-"
      },
      "source": [
        "# Report question relevance performance\n",
        "! python ./ClariQ-repo/src/clariq_eval_tool.py  --eval_task question_relevance\\\n",
        "                                                --data_dir ./ClariQ-repo/data/ \\\n",
        "                                                --experiment_type dev \\\n",
        "                                                --run_file {run_file_path} \\\n",
        "                                                --out_file {run_file_path}_question_relevance.eval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Znqwz6ZwUXY"
      },
      "source": [
        "! python ./ClariQ-repo/src/clariq_eval_tool.py  --eval_task document_relevance\\\n",
        "                                                --data_dir ./ClariQ-repo/data/ \\\n",
        "                                                --experiment_type dev \\\n",
        "                                                --run_file {run_file_path} \\\n",
        "                                                --out_file {run_file_path}.eval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKArX9JZBRUI"
      },
      "source": [
        "## Results comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDPJh2oX_i4G"
      },
      "source": [
        "import json\n",
        "\n",
        "models = [\"bm25\", \"BERT-reranker\", \"BERT-ranker\"]\n",
        "results = []\n",
        "for model in models:\n",
        "  with open('./ClariQ-repo/sample_runs/dev_{}_question_relevance.eval'.format(model)) as f:\n",
        "    res = json.load(f)\n",
        "    for metric_name in res:\n",
        "        metric_avg = np.mean([res[metric_name][k] for k in res[metric_name]])\n",
        "        results.append([model, metric_name, metric_avg])\n",
        "res_df = pd.DataFrame(results, columns = [\"model\", \"metric\", \"value\"])\n",
        "\n",
        "pd.set_option(\"display.precision\", 4)\n",
        "res_df = res_df.set_index([\"model\", \"metric\"]).unstack()\n",
        "cols = res_df.columns.tolist()\n",
        "res_df.sort_values([(\"value\",\"Recall10\")])[cols[-1:] + cols[:-1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4SF6f7RDptU"
      },
      "source": [
        "import json\n",
        "\n",
        "models = [\"bm25\", \"BERT-reranker\", \"BERT-ranker\"]\n",
        "results = []\n",
        "for model in models:\n",
        "  with open('./ClariQ-repo/sample_runs/dev_{}.eval'.format(model)) as f:\n",
        "    res = json.load(f)\n",
        "    for metric_name in res:\n",
        "        metric_avg = np.mean([res[metric_name][k] for k in res[metric_name]])\n",
        "        results.append([model, metric_name, metric_avg])\n",
        "res_df = pd.DataFrame(results, columns = [\"model\", \"metric\", \"value\"])\n",
        "\n",
        "pd.set_option(\"display.precision\", 4)\n",
        "res_df = res_df.set_index([\"model\", \"metric\"]).unstack()\n",
        "res_df.sort_values((\"value\", \"MRR100\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eldz1QAhCrGC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}