{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DialogRPT Demo (original).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrS8i1tq7Tw5"
      },
      "source": [
        "\n",
        "<img src=\"https://github.com/golsun/DialogRPT/blob/master/doc/icon.png?raw=true\" width=\"500\">\n",
        "\n",
        "\n",
        "#DialogRPT Online Demo\n",
        "\n",
        "How likely a dialog response is upvoted by people and/or trigger more replies? This is what [DialogRPT](https://github.com/golsun/DialogRPT) is learned to predict.\n",
        "It is a set of dialog response ranking transformer-based models trained on millions of human feedback data. \n",
        "\n",
        "This demo is based on the [original implementation](https://github.com/golsun/DialogRPT). A [Demo with HuggingFace implementation](https://colab.research.google.com/drive/1cAtfkbhqsRsT59y3imjR1APw3MHDMkuV?usp=sharing) is also available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJJYHKXJaRzx"
      },
      "source": [
        "* Download the repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdqEFuPrY6os",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "1899292d-8ba2-4090-f29a-44b49ebbee96"
      },
      "source": [
        "!git clone https://github.com/golsun/DialogRPT\n",
        "%cd DialogRPT/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DialogRPT'...\n",
            "remote: Enumerating objects: 249, done.\u001b[K\n",
            "remote: Counting objects: 100% (249/249), done.\u001b[K\n",
            "remote: Compressing objects: 100% (186/186), done.\u001b[K\n",
            "remote: Total 249 (delta 147), reused 147 (delta 55), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (249/249), 277.65 KiB | 1.08 MiB/s, done.\n",
            "Resolving deltas: 100% (147/147), done.\n",
            "/content/DialogRPT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N50XCf6jeS9e"
      },
      "source": [
        "## Play with a single model\n",
        "### option1: rankers only\n",
        "In the following example, the model predicts that, given the same context \"I love NLP!\", response B is gets more upvotes than response A.\n",
        "\n",
        "|  | Response of \"I love NLP!\"  | Score |\n",
        "| :-----------: | : ----------- | :----------- : |\n",
        "|  A |  Me too! | 0.111|\n",
        "|  B |  Here’s a free textbook (URL) in case anyone needs it. | 0.613|\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sanrXqyFaZRv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "177fc4c1-38f9-4e27-a7d0-ea548f070b3d"
      },
      "source": [
        "!python src/score.py play -p=restore/updown.pth"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100% 1042301/1042301 [00:00<00:00, 2691834.25B/s]\n",
            "100% 456318/456318 [00:00<00:00, 1772148.58B/s]\n",
            "--2020-09-16 00:17:24--  https://xiagnlp2.blob.core.windows.net/dialogrpt/updown.pth\n",
            "Resolving xiagnlp2.blob.core.windows.net (xiagnlp2.blob.core.windows.net)... 52.239.160.106\n",
            "Connecting to xiagnlp2.blob.core.windows.net (xiagnlp2.blob.core.windows.net)|52.239.160.106|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1520029114 (1.4G) [application/octet-stream]\n",
            "Saving to: ‘restore/updown.pth’\n",
            "\n",
            "updown.pth          100%[===================>]   1.42G  65.6MB/s    in 23s     \n",
            "\n",
            "2020-09-16 00:17:48 (63.3 MB/s) - ‘restore/updown.pth’ saved [1520029114/1520029114]\n",
            "\n",
            "loading from restore/updown.pth\n",
            "enter empty to stop\n",
            "use `_EOS_` to delimite turns for a multi-turn context\n",
            "\n",
            "Context:  I love NLP!\n",
            "Response: Here’s a free textbook (URL) in case anyone needs it.\n",
            "score = 0.613\n",
            "\n",
            "Context:  I love NLP!\n",
            "Response: Me too!\n",
            "score = 0.111\n",
            "\n",
            "Context:  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTdLKk-6Q3p3"
      },
      "source": [
        "### Option2: generate and re-rank\n",
        "You can integrate the rankers with your generation models by reranking the candidates, and thus improve the generation quality. In the following example, response A and B are two hypotheses generated by DialoGPT. Though A is more likely to be generated (higher Generation Score), it's less interesting. Ranker helps to re-rank the more interesting one, Respone B, to top position.\n",
        "\n",
        "\n",
        "|  | Response of \"Can we restart 2020?\"  | Generation Probability | Ranker Score |\n",
        "| :-----------: | : ----------- | :----------- : |:----------- : |\n",
        "|  A |  No, we can't. | 0.314| 0.350 |\n",
        "|  B |  No, we can't. It's too late for that. We need to go back in time and start from the beginning of the universe | 0.210 | 0.506 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzvjssLiQ2Wb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "535854b2-9d88-4b24-97b5-c12f1ab38dfb"
      },
      "source": [
        "!python src/generation.py -pg=restore/medium_ft.pkl -pr=restore/updown.pth"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading from restore/updown.pth\n",
            "\n",
            "cxt:\tCan we restart 2020?\n",
            "0.506 gen 0.210 ranker 0.506\tNo, we can't. It's too late for that. We need to go back in time and start from the beginning of the universe\n",
            "0.505 gen 0.198 ranker 0.505\tNo, we can't. It's too late for that. We need to go back to 2015\n",
            "0.483 gen 0.245 ranker 0.483\tNo, we can't. It's too late for that. We need to go back in time and start from the beginning of the universe.\n",
            "0.471 gen 0.268 ranker 0.471\tNo, we can't. It's too late for that. We need to go back in time and start from the beginning.\n",
            "0.470 gen 0.251 ranker 0.470\tNo, we can't. It's too late for that. We need to go back in time and start from the start.\n",
            "0.462 gen 0.243 ranker 0.462\tNo, we can't. It's too late for that. We need to go back in time to when we started.\n",
            "0.462 gen 0.258 ranker 0.462\tNo, we can't. It's too late for that. We need to go back in time and start from the beginning of time.\n",
            "0.430 gen 0.247 ranker 0.430\tNo, we can't. It's too late for that. We need to start now.\n",
            "0.422 gen 0.261 ranker 0.422\tNo, we can't. It's too late for that. We need to go back in time.\n",
            "0.377 gen 0.265 ranker 0.377\tNo, we can't. It's too early.\n",
            "0.350 gen 0.314 ranker 0.350\tNo, we can't.\n",
            "0.345 gen 0.216 ranker 0.345\tNo, we can't\n",
            "0.343 gen 0.295 ranker 0.343\tNo, we can't. It's too late for that.\n",
            "0.333 gen 0.291 ranker 0.333\tNo, we can't. It's too late.\n",
            "0.301 gen 0.167 ranker 0.301\tWe can't.\n",
            "0.168 gen 0.143 ranker 0.168\tI'm down\n",
            "\n",
            "cxt:\t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTR03wWErf8B"
      },
      "source": [
        "## Play with an ensemble model\n",
        "In the following example, the response is scored by multiple models trained on different tasks.\n",
        "\n",
        "| Task | Description  |\n",
        "| :-----------: | :----------- |\n",
        "|  **Human feedback**  | **given a context and its two human responses, predict...** |\n",
        "| `updown`|  ... which gets more upvotes?  | \n",
        "| `width` | ... which gets more direct replies?  | \n",
        "| `depth` |  ... which gets longer follow-up thread? | \n",
        "| **Human-like** (i.e., human vs fake)  | **given a context and one human response, distinguish it with...**  |\n",
        "| `human_vs_rand` | ... a random human response  |\n",
        "| `human_vs_machine` | ... a machine generated response  | \n",
        "\n",
        "the final score is a weighted average of these models. See file `restore/ensemble.yml` for details"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMcfx5q4rij-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aa1beac7-f3dc-4968-d21f-e79a87cc1109"
      },
      "source": [
        "!python src/score.py play -p=restore/ensemble.yml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'prior': [{'name': 'human_vs_rand', 'wt': 0.5, 'path': 'restore/human_vs_rand.pth'}, {'name': 'human_vs_machine', 'wt': 0.5, 'path': 'restore/human_vs_machine.pth'}], 'cond': [{'name': 'updown', 'wt': 1, 'path': 'restore/updown.pth'}, {'name': 'depth', 'wt': 0.48, 'path': 'restore/depth.pth'}, {'name': 'width', 'wt': -0.5, 'path': 'restore/width.pth'}]}\n",
            "setting up model `human_vs_rand`\n",
            "--2020-09-16 00:33:01--  https://xiagnlp2.blob.core.windows.net/dialogrpt/human_vs_rand.pth\n",
            "Resolving xiagnlp2.blob.core.windows.net (xiagnlp2.blob.core.windows.net)... 52.239.160.106\n",
            "Connecting to xiagnlp2.blob.core.windows.net (xiagnlp2.blob.core.windows.net)|52.239.160.106|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1520029114 (1.4G) [application/octet-stream]\n",
            "Saving to: ‘restore/human_vs_rand.pth’\n",
            "\n",
            "human_vs_rand.pth   100%[===================>]   1.42G  30.7MB/s    in 33s     \n",
            "\n",
            "2020-09-16 00:33:35 (43.6 MB/s) - ‘restore/human_vs_rand.pth’ saved [1520029114/1520029114]\n",
            "\n",
            "loading from restore/human_vs_rand.pth\n",
            "setting up model `human_vs_machine`\n",
            "--2020-09-16 00:33:49--  https://xiagnlp2.blob.core.windows.net/dialogrpt/human_vs_machine.pth\n",
            "Resolving xiagnlp2.blob.core.windows.net (xiagnlp2.blob.core.windows.net)... 52.239.160.106\n",
            "Connecting to xiagnlp2.blob.core.windows.net (xiagnlp2.blob.core.windows.net)|52.239.160.106|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1520029114 (1.4G) [application/octet-stream]\n",
            "Saving to: ‘restore/human_vs_machine.pth’\n",
            "\n",
            "human_vs_machine.pt 100%[===================>]   1.42G  26.1MB/s    in 90s     \n",
            "\n",
            "2020-09-16 00:35:20 (16.1 MB/s) - ‘restore/human_vs_machine.pth’ saved [1520029114/1520029114]\n",
            "\n",
            "loading from restore/human_vs_machine.pth\n",
            "setting up model `updown`\n",
            "loading from restore/updown.pth\n",
            "setting up model `depth`\n",
            "--2020-09-16 00:36:20--  https://xiagnlp2.blob.core.windows.net/dialogrpt/depth.pth\n",
            "Resolving xiagnlp2.blob.core.windows.net (xiagnlp2.blob.core.windows.net)... 52.239.160.106\n",
            "Connecting to xiagnlp2.blob.core.windows.net (xiagnlp2.blob.core.windows.net)|52.239.160.106|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1520029114 (1.4G) [application/octet-stream]\n",
            "Saving to: ‘restore/depth.pth’\n",
            "\n",
            "depth.pth           100%[===================>]   1.42G  25.6MB/s    in 65s     \n",
            "\n",
            "2020-09-16 00:37:25 (22.2 MB/s) - ‘restore/depth.pth’ saved [1520029114/1520029114]\n",
            "\n",
            "loading from restore/depth.pth\n",
            "setting up model `width`\n",
            "--2020-09-16 00:37:35--  https://xiagnlp2.blob.core.windows.net/dialogrpt/width.pth\n",
            "Resolving xiagnlp2.blob.core.windows.net (xiagnlp2.blob.core.windows.net)... 52.239.160.106\n",
            "Connecting to xiagnlp2.blob.core.windows.net (xiagnlp2.blob.core.windows.net)|52.239.160.106|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1520029114 (1.4G) [application/octet-stream]\n",
            "Saving to: ‘restore/width.pth’\n",
            "\n",
            "width.pth           100%[===================>]   1.42G  18.5MB/s    in 50s     \n",
            "\n",
            "2020-09-16 00:38:25 (29.0 MB/s) - ‘restore/width.pth’ saved [1520029114/1520029114]\n",
            "\n",
            "loading from restore/width.pth\n",
            "enter empty to stop\n",
            "use `_EOS_` to delimite turns for a multi-turn context\n",
            "\n",
            "Context:  hello\n",
            "Response: hello\n",
            "human_vs_rand = 0.954, human_vs_machine = 0.037, updown = 0.171, depth = 0.185, width = 0.146, final = 0.095\n",
            "\n",
            "Context:  I love NLP!\n",
            "Response: Here’s a free textbook (URL) in case anyone needs it.\n",
            "human_vs_rand = 0.876, human_vs_machine = 0.687, updown = 0.613, depth = 0.497, width = 0.368, final = 0.532\n",
            "\n",
            "Context:  \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}