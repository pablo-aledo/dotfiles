{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Using tree-hugger to Enhance CodeXGLUE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot7DoEZMTdNs"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Microsoft Research Asia working together with Developer Division and Bing introduce [CodeXGLUE](https://github.com/microsoft/CodeXGLUE), a **benchmark dataset and open challenge for code intelligence**.\n",
        "\n",
        "It includes 14 datasets ([CodeSearchNet](https://github.com/github/CodeSearchNet), [Py150](https://eth-sri.github.io/py150)...) for 10 diversified code intelligence tasks. Those datasets are all created from Open Source repos. CodeXGLUE also includes baseline model implementations.\n",
        "\n",
        "CodeXGLUE is for code what ImageNet is for Computer Vision or GLUE for NLP. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**🤔 BUT**\n",
        "\n",
        "What if you want to **add your own dataset to these pre-built ones** or **test the baseline models on your code**?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjmjNVmcWso3"
      },
      "source": [
        "## tree-hugger: code pre-processing library\n",
        "\n",
        "At Codist, we recently open sourced our **code processing library** [tree-hugger](https://github.com/autosoft-dev/tree-hugger). In this tutorial we will show you how to :\n",
        "* install and set tree-hugger code processing library\n",
        "* create your own dataset similar to the Open Source dataset supplied by CodeXGLUE\n",
        "\n",
        "\n",
        "\n",
        "🏆 You can then **test the baseline model** on your own data and see how it performs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wQw_15yY1Vf"
      },
      "source": [
        "### Let's install tree-hugger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCSFPT6xYtkA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "outputId": "1ce199ec-96e4-434d-eea7-98161d3f4663"
      },
      "source": [
        "!pip install -U tree-hugger PyYAML"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tree-hugger\n",
            "  Downloading https://files.pythonhosted.org/packages/6d/26/24fb7a07b986665798f00fc02a892f97264f13b16a0f08ca2460fc78ccce/tree_hugger-0.9.2-py3-none-any.whl\n",
            "Collecting PyYAML\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 4.0MB/s \n",
            "\u001b[?25hCollecting pygit2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/35/48842d925612f773ba409067326f329c0650a1a05a65722e737021f758e7/pygit2-1.3.0-cp36-cp36m-manylinux2010_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 12.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pytest in /usr/local/lib/python3.6/dist-packages (from tree-hugger) (3.6.4)\n",
            "Collecting tree-sitter\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/52/26d11536a8fafaadabe9deeb0611abdd71e11602904f60a6debdde053e6f/tree_sitter-0.2.0.tar.gz (110kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 37.1MB/s \n",
            "\u001b[?25hCollecting cached-property\n",
            "  Downloading https://files.pythonhosted.org/packages/48/19/f2090f7dad41e225c7f2326e4cfe6fff49e57dedb5b53636c9551f86b069/cached_property-1.5.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: cffi>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from pygit2->tree-hugger) (1.14.3)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->tree-hugger) (20.2.0)\n",
            "Requirement already satisfied, skipping upgrade: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->tree-hugger) (1.4.0)\n",
            "Requirement already satisfied, skipping upgrade: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->tree-hugger) (8.5.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->tree-hugger) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->tree-hugger) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->tree-hugger) (50.3.0)\n",
            "Requirement already satisfied, skipping upgrade: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->tree-hugger) (1.9.0)\n",
            "Requirement already satisfied, skipping upgrade: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.4.0->pygit2->tree-hugger) (2.20)\n",
            "Building wheels for collected packages: PyYAML, tree-sitter\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44619 sha256=210cd124adfa8c3f0b0cc9b68589f75baa32c717349725d3adae72d671fe116a\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "  Building wheel for tree-sitter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tree-sitter: filename=tree_sitter-0.2.0-cp36-cp36m-linux_x86_64.whl size=297395 sha256=5a642588b6c6967819047de6126a183ece612cea885b1436e6ac4a4d233a44cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/a6/01/2290cc8441301a07e7717e9e03c6bbc0388f71c6bf1f7f37c1\n",
            "Successfully built PyYAML tree-sitter\n",
            "Installing collected packages: cached-property, pygit2, PyYAML, tree-sitter, tree-hugger\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.3.1 cached-property-1.5.2 pygit2-1.3.0 tree-hugger-0.9.2 tree-sitter-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7I6O9LHZBtt"
      },
      "source": [
        "### And use this command to build the necessary processing libary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Bgf9OYFTBgZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "5954596c-a032-42c7-cff8-6d457a95babc"
      },
      "source": [
        "!create_libs -c python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-09 13:28:58,710 INFO:Cloneing python repo from tree-sitter collections\n",
            "2020-10-09 13:29:10,095 INFO:Creating the library my-languages.so at /content\n",
            "2020-10-09 13:29:10,978 INFO:Finished creating library!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbRAiCdjdkqE"
      },
      "source": [
        "Now that we have all the necessary set-up done, let's download some files. For the ease of the tutorial we have created a small github example repo with a collection of files. Some of it is coming from Open Source repos and some we created as example files. \n",
        "\n",
        "Let's clone that"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sVdIc1idhcj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "a4e65413-7782-46e8-ab64-1a42f5bd7a16"
      },
      "source": [
        "!git clone https://github.com/autosoft-dev/example-files.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'example-files'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 16 (delta 2), reused 11 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (16/16), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHM0NAnCeLaf"
      },
      "source": [
        "We are going to declare a small function that will help us go over each files in a nested directory tree (like the one above we cloned) and get each file at a time. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cWsYhNyeIb5"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def check_out_path(target_path: Path):\n",
        "    \"\"\"\"\n",
        "    This function recursively yields all contents of a pathlib.Path object\n",
        "    \"\"\"\n",
        "    yield target_path\n",
        "    for file in target_path.iterdir():\n",
        "        if file.is_dir():\n",
        "            yield from check_out_path(file)\n",
        "        else:\n",
        "            yield file.absolute()\n",
        "\n",
        "\n",
        "def is_python_file(file_path: Path):\n",
        "  \"\"\"\n",
        "  This little function will help us to filter the result and keep only the python files\n",
        "  \"\"\"\n",
        "  return file_path.is_file() and file_path.suffix == \".py\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB6OniXlenyC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "a2ae5125-aa6b-47eb-f5dd-272c58af30c4"
      },
      "source": [
        "for file_path in check_out_path(Path(\"example-files\")):\n",
        "  if is_python_file(file_path):\n",
        "    print(file_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/example-files/flask_files/cli.py\n",
            "/content/example-files/inner_dir/_internal_utils.py\n",
            "/content/example-files/api.py\n",
            "/content/example-files/simple_funcs/simple_funcs.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU92tn86f93x"
      },
      "source": [
        "And now, we will define another small function, which, given a string which represents Python code will tokeize that"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud727MLOfyUY"
      },
      "source": [
        "from tokenize import tokenize\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "def tokenize_code_string(text):\n",
        "    code_tokens = []\n",
        "    for tok in tokenize(BytesIO(text.encode('utf-8')).readline):\n",
        "        if tok.string.strip() != \"\" and tok.string.strip() != \"utf-8\":\n",
        "            code_tokens.append(tok.string.strip().lower())\n",
        "    return code_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxp1hwkWgKOQ"
      },
      "source": [
        "That is all for the pre-processing. Let's use tree-hugger's powerful API in conjunction with those functions to define a dataset from those custom files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTQTIPvFexFo"
      },
      "source": [
        "# We first create our PythonParser object\n",
        "from tree_hugger.core import PythonParser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrlIUSCBgecv"
      },
      "source": [
        "pp = PythonParser(library_loc=\"/content/my-languages.so\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8Eyii31gjk1"
      },
      "source": [
        "# We will now define a dict and populate it with the necessary data in a for loop\n",
        "\n",
        "final_out_put_data = {}\n",
        "\n",
        "for file_path in check_out_path(Path(\"example-files\")):\n",
        "  if is_python_file(file_path):\n",
        "    final_out_put_data[file_path.stem] = None\n",
        "    # we use one line, super convinient tree-hugger API call to get the needed data\n",
        "    if pp.parse_file(str(file_path)):\n",
        "      temp_cache = []\n",
        "      # The following call returns a dict where each key is a name of a function\n",
        "      # And each value is a tuple, (function_body, function_docstring)\n",
        "      func_and_docstr = pp.get_all_function_bodies(strip_docstr=True)\n",
        "      for func_name, (body, docstr) in func_and_docstr.items():\n",
        "        code_tokens = tokenize_code_string(body)\n",
        "        # Let's strip out all the internal comments\n",
        "        final_code_tokens = [t for t in code_tokens if not t.startswith(\"#\")]\n",
        "        # Split the first line of docstring and remove all the tripple quotes and strip white spaces and make it lower\n",
        "        docstr_tokens = docstr.split(\"\\n\")[0].strip().replace('\"\"\"', '').replace(\"'''\", \"\").lower().split()\n",
        "        temp_cache.append({\"code\": final_code_tokens, \"docstr\": docstr_tokens})\n",
        "      # Let's add the result to the final output\n",
        "      final_out_put_data[file_path.stem] = temp_cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i9vdl2nlLQO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "outputId": "7c23ac2d-6816-49e9-d47d-34ba2d5e9b93"
      },
      "source": [
        "# And we are DONE!\n",
        "\n",
        "final_out_put_data[\"api\"][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'code': ['def',\n",
              "  'request',\n",
              "  '(',\n",
              "  'method',\n",
              "  ',',\n",
              "  'url',\n",
              "  ')',\n",
              "  ':',\n",
              "  'with',\n",
              "  'sessions',\n",
              "  '.',\n",
              "  'session',\n",
              "  '(',\n",
              "  ')',\n",
              "  'as',\n",
              "  'session',\n",
              "  ':',\n",
              "  'return',\n",
              "  'session',\n",
              "  '.',\n",
              "  'request',\n",
              "  '(',\n",
              "  'method',\n",
              "  '=',\n",
              "  'method',\n",
              "  ',',\n",
              "  'url',\n",
              "  '=',\n",
              "  'url',\n",
              "  ',',\n",
              "  '**',\n",
              "  'kwargs',\n",
              "  ')'],\n",
              " 'docstr': ['constructs',\n",
              "  'and',\n",
              "  'sends',\n",
              "  'a',\n",
              "  ':class:`request',\n",
              "  '<request>`.']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM38KwJ5mzH0"
      },
      "source": [
        "With this code, you can very easily create a dataset out of your own code files and then test the baseline models against it. \n",
        "\n",
        "\n",
        "That was easy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYpSBwH11Shw"
      },
      "source": [
        " (We are about to release `docly` a small command line tool which helps you to write function documentation for your Python code and we use the same parsing technique there as well 😀 )"
      ]
    }
  ]
}