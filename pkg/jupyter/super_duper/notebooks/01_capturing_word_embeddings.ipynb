{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-capturing-word-embeddings.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTTPhI_liU-u"
      },
      "source": [
        "# Generate word embeddings using Swivel\n",
        "\n",
        "## Overview\n",
        "\n",
        "In this notebook we show how to generate word embeddings based on the K-Cap corpus using the [Swivel algorithm](https://arxiv.org/pdf/1602.02215). In particular, we reuse the implementation included in the [Tensorflow models repo on Github](https://github.com/tensorflow/models/tree/master/research/swivel) (with some small modifications)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPx_bqtS7OCj"
      },
      "source": [
        "## Download a small text corpus\n",
        "First, let's download a corpus into our environment. We will use a small sample of the UMBC corpus that has been pre-tokenized and that we have included as part of our GitHub repository. First, we will clone the repo so we have access to it from this environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc-8xkaofomg"
      },
      "source": [
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjX5TGjMeLT0"
      },
      "source": [
        "!git clone https://github.com/hybridNLP2018/tutorial.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSNfGrTQ7qiN"
      },
      "source": [
        "The dataset comes as a zip file, so we unzip it by executing the following cell. We also define a variable pointing to the corpus file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUFY-Sl8lFDK"
      },
      "source": [
        "!unzip /content/tutorial/datasamples/umbc_t_5K.zip -d /content/tutorial/datasamples/\n",
        "input_corpus='/content/tutorial/datasamples/umbc_t_5K'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ymuxrBHQr7u"
      },
      "source": [
        "You can inspect the file using the `%less` command to print the whole input file at the bottom of the screen. It'll be quicker to just print a few lines:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43SExjL46jdZ"
      },
      "source": [
        "#%less {input_corpus}\n",
        "!head -n1 {input_corpus}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ju7vkSHcNmcS"
      },
      "source": [
        "The output above shows that the input text has already been pre-processed. \n",
        " * All words have been converted to lower-case (this will avoid having two separate words for *The* and *the*)\n",
        " * punctuation marks have been separated from words. This will avoid creating \"words\" such as \"staff.\" or \"grimm,\" in the example above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PyH6EI783Kw"
      },
      "source": [
        "## `swivel`: an algorithm for learning word embeddings\n",
        "Now that we have a corpus, we need an (implementation of an) algorithm for learning embeddings. There are various libraries and implementations for this:\n",
        "  * [word2vec](https://pypi.org/project/word2vec/) the system proposed by Mikolov that introduced many of the techniques now commonly used for learning word embeddings. It directly generates word embeddings from the text corpus by using a sliding window and trying to predict a target word based on neighbouring context words.\n",
        "  * [GloVe](https://github.com/stanfordnlp/GloVe) an alternative algorithm by Pennington, Socher and Manning. It splits the process in two steps: \n",
        "    1. calculating a word-word co-occurrence matrix \n",
        "    2. learning embeddings from this matrix\n",
        "  * [FastText](https://fasttext.cc/) is a more recent algorithm by Mikolov et al (now at Facebook) that extends the original word2vec algorithm in various ways. Among others, this algorithm takes into accout subword information.\n",
        "  \n",
        "In this tutorial we will be using [Swivel](https://github.com/tensorflow/models/tree/master/research/swivel) an algorithm similar to GloVe, which makes it easier to extend to include both words and concepts (which we will do in [notebook 03 vecsigrafo](https://colab.research.google.com/github/HybridNLP2018/tutorial/blob/master/03_vecsigrafo.ipynb)). As with GloVe, Swivel first extracts a word-word co-occurence matrix from a text corpus and then uses this matrix to learn the embeddings.\n",
        "\n",
        "The official  [Swivel](https://github.com/tensorflow/models/tree/master/research/swivel)  implementation has a few issues when running on Colaboratory, hence we have included a slightly modified version as part of the HybridNLP2018 github repository. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8A0IVDoDTU3_"
      },
      "source": [
        "%ls /content/tutorial/scripts/swivel/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xppOkdyiU-1"
      },
      "source": [
        "## Learn embeddings\n",
        "\n",
        "### Generate co-occurrence matrix using Swivel `prep`\n",
        "\n",
        "Call swivel's `prep` command to calculate the word co-occurrence matrix. We use the `%run` magic command, which runs the named python file as a program, allowing us to pass parameters as if using a command-line terminal.\n",
        "\n",
        "We set the `shard_size` to 512 since the corpus is quite small. For larger corpora we could use the standard value of 4096."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wi_izVOiU-2"
      },
      "source": [
        "coocs_path = '/content/umbc/coocs/t_5K/'\n",
        "shard_size = 512\n",
        "!python /content/tutorial/scripts/swivel/prep.py \\\n",
        "  --input=\"/content/tutorial/datasamples/umbc_t_5K\" \\\n",
        "  --output_dir=\"/content/umbc/coocs/t_5K/\" \\\n",
        "  --shard_size=512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fyTfMneQnQc"
      },
      "source": [
        "The expected output is:\n",
        "\n",
        "```\n",
        "   ... tensorflow parameters ...\n",
        "    vocabulary contains 5120 tokens\n",
        "\n",
        "    writing shard 100/100\n",
        "    done!\n",
        "```\n",
        "\n",
        "We see that first, the algorithm determined the **vocabulary** $V$, this is the list of words for which an embedding will be generated. Since the corpus is fairly small, so is the vocabulary, which consists of only about 5K words (large corpora can result in vocabularies with millions of words).\n",
        "\n",
        "The co-occurrence matrix is a sparse matrix of $|V| \\times |V|$ elements. Swivel uses shards to create submatrices of $|S| \\times |S|$, where $S$ is the shard-size specified above. In this case, we have 100 sub-matrices.\n",
        "\n",
        "All this information is stored in the output folder we specified above. It consists of  100 files, one per shard/sub-matrix and a few additional files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eiJEg8mSOlJ"
      },
      "source": [
        "%ls {coocs_path} | head -n 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMm9rXSbiU-8"
      },
      "source": [
        "\n",
        "The `prep` step does the following:\n",
        "  - it uses a basic, white space, tokenization to get sequences of tokens\n",
        "  - in a first pass through the corpus, it counts all tokens and keeps only those that have a minimum frequency (5) in the corpus. Then it keeps a multiple of the `shard_size` of that. The tokens that are kept form the **vocabulary** with size $v = |V|$.\n",
        "  - on a second pass through the corpus, it uses a sliding window to count co-occurrences between the focus token and the context tokens (similar to `word2vec`). The result is a sparse co-occurrence matrix of size $v \\times v$.\n",
        "  - for easier storage and manipulation, Swivel uses *sharding* to split the co-occurrence matrix into sub-matrices of size $s \\times s$, where $s$ is the `shard_size`.\n",
        "  ![Swivel co-occurrence matrix sharding](https://github.com/hybridNLP2018/tutorial/blob/master/images/swivel-sharding.PNG?raw=1)\n",
        "  - store the sharded co-occurrence submatrices as [protobuf files](https://developers.google.com/protocol-buffers/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9tIXHWbiU-9"
      },
      "source": [
        "## Learn embeddings from co-occurrence matrix\n",
        "With the sharded co-occurrence matrix it is now possible to learn embeddings:\n",
        " - the input is the folder with the co-occurrence matrix (protobuf files with the sparse matrix).\n",
        " - `submatrix_` rows and columns need to be the same size as the `shard_size` used in the `prep` step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyk4vjv6iU--"
      },
      "source": [
        "vec_path = '/content/umbc/vec/t_5K/'\n",
        "!python /content/tutorial/scripts/swivel/swivel.py --input_base_path={coocs_path} \\\n",
        "    --output_base_path={vec_path} \\\n",
        "    --num_epochs=40 --dim=150 \\\n",
        "    --submatrix_rows={shard_size} --submatrix_cols={shard_size}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEDrblTXiU_B"
      },
      "source": [
        "This should take a few minutes, depending on your machine.\n",
        "The result is a list of files in the specified output folder, including:\n",
        " - checkpoints of the model\n",
        " - `tsv` files for the column and row embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BOhXpZiiU_C"
      },
      "source": [
        "%ls {vec_path}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbZo4fB9WaQ6"
      },
      "source": [
        "One thing missing from the output folder is a file with just the vocabulary, which we'll need later on. We copy this file from the folder with the co-occurrenc matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCOwkAJHWN14"
      },
      "source": [
        "%cp {coocs_path}/row_vocab.txt {vec_path}vocab.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI0JCPPpiU_G"
      },
      "source": [
        "### Convert `tsv` files to `bin` file\n",
        "The `tsv` files are easy to inspect, but they take too much space and they are slow to load since we need to convert the different values to floats and pack them as vectors. Swivel offers a utility to convert the `tsv` files into a `bin`ary format. At the same time it combines the column and row embeddings into a single space (it simply adds the two vectors for each word in the vocabulary)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qn0FHiYJiU_H"
      },
      "source": [
        "!python /content/tutorial/scripts/swivel/text2bin.py --vocab={vec_path}vocab.txt --output={vec_path}vecs.bin \\\n",
        "        {vec_path}row_embedding.tsv \\\n",
        "        {vec_path}col_embedding.tsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrAWhqbViU_M"
      },
      "source": [
        "This adds the `vocab.txt` and `vecs.bin` to the folder with the vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX6hOIGqiU_M"
      },
      "source": [
        "%ls -lah {vec_path}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY-1lWr3iU_R"
      },
      "source": [
        "## Read stored binary embeddings and inspect them\n",
        "\n",
        "Swivel provides the `vecs` library which implements the basic `Vecs` class. It accepts a `vocab_file` and a file for the binary serialization of the vectors (`vecs.bin`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAB_mb9ViU_R"
      },
      "source": [
        "from tutorial.scripts.swivel import vecs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifovawLuiU_U"
      },
      "source": [
        "...and we can load existing vectors. We assume you managed to generate the embeddings by following the tutorial up to now. Note that,  due to random initialization of weight during the training step, your results may be different from the ones presented below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4l1xtJNiU_W"
      },
      "source": [
        "#uncommend the following two lines if you did not manage to train embedding above \n",
        "#!tar -xzf /content/tutorial/datasamples/umbc_swivel_vec_t_5K.tar.gz -C / \n",
        "#vec_path = /content/umbc/vec/t_5K/\n",
        "vectors = vecs.Vecs(vec_path + 'vocab.txt', \n",
        "            vec_path + 'vecs.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjgSQtsPiU_Z"
      },
      "source": [
        "We have extended the standard implementation of `swivel.vecs.Vecs` to include a method `k_neighbors`. It accepts a string with the word and an optional `k` parameter, that defaults to $10$. It returns a list of python dictionaries with fields:\n",
        "  * `word`: a word in the vocabulary that is near the input word\n",
        "  * `cosim`: the cosine similiarity between the input word and the near word.\n",
        "It's easier to display the results as a `pandas` table:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iZ9bExziU_e"
      },
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(vectors.k_neighbors('california'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfCmWTFsiU_i"
      },
      "source": [
        "pd.DataFrame(vectors.k_neighbors('knowledge'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwO2zXERiU_o"
      },
      "source": [
        "pd.DataFrame(vectors.k_neighbors('semantic'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ze0ApZMluvX"
      },
      "source": [
        "pd.DataFrame(vectors.k_neighbors('conference'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzGSaASBfeLU"
      },
      "source": [
        "The cells above should display results similar the the following (for words *california* and *conference*):\n",
        "\n",
        "|\tcosim\t| word | | cosim\t| word |\n",
        "| ---------- | -------- || ---------- | -------- |\n",
        "| 0\t1.000 |\tcalifornia ||\t1.0000\t| conference |\n",
        "| 0.5060 |\tuniversity ||\t0.4320\t| international |\n",
        "| 0.4239 |\tberkeley ||\t0.4063\t| secretariat |\n",
        "| 0.4103 |\tbarbara ||\t0.3857\t| jcdl |\n",
        "|\t0.3941 |\tsanta ||\t0.3798\t| annual |\n",
        "| 0.3899 |\tsouthern ||\t0.3708\t| conferences |\n",
        "| 0.3673 |\tuc ||\t0.3705\t| forum |\n",
        "| 0.3542 |\tjohns ||\t0.3629\t| presentations |\n",
        "| 0.3396 |\tindiana ||\t0.3601\t| workshop |\n",
        "| 0.3388 | melvy ||\t0.3580\t| ... |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAqlranRiOE9"
      },
      "source": [
        "### Compound words\n",
        "\n",
        "Note that the vocabulary only has single words, i.e. compound words are not present:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Mcdr8dIiU_v"
      },
      "source": [
        "pd.DataFrame(vectors.k_neighbors('semantic web'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNc2267PknJY"
      },
      "source": [
        "A common way to work around this issue is to use the average vector of the two individual words (of course this only works if both words are in the vocabulary):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj0fNR6Li5mh"
      },
      "source": [
        "semantic_vec = vectors.lookup('semantic')\n",
        "web_vec = vectors.lookup('web')\n",
        "semweb_vec = (semantic_vec + web_vec)/2\n",
        "pd.DataFrame(vectors.k_neighbors(semweb_vec))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJAlDOIZiU_z"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we used swivel to generate word embeddings and we explored the resulting embeddings using `k neighbors` exploration. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvn6g8WCLieL"
      },
      "source": [
        "# Optional Excercise\n",
        "\n",
        "## Create word-embeddings for texts from Project Gutenburg\n",
        "\n",
        "### Download and pre-process the corpus\n",
        "\n",
        "You can try generating new embeddings using a small `gutenberg` corpus, that is provided as part of the NLTK library. It consists of a few public-domain works published as part of the Project Gutenberg.\n",
        "\n",
        "First, we download the dataset into out environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CQi8I19iU-x"
      },
      "source": [
        "import os\n",
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "%ls '/root/nltk_data/corpora/gutenberg/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrh3N-bq72We"
      },
      "source": [
        "As you can see, the corpus consists of various books, one per file. Most word2vec implementations require you to pass a corpus as a single text file. We can issue a few commands to do this by concatenating all the `txt` files in the folder into a single `all.txt` file, which we will use later on.\n",
        "\n",
        "A couple of the files are encoded using iso-8859-1 or binary encodings, which will cause trouble later on, so we rename them to avoid including them into our corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UxGMz7ONMnf"
      },
      "source": [
        "%cd /root/nltk_data/corpora/gutenberg/\n",
        "# avoid including books with incorrect encoding\n",
        "!mv chesterton-ball.txt chesterton-ball.badenc-txt\n",
        "!mv milton-paradise.txt milton-paradise.badenc-txt\n",
        "!mv shakespeare-caesar.txt shakespeare-caesar.badenc-txt\n",
        "# now concatenate all other files into 'all.txt'\n",
        "!cat *.txt >> all.txt\n",
        "# print result\n",
        "%ls -lah '/root/nltk_data/corpora/gutenberg/all.txt'\n",
        "# go back to standard folder \n",
        "%cd /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf2qQSE98VuR"
      },
      "source": [
        "The full dataset is about 11MB."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfD_c2yYOMIt"
      },
      "source": [
        "### Learn embeddings\n",
        "\n",
        "Run the steps described above to generate embeddings for the gutenberg dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbAI-zc0OXL9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd4bvnH6OZic"
      },
      "source": [
        "### Inspect embeddings\n",
        "Use methods similar to the ones shown above to get a feeling for whether the generated embeddings have captured interesting relations between words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZksSgO2GOcnP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}