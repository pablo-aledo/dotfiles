{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Codalab Emocontext Competition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75rFx3WpZUH7"
      },
      "source": [
        "#Codalab Emocontext Language Competition.ipynb\n",
        "\n",
        "We train a model for the [Codalab Emocontext Competition](https://competitions.codalab.org/competitions/19790) that achieves an F1 score of 0.65\n",
        "on the validation set (compared to a 0.7 score which won the competition). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIBJ-vPEZNNZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "fadc2a20-06f1-4943-bc38-ea8ab946ffdb"
      },
      "source": [
        "!pip install tensorflow==2.0 -q\n",
        "!pip install tensorflow-gpu==2.0 -q\n",
        "!pip install transformers -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 86.3MB 26.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 450kB 44.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 54.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 380.8MB 42kB/s \n",
            "\u001b[K     |████████████████████████████████| 317kB 2.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 860kB 44.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 43.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 40.8MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoAMWXNFZh1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "0484a611-4ba3-4821-9b72-39fe1eacc113"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import drive \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "SEED = 0\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGdrGJ5XZk8h"
      },
      "source": [
        "#1. Download the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UKnWylFZi-Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "24372cc1-d100-44e6-c834-10e1414da7b5"
      },
      "source": [
        "#Download the data\n",
        "#Replace these lines with paths to the data for the competition which can be found here: https://competitions.codalab.org/competitions/19790\n",
        "DRIVE_FOLDER = '/content/drive/My Drive/Emocontext/'\n",
        "\n",
        "train = pd.read_csv(DRIVE_FOLDER + 'train.txt', sep= \"\\t\") \n",
        "test = pd.read_csv(DRIVE_FOLDER + 'devwithoutlabels.txt', sep= \"\\t\") \n",
        "\n",
        "print(\"Loaded training data, shape: \", train.shape)\n",
        "print(train.head().to_string())\n",
        "print(\" \")\n",
        "\n",
        "#Check for null values\n",
        "print(train.isnull().any())\n",
        "print(test.isnull().any())\n",
        "\n",
        "#Split into features and dependent variables \n",
        "train_X = train.iloc[:, 1:4].values\n",
        "test_X = test.iloc[:, 1:4].values\n",
        "train_y = train.iloc[:, 4].values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded training data, shape:  (30160, 5)\n",
            "   id                  turn1                                              turn2                      turn3   label\n",
            "0   0  Don't worry  I'm girl                       hmm how do I know if you are            What's ur name?  others\n",
            "1   1            When did I?                         saw many times i think -_-        No. I never saw you   angry\n",
            "2   2                     By                                   by Google Chrome             Where you live  others\n",
            "3   3         U r ridiculous  I might be ridiculous but I am telling the truth.  U little disgusting whore   angry\n",
            "4   4     Just for time pass                         wt do u do 4 a living then                      Maybe  others\n",
            " \n",
            "id       False\n",
            "turn1    False\n",
            "turn2    False\n",
            "turn3    False\n",
            "label    False\n",
            "dtype: bool\n",
            "id       False\n",
            "turn1    False\n",
            "turn2    False\n",
            "turn3    False\n",
            "dtype: bool\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqKutsJGysBr"
      },
      "source": [
        "#2. Retrieve and Save BERT Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UgteT-luPFL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "70c9d407-644a-4297-a34e-da82291154eb"
      },
      "source": [
        "# First we explore the sentence lengths distribution in the text to help us decide\n",
        "# how much padding to use\n",
        "import seaborn as sns\n",
        "for ix in range(3):  \n",
        "  sentence_lengths = [len(sentence) for sentence in train_X[:, ix]]\n",
        "  sns.distplot(sentence_lengths)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQHDOOQd4IDjnnAs8EJxzzgHw/wF83LHZ+knG5AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOF_pZsMj1GY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "98478bc1-3b21-42fe-f76f-d693b6a076ac"
      },
      "source": [
        "# Embeds all 3 sentences in a datapoint using BERT and then saves to a file\n",
        "import torch\n",
        "from transformers import *\n",
        "\n",
        "tokenizer = tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "model = TFBertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "input_ids = {0:[], 1:[], 2:[]}\n",
        "\n",
        "#We pad up to 150 because the plots above show that this is more than enough to \n",
        "#cover the vast majority of sentences\n",
        "pad_up_to = 150 \n",
        "\n",
        "for ix in range(train_X.shape[0]):\n",
        "  data = train_X[ix]\n",
        "  for ix, turn in enumerate(data):\n",
        "    tokens = tokenizer.encode(turn, add_special_tokens=True)\n",
        "    if len(tokens) < pad_up_to:\n",
        "      tokens.extend([0 for _ in range(pad_up_to - len(tokens))])\n",
        "    input_ids[ix].append(tokens)\n",
        "\n",
        "train_X = np.zeros((train_X.shape[0], pad_up_to, 3), dtype=int)\n",
        "for ix in range(3):\n",
        "  train_X[:, :, ix] = input_ids[ix]\n",
        "\n",
        "all_embeddings = None\n",
        "\n",
        "for ix in range(3):\n",
        "  embeddings = None\n",
        "  data = tf.data.Dataset.from_tensor_slices(input_ids[ix])\n",
        "  for batch in data.batch(512):\n",
        "    sentence_embedding = model(batch)[1]\n",
        "    if embeddings is None:\n",
        "      embeddings = sentence_embedding\n",
        "    else:\n",
        "      embeddings = np.append(embeddings, sentence_embedding, axis=0)\n",
        "    print(embeddings.shape)\n",
        "  if all_embeddings is None:\n",
        "    all_embeddings = embeddings\n",
        "  else:\n",
        "    all_embeddings = np.append(all_embeddings, embeddings, axis=1)\n",
        "  print(\"ALL embeddings \", all_embeddings.shape)\n",
        "\n",
        "all_embeddings = np.reshape(all_embeddings, (30160, 768, 3))\n",
        "np.save(\"train_bert_embeddings.npy\", all_embeddings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 213450/213450 [00:00<00:00, 1055520.08B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfx-jtf9r_gk"
      },
      "source": [
        "# Load the word embeddings (no need to do the previous cell after we've done it once before)\n",
        "train_X = np.load(DRIVE_FOLDER + \"train_bert_embeddings.npy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsjuNnz_Y1Rf"
      },
      "source": [
        "#3. Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa-PWsFbY_C4"
      },
      "source": [
        "from tensorflow.keras import Model, activations\n",
        "from tensorflow.keras.layers import Dense, Concatenate, GRU, Dropout\n",
        "\n",
        "gru_hidden_size = 50\n",
        "dropout_rate = 0.15\n",
        "\n",
        "class gru_model(Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    Model.__init__(self)\n",
        "    self.gru = GRU(units=gru_hidden_size)\n",
        "    self.fc_layer1 = Dense(50, activation=\"relu\", kernel_initializer=\"he_normal\")    \n",
        "    self.fc_layer2 = Dense(4, activation=\"softmax\", kernel_initializer=\"he_normal\")\n",
        "    self.dropout = Dropout(rate=dropout_rate)\n",
        "  \n",
        "  def call(self, x, training=True):\n",
        "    \"\"\"Forward pass for the network. Note that it expects input data in the form (batch, seq length, features)\"\"\"    \n",
        "    x = self.dropout(self.gru(x))\n",
        "    x = self.dropout(self.fc_layer1(x))\n",
        "    x = self.fc_layer2(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qmb6z0G4c8SO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1abea369-f1c6-4b3a-856b-6cd55f63f221"
      },
      "source": [
        "# We create an F1 callback so we can keep track of the validation F1 score per epoch\n",
        "from keras.callbacks import Callback\n",
        "from sklearn.metrics import f1_score\n",
        "                                          \n",
        "class F1Evaluation(Callback):\n",
        "    def __init__(self, validation_data=(), interval=1):\n",
        "        super(Callback, self).__init__()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "      y_pred = tf.argmax(model(val_X), axis=1).numpy()\n",
        "      print(\"\\n Val F1 score \", f1_score(val_y, y_pred, average='micro'))\n",
        "        \n",
        "f1score = F1Evaluation()\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)                              "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kS0yCQ2cg45U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "outputId": "13da94a4-f724-4b29-f6a8-d9f177e0d437"
      },
      "source": [
        "# We run a method similar to fasti AI's learning rate finder https://docs.fast.ai/callbacks.lr_finder.html\n",
        "# this method runs iterations at different learning rates and plots the loss. \n",
        "# Areas of downward slope then gives us the rough area in which we should choose our learning rate\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "\n",
        "def lr_finder_tf(model, x_batch, y_batch, loss_object):      \n",
        "    \"\"\"Based off of the fast AI learning rate finder - https://docs.fast.ai/callbacks.lr_finder.html \n",
        "    It works by finding the learning rate at which the gradient of the loss is the most negative\"\"\"\n",
        "    lrs = []\n",
        "    losses = []\n",
        "\n",
        "    lr = 0.0000001\n",
        "\n",
        "    while lr < 0.1:\n",
        "      lrs.append(lr)\n",
        "      with tf.GradientTape() as tape:\n",
        "            y_ = model(x_batch)\n",
        "            loss_value = loss_object(y_true=y_batch, y_pred=y_)\n",
        "      grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "      losses.append(loss_value.numpy())\n",
        "      optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "      optimizer.apply_gradients(zip(grads, model.trainable_variables))  \n",
        "      lr *= 1.2 \n",
        "\n",
        "    running_mean = []\n",
        "    for ix in range(1, len(losses) - 1):\n",
        "      mean = np.mean(losses[ix-1:ix+1])\n",
        "      running_mean.append(mean)\n",
        "\n",
        "    plt.plot(lrs[1:-1], running_mean)\n",
        "    plt.xscale('log')\n",
        "    \n",
        "\n",
        "model = gru_model()\n",
        "tr_X, val_X, tr_y, val_y = train_test_split(train_X, train_y, train_size=0.9, random_state=SEED)\n",
        "tr_X = np.swapaxes(tr_X,1,2)\n",
        "val_X = np.swapaxes(val_X,1,2)\n",
        "class_num = {\"others\": 0, \"sad\": 1, \"angry\": 2, \"happy\": 3}\n",
        "tr_y = np.vectorize(class_num.get)(tr_y)\n",
        "val_y = np.vectorize(class_num.get)(val_y)\n",
        "\n",
        "lr_finder_tf(model, tr_X[:1000, :, :], tr_y[:1000], loss_object)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEACAYAAAC57G0KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFOBFRDzq/wOOlFd5ugixggAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8TQuU8BZboe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "db1cfc0c-ac98-4b25-daf4-b125ec60f808"
      },
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 300\n",
        "learning_rate = 0.0003\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['acc'])\n",
        "model.fit(tr_X, tr_y, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(val_X, val_y), callbacks=[f1score, reduce_lr])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 27144 samples, validate on 3016 samples\n",
            "Epoch 1/300\n",
            "26912/27144 [============================>.] - ETA: 0s - loss: 1.6533 - acc: 0.4026\n",
            " Val F1 score  0.48574270557029176\n",
            "27144/27144 [==============================] - 7s 268us/sample - loss: 1.6504 - acc: 0.4031 - val_loss: 1.2463 - val_acc: 0.4857\n",
            "Epoch 2/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 1.2632 - acc: 0.4877\n",
            " Val F1 score  0.48574270557029176\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.2630 - acc: 0.4878 - val_loss: 1.2424 - val_acc: 0.4857\n",
            "Epoch 3/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 1.2430 - acc: 0.4958\n",
            " Val F1 score  0.48574270557029176\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 1.2431 - acc: 0.4957 - val_loss: 1.2314 - val_acc: 0.4857\n",
            "Epoch 4/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 1.2302 - acc: 0.4974\n",
            " Val F1 score  0.4877320954907162\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.2305 - acc: 0.4971 - val_loss: 1.2195 - val_acc: 0.4877\n",
            "Epoch 5/300\n",
            "27040/27144 [============================>.] - ETA: 0s - loss: 1.2224 - acc: 0.4996\n",
            " Val F1 score  0.498342175066313\n",
            "27144/27144 [==============================] - 5s 181us/sample - loss: 1.2225 - acc: 0.4996 - val_loss: 1.2328 - val_acc: 0.4983\n",
            "Epoch 6/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 1.2152 - acc: 0.5029\n",
            " Val F1 score  0.4897214854111406\n",
            "27144/27144 [==============================] - 5s 187us/sample - loss: 1.2149 - acc: 0.5030 - val_loss: 1.2057 - val_acc: 0.4897\n",
            "Epoch 7/300\n",
            "26848/27144 [============================>.] - ETA: 0s - loss: 1.2078 - acc: 0.5072\n",
            " Val F1 score  0.503315649867374\n",
            "27144/27144 [==============================] - 5s 193us/sample - loss: 1.2076 - acc: 0.5073 - val_loss: 1.2047 - val_acc: 0.5033\n",
            "Epoch 8/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 1.2014 - acc: 0.5102\n",
            " Val F1 score  0.5056366047745358\n",
            "27144/27144 [==============================] - 5s 191us/sample - loss: 1.2012 - acc: 0.5104 - val_loss: 1.1919 - val_acc: 0.5056\n",
            "Epoch 9/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 1.1967 - acc: 0.5122\n",
            " Val F1 score  0.5102785145888594\n",
            "27144/27144 [==============================] - 5s 195us/sample - loss: 1.1966 - acc: 0.5125 - val_loss: 1.1889 - val_acc: 0.5103\n",
            "Epoch 10/300\n",
            "26880/27144 [============================>.] - ETA: 0s - loss: 1.1949 - acc: 0.5140\n",
            " Val F1 score  0.5013262599469496\n",
            "27144/27144 [==============================] - 5s 186us/sample - loss: 1.1953 - acc: 0.5135 - val_loss: 1.1870 - val_acc: 0.5013\n",
            "Epoch 11/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 1.1876 - acc: 0.5189\n",
            " Val F1 score  0.5122679045092838\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 1.1873 - acc: 0.5191 - val_loss: 1.1810 - val_acc: 0.5123\n",
            "Epoch 12/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 1.1848 - acc: 0.5182\n",
            " Val F1 score  0.5119363395225465\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 1.1848 - acc: 0.5181 - val_loss: 1.1772 - val_acc: 0.5119\n",
            "Epoch 13/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 1.1806 - acc: 0.5205\n",
            " Val F1 score  0.5106100795755968\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 1.1805 - acc: 0.5206 - val_loss: 1.1775 - val_acc: 0.5106\n",
            "Epoch 14/300\n",
            "26912/27144 [============================>.] - ETA: 0s - loss: 1.1777 - acc: 0.5226\n",
            " Val F1 score  0.514920424403183\n",
            "27144/27144 [==============================] - 5s 181us/sample - loss: 1.1774 - acc: 0.5226 - val_loss: 1.1720 - val_acc: 0.5149\n",
            "Epoch 15/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 1.1755 - acc: 0.5209\n",
            " Val F1 score  0.5109416445623343\n",
            "27144/27144 [==============================] - 5s 181us/sample - loss: 1.1753 - acc: 0.5211 - val_loss: 1.1771 - val_acc: 0.5109\n",
            "Epoch 16/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 1.1701 - acc: 0.5254\n",
            " Val F1 score  0.5069628647214854\n",
            "27144/27144 [==============================] - 5s 180us/sample - loss: 1.1703 - acc: 0.5253 - val_loss: 1.1830 - val_acc: 0.5070\n",
            "Epoch 17/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 1.1684 - acc: 0.5242\n",
            " Val F1 score  0.5169098143236074\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 1.1684 - acc: 0.5242 - val_loss: 1.1632 - val_acc: 0.5169\n",
            "Epoch 18/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 1.1674 - acc: 0.5271\n",
            " Val F1 score  0.5162466843501327\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 1.1674 - acc: 0.5271 - val_loss: 1.1612 - val_acc: 0.5162\n",
            "Epoch 19/300\n",
            "26912/27144 [============================>.] - ETA: 0s - loss: 1.1636 - acc: 0.5270\n",
            " Val F1 score  0.521551724137931\n",
            "27144/27144 [==============================] - 5s 188us/sample - loss: 1.1634 - acc: 0.5272 - val_loss: 1.1578 - val_acc: 0.5216\n",
            "Epoch 20/300\n",
            "26976/27144 [============================>.] - ETA: 0s - loss: 1.1601 - acc: 0.5277\n",
            " Val F1 score  0.5188992042440318\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.1599 - acc: 0.5280 - val_loss: 1.1583 - val_acc: 0.5189\n",
            "Epoch 21/300\n",
            "27040/27144 [============================>.] - ETA: 0s - loss: 1.1569 - acc: 0.5282\n",
            " Val F1 score  0.5261936339522546\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 1.1568 - acc: 0.5284 - val_loss: 1.1492 - val_acc: 0.5262\n",
            "Epoch 22/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 1.1550 - acc: 0.5305\n",
            " Val F1 score  0.5179045092838196\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 1.1553 - acc: 0.5304 - val_loss: 1.1657 - val_acc: 0.5179\n",
            "Epoch 23/300\n",
            "27040/27144 [============================>.] - ETA: 0s - loss: 1.1491 - acc: 0.5294\n",
            " Val F1 score  0.523209549071618\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.1490 - acc: 0.5294 - val_loss: 1.1464 - val_acc: 0.5232\n",
            "Epoch 24/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 1.1464 - acc: 0.5309\n",
            " Val F1 score  0.5242042440318302\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.1463 - acc: 0.5312 - val_loss: 1.1402 - val_acc: 0.5242\n",
            "Epoch 25/300\n",
            "26976/27144 [============================>.] - ETA: 0s - loss: 1.1413 - acc: 0.5318\n",
            " Val F1 score  0.5235411140583555\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 1.1410 - acc: 0.5320 - val_loss: 1.1314 - val_acc: 0.5235\n",
            "Epoch 26/300\n",
            "26880/27144 [============================>.] - ETA: 0s - loss: 1.1408 - acc: 0.5313\n",
            " Val F1 score  0.5212201591511937\n",
            "27144/27144 [==============================] - 5s 186us/sample - loss: 1.1398 - acc: 0.5322 - val_loss: 1.1604 - val_acc: 0.5212\n",
            "Epoch 27/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 1.1402 - acc: 0.5324\n",
            " Val F1 score  0.5295092838196287\n",
            "27144/27144 [==============================] - 5s 181us/sample - loss: 1.1402 - acc: 0.5323 - val_loss: 1.1264 - val_acc: 0.5295\n",
            "Epoch 28/300\n",
            "26880/27144 [============================>.] - ETA: 0s - loss: 1.1346 - acc: 0.5337\n",
            " Val F1 score  0.5308355437665783\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.1342 - acc: 0.5340 - val_loss: 1.1237 - val_acc: 0.5308\n",
            "Epoch 29/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 1.1300 - acc: 0.5341\n",
            " Val F1 score  0.5261936339522546\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 1.1297 - acc: 0.5343 - val_loss: 1.1292 - val_acc: 0.5262\n",
            "Epoch 30/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 1.1283 - acc: 0.5356\n",
            " Val F1 score  0.5222148541114059\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 1.1282 - acc: 0.5357 - val_loss: 1.1441 - val_acc: 0.5222\n",
            "Epoch 31/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 1.1278 - acc: 0.5373\n",
            " Val F1 score  0.53315649867374\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.1278 - acc: 0.5373 - val_loss: 1.1111 - val_acc: 0.5332\n",
            "Epoch 32/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 1.1257 - acc: 0.5383\n",
            " Val F1 score  0.506631299734748\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 1.1257 - acc: 0.5382 - val_loss: 1.1452 - val_acc: 0.5066\n",
            "Epoch 33/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 1.1186 - acc: 0.5402\n",
            " Val F1 score  0.5268567639257294\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 1.1193 - acc: 0.5397 - val_loss: 1.1275 - val_acc: 0.5269\n",
            "Epoch 34/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 1.1193 - acc: 0.5403\n",
            " Val F1 score  0.536472148541114\n",
            "27144/27144 [==============================] - 5s 179us/sample - loss: 1.1193 - acc: 0.5402 - val_loss: 1.1102 - val_acc: 0.5365\n",
            "Epoch 35/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 1.1179 - acc: 0.5391\n",
            " Val F1 score  0.5394562334217506\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.1180 - acc: 0.5391 - val_loss: 1.1052 - val_acc: 0.5395\n",
            "Epoch 36/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 1.1105 - acc: 0.5444\n",
            " Val F1 score  0.5368037135278515\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 1.1104 - acc: 0.5443 - val_loss: 1.1027 - val_acc: 0.5368\n",
            "Epoch 37/300\n",
            "26848/27144 [============================>.] - ETA: 0s - loss: 1.1122 - acc: 0.5422\n",
            " Val F1 score  0.5381299734748011\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 1.1116 - acc: 0.5424 - val_loss: 1.1058 - val_acc: 0.5381\n",
            "Epoch 38/300\n",
            "26880/27144 [============================>.] - ETA: 0s - loss: 1.1122 - acc: 0.5422\n",
            " Val F1 score  0.5381299734748011\n",
            "27144/27144 [==============================] - 5s 179us/sample - loss: 1.1121 - acc: 0.5425 - val_loss: 1.0953 - val_acc: 0.5381\n",
            "Epoch 39/300\n",
            "26848/27144 [============================>.] - ETA: 0s - loss: 1.1091 - acc: 0.5450\n",
            " Val F1 score  0.5377984084880637\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.1095 - acc: 0.5449 - val_loss: 1.0956 - val_acc: 0.5378\n",
            "Epoch 40/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 1.1081 - acc: 0.5453\n",
            " Val F1 score  0.5391246684350133\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.1080 - acc: 0.5453 - val_loss: 1.0990 - val_acc: 0.5391\n",
            "Epoch 41/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 1.1047 - acc: 0.5449\n",
            " Val F1 score  0.5421087533156499\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 1.1049 - acc: 0.5449 - val_loss: 1.0899 - val_acc: 0.5421\n",
            "Epoch 42/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 1.1041 - acc: 0.5467\n",
            " Val F1 score  0.5450928381962865\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.1037 - acc: 0.5470 - val_loss: 1.0888 - val_acc: 0.5451\n",
            "Epoch 43/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 1.1018 - acc: 0.5466\n",
            " Val F1 score  0.5411140583554377\n",
            "27144/27144 [==============================] - 5s 186us/sample - loss: 1.1013 - acc: 0.5469 - val_loss: 1.0911 - val_acc: 0.5411\n",
            "Epoch 44/300\n",
            "27104/27144 [============================>.] - ETA: 0s - loss: 1.0962 - acc: 0.5488\n",
            " Val F1 score  0.5490716180371353\n",
            "27144/27144 [==============================] - 5s 181us/sample - loss: 1.0963 - acc: 0.5486 - val_loss: 1.0795 - val_acc: 0.5491\n",
            "Epoch 45/300\n",
            "27040/27144 [============================>.] - ETA: 0s - loss: 1.0975 - acc: 0.5491\n",
            " Val F1 score  0.5431034482758621\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 1.0977 - acc: 0.5490 - val_loss: 1.0879 - val_acc: 0.5431\n",
            "Epoch 46/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 1.0973 - acc: 0.5492\n",
            " Val F1 score  0.5182360742705571\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 1.0975 - acc: 0.5491 - val_loss: 1.1407 - val_acc: 0.5182\n",
            "Epoch 47/300\n",
            "26848/27144 [============================>.] - ETA: 0s - loss: 1.0933 - acc: 0.5499\n",
            " Val F1 score  0.5440981432360743\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 1.0933 - acc: 0.5500 - val_loss: 1.0799 - val_acc: 0.5441\n",
            "Epoch 48/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 1.0971 - acc: 0.5504\n",
            " Val F1 score  0.5437665782493368\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 1.0968 - acc: 0.5506 - val_loss: 1.0941 - val_acc: 0.5438\n",
            "Epoch 49/300\n",
            "26976/27144 [============================>.] - ETA: 0s - loss: 1.0965 - acc: 0.5476\n",
            " Val F1 score  0.5510610079575596\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.0964 - acc: 0.5477 - val_loss: 1.0787 - val_acc: 0.5511\n",
            "Epoch 50/300\n",
            "27040/27144 [============================>.] - ETA: 0s - loss: 1.0946 - acc: 0.5493\n",
            " Val F1 score  0.536472148541114\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 1.0939 - acc: 0.5498 - val_loss: 1.1018 - val_acc: 0.5365\n",
            "Epoch 51/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 1.0904 - acc: 0.5495\n",
            " Val F1 score  0.5467506631299734\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.0902 - acc: 0.5496 - val_loss: 1.0762 - val_acc: 0.5468\n",
            "Epoch 52/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 1.0812 - acc: 0.5563\n",
            " Val F1 score  0.5583554376657824\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 1.0813 - acc: 0.5562 - val_loss: 1.0676 - val_acc: 0.5584\n",
            "Epoch 53/300\n",
            "27040/27144 [============================>.] - ETA: 0s - loss: 1.0883 - acc: 0.5521\n",
            " Val F1 score  0.5368037135278515\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 1.0886 - acc: 0.5518 - val_loss: 1.1110 - val_acc: 0.5368\n",
            "Epoch 54/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 1.0885 - acc: 0.5522\n",
            " Val F1 score  0.5437665782493368\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.0885 - acc: 0.5522 - val_loss: 1.0663 - val_acc: 0.5438\n",
            "Epoch 55/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 1.0811 - acc: 0.5560\n",
            " Val F1 score  0.53315649867374\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.0810 - acc: 0.5560 - val_loss: 1.0832 - val_acc: 0.5332\n",
            "Epoch 56/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 1.0843 - acc: 0.5523\n",
            " Val F1 score  0.5550397877984085\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 1.0842 - acc: 0.5523 - val_loss: 1.0666 - val_acc: 0.5550\n",
            "Epoch 57/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 1.0772 - acc: 0.5530\n",
            " Val F1 score  0.5527188328912467\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 1.0776 - acc: 0.5528 - val_loss: 1.0640 - val_acc: 0.5527\n",
            "Epoch 58/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 1.0807 - acc: 0.5556\n",
            " Val F1 score  0.5543766578249337\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.0807 - acc: 0.5557 - val_loss: 1.0635 - val_acc: 0.5544\n",
            "Epoch 59/300\n",
            "26848/27144 [============================>.] - ETA: 0s - loss: 1.0705 - acc: 0.5611\n",
            " Val F1 score  0.5351458885941645\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 1.0703 - acc: 0.5612 - val_loss: 1.1030 - val_acc: 0.5351\n",
            "Epoch 60/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 1.0789 - acc: 0.5557\n",
            " Val F1 score  0.5474137931034483\n",
            "27144/27144 [==============================] - 5s 186us/sample - loss: 1.0786 - acc: 0.5561 - val_loss: 1.0929 - val_acc: 0.5474\n",
            "Epoch 61/300\n",
            "27040/27144 [============================>.] - ETA: 0s - loss: 1.0765 - acc: 0.5578\n",
            " Val F1 score  0.5620026525198939\n",
            "27144/27144 [==============================] - 5s 181us/sample - loss: 1.0766 - acc: 0.5574 - val_loss: 1.0591 - val_acc: 0.5620\n",
            "Epoch 62/300\n",
            "26880/27144 [============================>.] - ETA: 0s - loss: 1.0761 - acc: 0.5564\n",
            " Val F1 score  0.5590185676392573\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 1.0765 - acc: 0.5560 - val_loss: 1.0597 - val_acc: 0.5590\n",
            "Epoch 63/300\n",
            "27104/27144 [============================>.] - ETA: 0s - loss: 1.0750 - acc: 0.5571\n",
            " Val F1 score  0.5570291777188329\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.0753 - acc: 0.5569 - val_loss: 1.0504 - val_acc: 0.5570\n",
            "Epoch 64/300\n",
            "26912/27144 [============================>.] - ETA: 0s - loss: 1.0680 - acc: 0.5615\n",
            " Val F1 score  0.5596816976127321\n",
            "27144/27144 [==============================] - 5s 180us/sample - loss: 1.0681 - acc: 0.5613 - val_loss: 1.0436 - val_acc: 0.5597\n",
            "Epoch 65/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 1.0699 - acc: 0.5612\n",
            " Val F1 score  0.5576923076923077\n",
            "27144/27144 [==============================] - 5s 181us/sample - loss: 1.0698 - acc: 0.5612 - val_loss: 1.0578 - val_acc: 0.5577\n",
            "Epoch 66/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 1.0713 - acc: 0.5604\n",
            " Val F1 score  0.5633289124668435\n",
            "27144/27144 [==============================] - 5s 181us/sample - loss: 1.0713 - acc: 0.5602 - val_loss: 1.0449 - val_acc: 0.5633\n",
            "Epoch 67/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 1.0664 - acc: 0.5621\n",
            " Val F1 score  0.5600132625994695\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.0660 - acc: 0.5623 - val_loss: 1.0452 - val_acc: 0.5600\n",
            "Epoch 68/300\n",
            "27040/27144 [============================>.] - ETA: 0s - loss: 1.0678 - acc: 0.5604\n",
            " Val F1 score  0.5537135278514589\n",
            "27144/27144 [==============================] - 5s 194us/sample - loss: 1.0676 - acc: 0.5605 - val_loss: 1.0726 - val_acc: 0.5537\n",
            "Epoch 69/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 1.0633 - acc: 0.5600\n",
            " Val F1 score  0.5663129973474801\n",
            "27144/27144 [==============================] - 5s 186us/sample - loss: 1.0634 - acc: 0.5597 - val_loss: 1.0426 - val_acc: 0.5663\n",
            "Epoch 70/300\n",
            "27104/27144 [============================>.] - ETA: 0s - loss: 1.0635 - acc: 0.5623\n",
            " Val F1 score  0.5712864721485411\n",
            "27144/27144 [==============================] - 5s 187us/sample - loss: 1.0634 - acc: 0.5624 - val_loss: 1.0427 - val_acc: 0.5713\n",
            "Epoch 71/300\n",
            "26880/27144 [============================>.] - ETA: 0s - loss: 1.0619 - acc: 0.5635\n",
            " Val F1 score  0.5603448275862069\n",
            "27144/27144 [==============================] - 5s 197us/sample - loss: 1.0623 - acc: 0.5637 - val_loss: 1.0511 - val_acc: 0.5603\n",
            "Epoch 72/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 1.0606 - acc: 0.5631\n",
            " Val F1 score  0.5610079575596817\n",
            "27144/27144 [==============================] - 5s 192us/sample - loss: 1.0601 - acc: 0.5633 - val_loss: 1.0515 - val_acc: 0.5610\n",
            "Epoch 73/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 1.0584 - acc: 0.5639\n",
            " Val F1 score  0.5391246684350133\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.0584 - acc: 0.5638 - val_loss: 1.0763 - val_acc: 0.5391\n",
            "Epoch 74/300\n",
            "27040/27144 [============================>.] - ETA: 0s - loss: 1.0540 - acc: 0.5667\n",
            " Val F1 score  0.5732758620689655\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 1.0542 - acc: 0.5665 - val_loss: 1.0298 - val_acc: 0.5733\n",
            "Epoch 75/300\n",
            "26912/27144 [============================>.] - ETA: 0s - loss: 1.0582 - acc: 0.5635\n",
            " Val F1 score  0.4976790450928382\n",
            "27144/27144 [==============================] - 5s 186us/sample - loss: 1.0587 - acc: 0.5632 - val_loss: 1.1396 - val_acc: 0.4977\n",
            "Epoch 76/300\n",
            "26976/27144 [============================>.] - ETA: 0s - loss: 1.0536 - acc: 0.5679\n",
            " Val F1 score  0.5613395225464191\n",
            "27144/27144 [==============================] - 5s 186us/sample - loss: 1.0541 - acc: 0.5676 - val_loss: 1.0337 - val_acc: 0.5613\n",
            "Epoch 77/300\n",
            "27104/27144 [============================>.] - ETA: 0s - loss: 1.0544 - acc: 0.5650\n",
            " Val F1 score  0.5649867374005305\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 1.0541 - acc: 0.5652 - val_loss: 1.0307 - val_acc: 0.5650\n",
            "Epoch 78/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 1.0507 - acc: 0.5678\n",
            " Val F1 score  0.5686339522546419\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.0514 - acc: 0.5670 - val_loss: 1.0304 - val_acc: 0.5686\n",
            "Epoch 79/300\n",
            "26912/27144 [============================>.] - ETA: 0s - loss: 1.0543 - acc: 0.5648\n",
            " Val F1 score  0.5593501326259946\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.0539 - acc: 0.5647 - val_loss: 1.0450 - val_acc: 0.5594\n",
            "Epoch 80/300\n",
            "26976/27144 [============================>.] - ETA: 0s - loss: 1.0346 - acc: 0.5734\n",
            " Val F1 score  0.5759283819628647\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.0341 - acc: 0.5737 - val_loss: 1.0203 - val_acc: 0.5759\n",
            "Epoch 81/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 1.0345 - acc: 0.5743\n",
            " Val F1 score  0.5666445623342176\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.0344 - acc: 0.5743 - val_loss: 1.0202 - val_acc: 0.5666\n",
            "Epoch 82/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 1.0328 - acc: 0.5731\n",
            " Val F1 score  0.5779177718832891\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.0328 - acc: 0.5732 - val_loss: 1.0118 - val_acc: 0.5779\n",
            "Epoch 83/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 1.0309 - acc: 0.5775\n",
            " Val F1 score  0.5749336870026526\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 1.0306 - acc: 0.5773 - val_loss: 1.0115 - val_acc: 0.5749\n",
            "Epoch 84/300\n",
            "26880/27144 [============================>.] - ETA: 0s - loss: 1.0299 - acc: 0.5752\n",
            " Val F1 score  0.5785809018567639\n",
            "27144/27144 [==============================] - 5s 181us/sample - loss: 1.0293 - acc: 0.5754 - val_loss: 1.0128 - val_acc: 0.5786\n",
            "Epoch 85/300\n",
            "26976/27144 [============================>.] - ETA: 0s - loss: 1.0301 - acc: 0.5770\n",
            " Val F1 score  0.5809018567639257\n",
            "27144/27144 [==============================] - 5s 186us/sample - loss: 1.0300 - acc: 0.5771 - val_loss: 1.0136 - val_acc: 0.5809\n",
            "Epoch 86/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 1.0295 - acc: 0.5723\n",
            " Val F1 score  0.5785809018567639\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 1.0302 - acc: 0.5720 - val_loss: 1.0081 - val_acc: 0.5786\n",
            "Epoch 87/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 1.0262 - acc: 0.5753\n",
            " Val F1 score  0.5726127320954907\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 1.0262 - acc: 0.5753 - val_loss: 1.0156 - val_acc: 0.5726\n",
            "Epoch 88/300\n",
            "26880/27144 [============================>.] - ETA: 0s - loss: 1.0288 - acc: 0.5764\n",
            " Val F1 score  0.5852122015915119\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 1.0290 - acc: 0.5761 - val_loss: 1.0053 - val_acc: 0.5852\n",
            "Epoch 89/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 1.0264 - acc: 0.5777\n",
            " Val F1 score  0.5769230769230769\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 1.0265 - acc: 0.5776 - val_loss: 1.0122 - val_acc: 0.5769\n",
            "Epoch 90/300\n",
            "26912/27144 [============================>.] - ETA: 0s - loss: 1.0245 - acc: 0.5770\n",
            " Val F1 score  0.5772546419098143\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 1.0242 - acc: 0.5771 - val_loss: 1.0104 - val_acc: 0.5773\n",
            "Epoch 91/300\n",
            "26976/27144 [============================>.] - ETA: 0s - loss: 1.0275 - acc: 0.5779\n",
            " Val F1 score  0.5838859416445623\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.0275 - acc: 0.5777 - val_loss: 1.0026 - val_acc: 0.5839\n",
            "Epoch 92/300\n",
            "26848/27144 [============================>.] - ETA: 0s - loss: 1.0243 - acc: 0.5789\n",
            " Val F1 score  0.5812334217506632\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 1.0248 - acc: 0.5788 - val_loss: 1.0013 - val_acc: 0.5812\n",
            "Epoch 93/300\n",
            "26912/27144 [============================>.] - ETA: 0s - loss: 1.0212 - acc: 0.5791\n",
            " Val F1 score  0.5666445623342176\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.0217 - acc: 0.5788 - val_loss: 1.0194 - val_acc: 0.5666\n",
            "Epoch 94/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 1.0229 - acc: 0.5799\n",
            " Val F1 score  0.5603448275862069\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 1.0231 - acc: 0.5797 - val_loss: 1.0218 - val_acc: 0.5603\n",
            "Epoch 95/300\n",
            "26976/27144 [============================>.] - ETA: 0s - loss: 1.0228 - acc: 0.5784\n",
            " Val F1 score  0.5842175066312998\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 1.0225 - acc: 0.5784 - val_loss: 1.0081 - val_acc: 0.5842\n",
            "Epoch 96/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 1.0205 - acc: 0.5765\n",
            " Val F1 score  0.5699602122015915\n",
            "27144/27144 [==============================] - 5s 181us/sample - loss: 1.0202 - acc: 0.5768 - val_loss: 1.0163 - val_acc: 0.5700\n",
            "Epoch 97/300\n",
            "27040/27144 [============================>.] - ETA: 0s - loss: 1.0215 - acc: 0.5807\n",
            " Val F1 score  0.5792440318302388\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 1.0212 - acc: 0.5810 - val_loss: 1.0037 - val_acc: 0.5792\n",
            "Epoch 98/300\n",
            "27040/27144 [============================>.] - ETA: 0s - loss: 1.0170 - acc: 0.5808\n",
            " Val F1 score  0.5739389920424404\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 1.0168 - acc: 0.5810 - val_loss: 1.0006 - val_acc: 0.5739\n",
            "Epoch 99/300\n",
            "27040/27144 [============================>.] - ETA: 0s - loss: 1.0194 - acc: 0.5821\n",
            " Val F1 score  0.5828912466843501\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 1.0194 - acc: 0.5820 - val_loss: 1.0004 - val_acc: 0.5829\n",
            "Epoch 100/300\n",
            "27040/27144 [============================>.] - ETA: 0s - loss: 1.0196 - acc: 0.5780\n",
            " Val F1 score  0.5779177718832891\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.0193 - acc: 0.5782 - val_loss: 1.0057 - val_acc: 0.5779\n",
            "Epoch 101/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 1.0162 - acc: 0.5816\n",
            " Val F1 score  0.5838859416445623\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 1.0163 - acc: 0.5817 - val_loss: 0.9918 - val_acc: 0.5839\n",
            "Epoch 102/300\n",
            "27104/27144 [============================>.] - ETA: 0s - loss: 1.0169 - acc: 0.5816\n",
            " Val F1 score  0.5881962864721485\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 1.0170 - acc: 0.5814 - val_loss: 0.9909 - val_acc: 0.5882\n",
            "Epoch 103/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 1.0154 - acc: 0.5819\n",
            " Val F1 score  0.5862068965517241\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 1.0155 - acc: 0.5819 - val_loss: 1.0024 - val_acc: 0.5862\n",
            "Epoch 104/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 1.0153 - acc: 0.5806\n",
            " Val F1 score  0.5832228116710876\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 1.0153 - acc: 0.5806 - val_loss: 0.9943 - val_acc: 0.5832\n",
            "Epoch 105/300\n",
            "27040/27144 [============================>.] - ETA: 0s - loss: 1.0160 - acc: 0.5824\n",
            " Val F1 score  0.5676392572944297\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 1.0161 - acc: 0.5820 - val_loss: 1.0273 - val_acc: 0.5676\n",
            "Epoch 106/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 1.0168 - acc: 0.5799\n",
            " Val F1 score  0.5875331564986738\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 1.0168 - acc: 0.5800 - val_loss: 0.9893 - val_acc: 0.5875\n",
            "Epoch 107/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 1.0143 - acc: 0.5790\n",
            " Val F1 score  0.5845490716180372\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 1.0141 - acc: 0.5792 - val_loss: 1.0076 - val_acc: 0.5845\n",
            "Epoch 108/300\n",
            "26976/27144 [============================>.] - ETA: 0s - loss: 1.0155 - acc: 0.5810\n",
            " Val F1 score  0.5852122015915119\n",
            "27144/27144 [==============================] - 5s 180us/sample - loss: 1.0153 - acc: 0.5812 - val_loss: 0.9924 - val_acc: 0.5852\n",
            "Epoch 109/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 1.0118 - acc: 0.5801\n",
            " Val F1 score  0.5759283819628647\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 1.0117 - acc: 0.5802 - val_loss: 0.9966 - val_acc: 0.5759\n",
            "Epoch 110/300\n",
            "26880/27144 [============================>.] - ETA: 0s - loss: 1.0102 - acc: 0.5818\n",
            " Val F1 score  0.5792440318302388\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 1.0104 - acc: 0.5816 - val_loss: 0.9920 - val_acc: 0.5792\n",
            "Epoch 111/300\n",
            "26880/27144 [============================>.] - ETA: 0s - loss: 1.0084 - acc: 0.5864\n",
            " Val F1 score  0.5848806366047745\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 1.0087 - acc: 0.5865 - val_loss: 0.9893 - val_acc: 0.5849\n",
            "Epoch 112/300\n",
            "26848/27144 [============================>.] - ETA: 0s - loss: 1.0138 - acc: 0.5825\n",
            " Val F1 score  0.5845490716180372\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 1.0134 - acc: 0.5827 - val_loss: 0.9915 - val_acc: 0.5845\n",
            "Epoch 113/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 1.0141 - acc: 0.5819\n",
            " Val F1 score  0.5666445623342176\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 1.0142 - acc: 0.5818 - val_loss: 1.0334 - val_acc: 0.5666\n",
            "Epoch 114/300\n",
            "26976/27144 [============================>.] - ETA: 0s - loss: 1.0103 - acc: 0.5811\n",
            " Val F1 score  0.5868700265251989\n",
            "27144/27144 [==============================] - 5s 186us/sample - loss: 1.0102 - acc: 0.5812 - val_loss: 0.9867 - val_acc: 0.5869\n",
            "Epoch 115/300\n",
            "26880/27144 [============================>.] - ETA: 0s - loss: 1.0077 - acc: 0.5850\n",
            " Val F1 score  0.5848806366047745\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.0082 - acc: 0.5852 - val_loss: 0.9869 - val_acc: 0.5849\n",
            "Epoch 116/300\n",
            "26880/27144 [============================>.] - ETA: 0s - loss: 1.0031 - acc: 0.5877\n",
            " Val F1 score  0.5457559681697612\n",
            "27144/27144 [==============================] - 5s 181us/sample - loss: 1.0041 - acc: 0.5872 - val_loss: 1.0484 - val_acc: 0.5458\n",
            "Epoch 117/300\n",
            "26848/27144 [============================>.] - ETA: 0s - loss: 1.0131 - acc: 0.5855\n",
            " Val F1 score  0.5815649867374005\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.0131 - acc: 0.5855 - val_loss: 0.9889 - val_acc: 0.5816\n",
            "Epoch 118/300\n",
            "27104/27144 [============================>.] - ETA: 0s - loss: 1.0084 - acc: 0.5836\n",
            " Val F1 score  0.5855437665782494\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 1.0081 - acc: 0.5837 - val_loss: 0.9920 - val_acc: 0.5855\n",
            "Epoch 119/300\n",
            "27040/27144 [============================>.] - ETA: 0s - loss: 1.0035 - acc: 0.5857\n",
            " Val F1 score  0.5872015915119363\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 1.0037 - acc: 0.5855 - val_loss: 0.9868 - val_acc: 0.5872\n",
            "Epoch 120/300\n",
            "26976/27144 [============================>.] - ETA: 0s - loss: 1.0071 - acc: 0.5860\n",
            " Val F1 score  0.5905172413793104\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 1.0067 - acc: 0.5863 - val_loss: 0.9792 - val_acc: 0.5905\n",
            "Epoch 121/300\n",
            "26848/27144 [============================>.] - ETA: 0s - loss: 1.0036 - acc: 0.5867\n",
            " Val F1 score  0.5911803713527851\n",
            "27144/27144 [==============================] - 5s 180us/sample - loss: 1.0035 - acc: 0.5871 - val_loss: 0.9766 - val_acc: 0.5912\n",
            "Epoch 122/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 1.0064 - acc: 0.5859\n",
            " Val F1 score  0.5921750663129973\n",
            "27144/27144 [==============================] - 5s 181us/sample - loss: 1.0061 - acc: 0.5863 - val_loss: 0.9833 - val_acc: 0.5922\n",
            "Epoch 123/300\n",
            "26880/27144 [============================>.] - ETA: 0s - loss: 1.0053 - acc: 0.5868\n",
            " Val F1 score  0.5938328912466844\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 1.0051 - acc: 0.5868 - val_loss: 0.9858 - val_acc: 0.5938\n",
            "Epoch 124/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 1.0033 - acc: 0.5853\n",
            " Val F1 score  0.5971485411140584\n",
            "27144/27144 [==============================] - 5s 179us/sample - loss: 1.0031 - acc: 0.5854 - val_loss: 0.9750 - val_acc: 0.5971\n",
            "Epoch 125/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 0.9996 - acc: 0.5887\n",
            " Val F1 score  0.5958222811671088\n",
            "27144/27144 [==============================] - 5s 181us/sample - loss: 0.9996 - acc: 0.5886 - val_loss: 0.9731 - val_acc: 0.5958\n",
            "Epoch 126/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 1.0005 - acc: 0.5875\n",
            " Val F1 score  0.5825596816976127\n",
            "27144/27144 [==============================] - 5s 181us/sample - loss: 1.0002 - acc: 0.5878 - val_loss: 0.9791 - val_acc: 0.5826\n",
            "Epoch 127/300\n",
            "26976/27144 [============================>.] - ETA: 0s - loss: 0.9985 - acc: 0.5891\n",
            " Val F1 score  0.5878647214854111\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9983 - acc: 0.5890 - val_loss: 0.9761 - val_acc: 0.5879\n",
            "Epoch 128/300\n",
            "26976/27144 [============================>.] - ETA: 0s - loss: 0.9988 - acc: 0.5880\n",
            " Val F1 score  0.5845490716180372\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 0.9987 - acc: 0.5879 - val_loss: 0.9766 - val_acc: 0.5845\n",
            "Epoch 129/300\n",
            "26912/27144 [============================>.] - ETA: 0s - loss: 0.9971 - acc: 0.5867\n",
            " Val F1 score  0.5944960212201591\n",
            "27144/27144 [==============================] - 5s 189us/sample - loss: 0.9975 - acc: 0.5868 - val_loss: 0.9742 - val_acc: 0.5945\n",
            "Epoch 130/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 0.9979 - acc: 0.5884\n",
            " Val F1 score  0.5891909814323607\n",
            "27144/27144 [==============================] - 5s 191us/sample - loss: 0.9980 - acc: 0.5883 - val_loss: 0.9884 - val_acc: 0.5892\n",
            "Epoch 131/300\n",
            "26848/27144 [============================>.] - ETA: 0s - loss: 0.9971 - acc: 0.5898\n",
            " Val F1 score  0.5901856763925729\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9973 - acc: 0.5897 - val_loss: 0.9771 - val_acc: 0.5902\n",
            "Epoch 132/300\n",
            "26976/27144 [============================>.] - ETA: 0s - loss: 0.9978 - acc: 0.5871\n",
            " Val F1 score  0.593501326259947\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9976 - acc: 0.5871 - val_loss: 0.9801 - val_acc: 0.5935\n",
            "Epoch 133/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 0.9965 - acc: 0.5893\n",
            " Val F1 score  0.5974801061007957\n",
            "27144/27144 [==============================] - 5s 197us/sample - loss: 0.9967 - acc: 0.5892 - val_loss: 0.9698 - val_acc: 0.5975\n",
            "Epoch 134/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 0.9962 - acc: 0.5917\n",
            " Val F1 score  0.5858753315649867\n",
            "27144/27144 [==============================] - 5s 198us/sample - loss: 0.9957 - acc: 0.5919 - val_loss: 0.9753 - val_acc: 0.5859\n",
            "Epoch 135/300\n",
            "27040/27144 [============================>.] - ETA: 0s - loss: 0.9961 - acc: 0.5920\n",
            " Val F1 score  0.588527851458886\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9966 - acc: 0.5918 - val_loss: 0.9765 - val_acc: 0.5885\n",
            "Epoch 136/300\n",
            "27104/27144 [============================>.] - ETA: 0s - loss: 0.9955 - acc: 0.5882\n",
            " Val F1 score  0.5878647214854111\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 0.9954 - acc: 0.5882 - val_loss: 0.9739 - val_acc: 0.5879\n",
            "Epoch 137/300\n",
            "26848/27144 [============================>.] - ETA: 0s - loss: 0.9944 - acc: 0.5911\n",
            " Val F1 score  0.5911803713527851\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9933 - acc: 0.5917 - val_loss: 0.9726 - val_acc: 0.5912\n",
            "Epoch 138/300\n",
            "27040/27144 [============================>.] - ETA: 0s - loss: 0.9922 - acc: 0.5902\n",
            " Val F1 score  0.5998010610079576\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9926 - acc: 0.5901 - val_loss: 0.9689 - val_acc: 0.5998\n",
            "Epoch 139/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 0.9926 - acc: 0.5895\n",
            " Val F1 score  0.5911803713527851\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9928 - acc: 0.5897 - val_loss: 0.9710 - val_acc: 0.5912\n",
            "Epoch 140/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 0.9915 - acc: 0.5905\n",
            " Val F1 score  0.5702917771883289\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 0.9916 - acc: 0.5904 - val_loss: 1.0140 - val_acc: 0.5703\n",
            "Epoch 141/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 0.9900 - acc: 0.5935\n",
            " Val F1 score  0.5789124668435013\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 0.9901 - acc: 0.5935 - val_loss: 0.9887 - val_acc: 0.5789\n",
            "Epoch 142/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 0.9949 - acc: 0.5894\n",
            " Val F1 score  0.5961538461538461\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9949 - acc: 0.5894 - val_loss: 0.9674 - val_acc: 0.5962\n",
            "Epoch 143/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 0.9910 - acc: 0.5930\n",
            " Val F1 score  0.5898541114058355\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9912 - acc: 0.5928 - val_loss: 0.9747 - val_acc: 0.5899\n",
            "Epoch 144/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 0.9884 - acc: 0.5922\n",
            " Val F1 score  0.5888594164456233\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9883 - acc: 0.5922 - val_loss: 0.9650 - val_acc: 0.5889\n",
            "Epoch 145/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 0.9904 - acc: 0.5944\n",
            " Val F1 score  0.5925066312997348\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9905 - acc: 0.5943 - val_loss: 0.9721 - val_acc: 0.5925\n",
            "Epoch 146/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 0.9883 - acc: 0.5931\n",
            " Val F1 score  0.5958222811671088\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9883 - acc: 0.5930 - val_loss: 0.9638 - val_acc: 0.5958\n",
            "Epoch 147/300\n",
            "26880/27144 [============================>.] - ETA: 0s - loss: 0.9885 - acc: 0.5932\n",
            " Val F1 score  0.5898541114058355\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 0.9884 - acc: 0.5933 - val_loss: 0.9674 - val_acc: 0.5899\n",
            "Epoch 148/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 0.9910 - acc: 0.5907\n",
            " Val F1 score  0.5908488063660478\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 0.9906 - acc: 0.5910 - val_loss: 0.9728 - val_acc: 0.5908\n",
            "Epoch 149/300\n",
            "26848/27144 [============================>.] - ETA: 0s - loss: 0.9847 - acc: 0.5959\n",
            " Val F1 score  0.5815649867374005\n",
            "27144/27144 [==============================] - 5s 181us/sample - loss: 0.9849 - acc: 0.5960 - val_loss: 0.9730 - val_acc: 0.5816\n",
            "Epoch 150/300\n",
            "27104/27144 [============================>.] - ETA: 0s - loss: 0.9862 - acc: 0.5942\n",
            " Val F1 score  0.6024535809018567\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9864 - acc: 0.5940 - val_loss: 0.9745 - val_acc: 0.6025\n",
            "Epoch 151/300\n",
            "27104/27144 [============================>.] - ETA: 0s - loss: 0.9856 - acc: 0.5938\n",
            " Val F1 score  0.5988063660477454\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 0.9855 - acc: 0.5939 - val_loss: 0.9640 - val_acc: 0.5988\n",
            "Epoch 152/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 0.9854 - acc: 0.5931\n",
            " Val F1 score  0.5656498673740054\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9852 - acc: 0.5932 - val_loss: 1.0084 - val_acc: 0.5656\n",
            "Epoch 153/300\n",
            "26880/27144 [============================>.] - ETA: 0s - loss: 0.9835 - acc: 0.5959\n",
            " Val F1 score  0.5964854111405835\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9838 - acc: 0.5955 - val_loss: 0.9624 - val_acc: 0.5965\n",
            "Epoch 154/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 0.9801 - acc: 0.5966\n",
            " Val F1 score  0.596816976127321\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9804 - acc: 0.5964 - val_loss: 0.9720 - val_acc: 0.5968\n",
            "Epoch 155/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 0.9824 - acc: 0.5971\n",
            " Val F1 score  0.5875331564986738\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9824 - acc: 0.5971 - val_loss: 0.9720 - val_acc: 0.5875\n",
            "Epoch 156/300\n",
            "26848/27144 [============================>.] - ETA: 0s - loss: 0.9831 - acc: 0.5948\n",
            " Val F1 score  0.5898541114058355\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 0.9836 - acc: 0.5949 - val_loss: 0.9616 - val_acc: 0.5899\n",
            "Epoch 157/300\n",
            "26880/27144 [============================>.] - ETA: 0s - loss: 0.9844 - acc: 0.5919\n",
            " Val F1 score  0.5998010610079576\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9846 - acc: 0.5919 - val_loss: 0.9734 - val_acc: 0.5998\n",
            "Epoch 158/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 0.9825 - acc: 0.5972\n",
            " Val F1 score  0.6061007957559682\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 0.9823 - acc: 0.5973 - val_loss: 0.9613 - val_acc: 0.6061\n",
            "Epoch 159/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 0.9810 - acc: 0.5973\n",
            " Val F1 score  0.583554376657825\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9810 - acc: 0.5972 - val_loss: 0.9839 - val_acc: 0.5836\n",
            "Epoch 160/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 0.9803 - acc: 0.5976\n",
            " Val F1 score  0.6024535809018567\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9806 - acc: 0.5977 - val_loss: 0.9567 - val_acc: 0.6025\n",
            "Epoch 161/300\n",
            "27104/27144 [============================>.] - ETA: 0s - loss: 0.9789 - acc: 0.5966\n",
            " Val F1 score  0.5958222811671088\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9791 - acc: 0.5966 - val_loss: 0.9563 - val_acc: 0.5958\n",
            "Epoch 162/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 0.9797 - acc: 0.5967\n",
            " Val F1 score  0.6011273209549072\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9793 - acc: 0.5968 - val_loss: 0.9546 - val_acc: 0.6011\n",
            "Epoch 163/300\n",
            "26976/27144 [============================>.] - ETA: 0s - loss: 0.9739 - acc: 0.5996\n",
            " Val F1 score  0.5964854111405835\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 0.9738 - acc: 0.5998 - val_loss: 0.9560 - val_acc: 0.5965\n",
            "Epoch 164/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 0.9783 - acc: 0.5983\n",
            " Val F1 score  0.5931697612732095\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9787 - acc: 0.5980 - val_loss: 0.9568 - val_acc: 0.5932\n",
            "Epoch 165/300\n",
            "26976/27144 [============================>.] - ETA: 0s - loss: 0.9786 - acc: 0.5933\n",
            " Val F1 score  0.5858753315649867\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9784 - acc: 0.5935 - val_loss: 0.9806 - val_acc: 0.5859\n",
            "Epoch 166/300\n",
            "26976/27144 [============================>.] - ETA: 0s - loss: 0.9771 - acc: 0.5985\n",
            " Val F1 score  0.600132625994695\n",
            "27144/27144 [==============================] - 5s 181us/sample - loss: 0.9763 - acc: 0.5992 - val_loss: 0.9533 - val_acc: 0.6001\n",
            "Epoch 167/300\n",
            "27104/27144 [============================>.] - ETA: 0s - loss: 0.9768 - acc: 0.5960\n",
            " Val F1 score  0.6104111405835544\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 0.9767 - acc: 0.5961 - val_loss: 0.9501 - val_acc: 0.6104\n",
            "Epoch 168/300\n",
            "26976/27144 [============================>.] - ETA: 0s - loss: 0.9783 - acc: 0.6005\n",
            " Val F1 score  0.5865384615384616\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 0.9780 - acc: 0.6005 - val_loss: 0.9891 - val_acc: 0.5865\n",
            "Epoch 169/300\n",
            "27040/27144 [============================>.] - ETA: 0s - loss: 0.9774 - acc: 0.5982\n",
            " Val F1 score  0.5941644562334217\n",
            "27144/27144 [==============================] - 5s 181us/sample - loss: 0.9777 - acc: 0.5982 - val_loss: 0.9669 - val_acc: 0.5942\n",
            "Epoch 170/300\n",
            "26880/27144 [============================>.] - ETA: 0s - loss: 0.9745 - acc: 0.6004\n",
            " Val F1 score  0.6057692307692307\n",
            "27144/27144 [==============================] - 5s 181us/sample - loss: 0.9746 - acc: 0.6003 - val_loss: 0.9515 - val_acc: 0.6058\n",
            "Epoch 171/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 0.9742 - acc: 0.5993\n",
            " Val F1 score  0.5981432360742706\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9742 - acc: 0.5994 - val_loss: 0.9540 - val_acc: 0.5981\n",
            "Epoch 172/300\n",
            "26976/27144 [============================>.] - ETA: 0s - loss: 0.9757 - acc: 0.5979\n",
            " Val F1 score  0.5994694960212201\n",
            "27144/27144 [==============================] - 5s 181us/sample - loss: 0.9754 - acc: 0.5978 - val_loss: 0.9538 - val_acc: 0.5995\n",
            "Epoch 173/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 0.9754 - acc: 0.5989\n",
            " Val F1 score  0.6057692307692307\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 0.9756 - acc: 0.5988 - val_loss: 0.9475 - val_acc: 0.6058\n",
            "Epoch 174/300\n",
            "26912/27144 [============================>.] - ETA: 0s - loss: 0.9756 - acc: 0.5979\n",
            " Val F1 score  0.5941644562334217\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9748 - acc: 0.5981 - val_loss: 0.9679 - val_acc: 0.5942\n",
            "Epoch 175/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 0.9703 - acc: 0.6000\n",
            " Val F1 score  0.5998010610079576\n",
            "27144/27144 [==============================] - 5s 181us/sample - loss: 0.9702 - acc: 0.5999 - val_loss: 0.9521 - val_acc: 0.5998\n",
            "Epoch 176/300\n",
            "26880/27144 [============================>.] - ETA: 0s - loss: 0.9724 - acc: 0.6008\n",
            " Val F1 score  0.6041114058355438\n",
            "27144/27144 [==============================] - 5s 181us/sample - loss: 0.9721 - acc: 0.6007 - val_loss: 0.9505 - val_acc: 0.6041\n",
            "Epoch 177/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 0.9696 - acc: 0.6010\n",
            " Val F1 score  0.603448275862069\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 0.9693 - acc: 0.6009 - val_loss: 0.9396 - val_acc: 0.6034\n",
            "Epoch 178/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 0.9702 - acc: 0.6004\n",
            " Val F1 score  0.6024535809018567\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9700 - acc: 0.6005 - val_loss: 0.9461 - val_acc: 0.6025\n",
            "Epoch 179/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 0.9731 - acc: 0.5988\n",
            " Val F1 score  0.6044429708222812\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 0.9728 - acc: 0.5987 - val_loss: 0.9457 - val_acc: 0.6044\n",
            "Epoch 180/300\n",
            "26976/27144 [============================>.] - ETA: 0s - loss: 0.9735 - acc: 0.5983\n",
            " Val F1 score  0.5981432360742706\n",
            "27144/27144 [==============================] - 5s 181us/sample - loss: 0.9731 - acc: 0.5984 - val_loss: 0.9442 - val_acc: 0.5981\n",
            "Epoch 181/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 0.9703 - acc: 0.6035\n",
            " Val F1 score  0.5908488063660478\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9704 - acc: 0.6031 - val_loss: 0.9535 - val_acc: 0.5908\n",
            "Epoch 182/300\n",
            "26848/27144 [============================>.] - ETA: 0s - loss: 0.9658 - acc: 0.6007\n",
            " Val F1 score  0.6097480106100795\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9656 - acc: 0.6011 - val_loss: 0.9415 - val_acc: 0.6097\n",
            "Epoch 183/300\n",
            "26880/27144 [============================>.] - ETA: 0s - loss: 0.9700 - acc: 0.6006\n",
            " Val F1 score  0.588527851458886\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 0.9700 - acc: 0.6008 - val_loss: 0.9720 - val_acc: 0.5885\n",
            "Epoch 184/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 0.9675 - acc: 0.5996\n",
            " Val F1 score  0.6061007957559682\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9676 - acc: 0.5998 - val_loss: 0.9383 - val_acc: 0.6061\n",
            "Epoch 185/300\n",
            "27040/27144 [============================>.] - ETA: 0s - loss: 0.9635 - acc: 0.6039\n",
            " Val F1 score  0.6021220159151194\n",
            "27144/27144 [==============================] - 5s 181us/sample - loss: 0.9641 - acc: 0.6038 - val_loss: 0.9427 - val_acc: 0.6021\n",
            "Epoch 186/300\n",
            "26976/27144 [============================>.] - ETA: 0s - loss: 0.9641 - acc: 0.6032\n",
            " Val F1 score  0.5941644562334217\n",
            "27144/27144 [==============================] - 5s 180us/sample - loss: 0.9651 - acc: 0.6025 - val_loss: 0.9633 - val_acc: 0.5942\n",
            "Epoch 187/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 0.9612 - acc: 0.6048\n",
            " Val F1 score  0.6064323607427056\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9611 - acc: 0.6048 - val_loss: 0.9358 - val_acc: 0.6064\n",
            "Epoch 188/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 0.9684 - acc: 0.6027\n",
            " Val F1 score  0.596816976127321\n",
            "27144/27144 [==============================] - 5s 180us/sample - loss: 0.9690 - acc: 0.6023 - val_loss: 0.9663 - val_acc: 0.5968\n",
            "Epoch 189/300\n",
            "26848/27144 [============================>.] - ETA: 0s - loss: 0.9665 - acc: 0.6010\n",
            " Val F1 score  0.6027851458885941\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 0.9658 - acc: 0.6016 - val_loss: 0.9478 - val_acc: 0.6028\n",
            "Epoch 190/300\n",
            "26880/27144 [============================>.] - ETA: 0s - loss: 0.9665 - acc: 0.6042\n",
            " Val F1 score  0.6067639257294429\n",
            "27144/27144 [==============================] - 5s 186us/sample - loss: 0.9666 - acc: 0.6043 - val_loss: 0.9488 - val_acc: 0.6068\n",
            "Epoch 191/300\n",
            "27104/27144 [============================>.] - ETA: 0s - loss: 0.9621 - acc: 0.6051\n",
            " Val F1 score  0.5998010610079576\n",
            "27144/27144 [==============================] - 5s 198us/sample - loss: 0.9621 - acc: 0.6051 - val_loss: 0.9385 - val_acc: 0.5998\n",
            "Epoch 192/300\n",
            "26912/27144 [============================>.] - ETA: 0s - loss: 0.9643 - acc: 0.6052\n",
            " Val F1 score  0.6170424403183024\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9636 - acc: 0.6053 - val_loss: 0.9367 - val_acc: 0.6170\n",
            "Epoch 193/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 0.9649 - acc: 0.6045\n",
            " Val F1 score  0.6213527851458885\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9651 - acc: 0.6044 - val_loss: 0.9333 - val_acc: 0.6214\n",
            "Epoch 194/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 0.9631 - acc: 0.6051\n",
            " Val F1 score  0.571949602122016\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 0.9633 - acc: 0.6052 - val_loss: 0.9927 - val_acc: 0.5719\n",
            "Epoch 195/300\n",
            "27104/27144 [============================>.] - ETA: 0s - loss: 0.9617 - acc: 0.6065\n",
            " Val F1 score  0.60842175066313\n",
            "27144/27144 [==============================] - 5s 194us/sample - loss: 0.9616 - acc: 0.6066 - val_loss: 0.9374 - val_acc: 0.6084\n",
            "Epoch 196/300\n",
            "26976/27144 [============================>.] - ETA: 0s - loss: 0.9634 - acc: 0.6046\n",
            " Val F1 score  0.6127320954907162\n",
            "27144/27144 [==============================] - 5s 195us/sample - loss: 0.9635 - acc: 0.6044 - val_loss: 0.9347 - val_acc: 0.6127\n",
            "Epoch 197/300\n",
            "27040/27144 [============================>.] - ETA: 0s - loss: 0.9624 - acc: 0.6044\n",
            " Val F1 score  0.6160477453580901\n",
            "27144/27144 [==============================] - 5s 188us/sample - loss: 0.9626 - acc: 0.6042 - val_loss: 0.9365 - val_acc: 0.6160\n",
            "Epoch 198/300\n",
            "26848/27144 [============================>.] - ETA: 0s - loss: 0.9581 - acc: 0.6055\n",
            " Val F1 score  0.5994694960212201\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9581 - acc: 0.6055 - val_loss: 0.9418 - val_acc: 0.5995\n",
            "Epoch 199/300\n",
            "26848/27144 [============================>.] - ETA: 0s - loss: 0.9558 - acc: 0.6078\n",
            " Val F1 score  0.6187002652519894\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9561 - acc: 0.6078 - val_loss: 0.9343 - val_acc: 0.6187\n",
            "Epoch 200/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 0.9571 - acc: 0.6049\n",
            " Val F1 score  0.6104111405835544\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 0.9574 - acc: 0.6047 - val_loss: 0.9441 - val_acc: 0.6104\n",
            "Epoch 201/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 0.9561 - acc: 0.6071\n",
            " Val F1 score  0.5696286472148541\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9566 - acc: 0.6068 - val_loss: 0.9963 - val_acc: 0.5696\n",
            "Epoch 202/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 0.9571 - acc: 0.6065\n",
            " Val F1 score  0.6027851458885941\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 0.9571 - acc: 0.6065 - val_loss: 0.9599 - val_acc: 0.6028\n",
            "Epoch 203/300\n",
            "27040/27144 [============================>.] - ETA: 0s - loss: 0.9576 - acc: 0.6062\n",
            " Val F1 score  0.6057692307692307\n",
            "27144/27144 [==============================] - 5s 180us/sample - loss: 0.9573 - acc: 0.6065 - val_loss: 0.9293 - val_acc: 0.6058\n",
            "Epoch 204/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 0.9556 - acc: 0.6066\n",
            " Val F1 score  0.6087533156498673\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9560 - acc: 0.6062 - val_loss: 0.9358 - val_acc: 0.6088\n",
            "Epoch 205/300\n",
            "26848/27144 [============================>.] - ETA: 0s - loss: 0.9546 - acc: 0.6082\n",
            " Val F1 score  0.6127320954907162\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9537 - acc: 0.6088 - val_loss: 0.9342 - val_acc: 0.6127\n",
            "Epoch 206/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 0.9499 - acc: 0.6107\n",
            " Val F1 score  0.5954907161803713\n",
            "27144/27144 [==============================] - 5s 181us/sample - loss: 0.9498 - acc: 0.6109 - val_loss: 0.9653 - val_acc: 0.5955\n",
            "Epoch 207/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 0.9545 - acc: 0.6079\n",
            " Val F1 score  0.6067639257294429\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9545 - acc: 0.6079 - val_loss: 0.9383 - val_acc: 0.6068\n",
            "Epoch 208/300\n",
            "26912/27144 [============================>.] - ETA: 0s - loss: 0.9544 - acc: 0.6076\n",
            " Val F1 score  0.6137267904509284\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 0.9536 - acc: 0.6078 - val_loss: 0.9303 - val_acc: 0.6137\n",
            "Epoch 209/300\n",
            "27104/27144 [============================>.] - ETA: 0s - loss: 0.9536 - acc: 0.6120\n",
            " Val F1 score  0.6163793103448276\n",
            "27144/27144 [==============================] - 5s 186us/sample - loss: 0.9535 - acc: 0.6118 - val_loss: 0.9307 - val_acc: 0.6164\n",
            "Epoch 210/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 0.9526 - acc: 0.6092\n",
            " Val F1 score  0.6120689655172413\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9526 - acc: 0.6093 - val_loss: 0.9236 - val_acc: 0.6121\n",
            "Epoch 211/300\n",
            "27040/27144 [============================>.] - ETA: 0s - loss: 0.9519 - acc: 0.6102\n",
            " Val F1 score  0.6097480106100795\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9519 - acc: 0.6103 - val_loss: 0.9395 - val_acc: 0.6097\n",
            "Epoch 212/300\n",
            "26976/27144 [============================>.] - ETA: 0s - loss: 0.9540 - acc: 0.6074\n",
            " Val F1 score  0.6147214854111406\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 0.9544 - acc: 0.6074 - val_loss: 0.9328 - val_acc: 0.6147\n",
            "Epoch 213/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 0.9497 - acc: 0.6103\n",
            " Val F1 score  0.611737400530504\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9502 - acc: 0.6102 - val_loss: 0.9251 - val_acc: 0.6117\n",
            "Epoch 214/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 0.9512 - acc: 0.6123\n",
            " Val F1 score  0.620026525198939\n",
            "27144/27144 [==============================] - 5s 186us/sample - loss: 0.9512 - acc: 0.6123 - val_loss: 0.9221 - val_acc: 0.6200\n",
            "Epoch 215/300\n",
            "26848/27144 [============================>.] - ETA: 0s - loss: 0.9489 - acc: 0.6115\n",
            " Val F1 score  0.6124005305039788\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9498 - acc: 0.6113 - val_loss: 0.9176 - val_acc: 0.6124\n",
            "Epoch 216/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 0.9478 - acc: 0.6116\n",
            " Val F1 score  0.618368700265252\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9485 - acc: 0.6113 - val_loss: 0.9263 - val_acc: 0.6184\n",
            "Epoch 217/300\n",
            "27104/27144 [============================>.] - ETA: 0s - loss: 0.9472 - acc: 0.6129\n",
            " Val F1 score  0.616710875331565\n",
            "27144/27144 [==============================] - 5s 186us/sample - loss: 0.9473 - acc: 0.6131 - val_loss: 0.9285 - val_acc: 0.6167\n",
            "Epoch 218/300\n",
            "27040/27144 [============================>.] - ETA: 0s - loss: 0.9509 - acc: 0.6100\n",
            " Val F1 score  0.6087533156498673\n",
            "27144/27144 [==============================] - 5s 186us/sample - loss: 0.9508 - acc: 0.6099 - val_loss: 0.9460 - val_acc: 0.6088\n",
            "Epoch 219/300\n",
            "26912/27144 [============================>.] - ETA: 0s - loss: 0.9497 - acc: 0.6102\n",
            " Val F1 score  0.6080901856763926\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 0.9490 - acc: 0.6105 - val_loss: 0.9441 - val_acc: 0.6081\n",
            "Epoch 220/300\n",
            "26848/27144 [============================>.] - ETA: 0s - loss: 0.9469 - acc: 0.6119\n",
            " Val F1 score  0.6090848806366048\n",
            "27144/27144 [==============================] - 5s 181us/sample - loss: 0.9475 - acc: 0.6115 - val_loss: 0.9357 - val_acc: 0.6091\n",
            "Epoch 221/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 0.9455 - acc: 0.6113\n",
            " Val F1 score  0.5984748010610079\n",
            "27144/27144 [==============================] - 5s 187us/sample - loss: 0.9461 - acc: 0.6110 - val_loss: 0.9517 - val_acc: 0.5985\n",
            "Epoch 222/300\n",
            "26880/27144 [============================>.] - ETA: 0s - loss: 0.9460 - acc: 0.6114\n",
            " Val F1 score  0.6057692307692307\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 0.9465 - acc: 0.6112 - val_loss: 0.9438 - val_acc: 0.6058\n",
            "Epoch 223/300\n",
            "26912/27144 [============================>.] - ETA: 0s - loss: 0.9423 - acc: 0.6127\n",
            " Val F1 score  0.6110742705570292\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9424 - acc: 0.6127 - val_loss: 0.9252 - val_acc: 0.6111\n",
            "Epoch 224/300\n",
            "27040/27144 [============================>.] - ETA: 0s - loss: 0.9460 - acc: 0.6116\n",
            " Val F1 score  0.6064323607427056\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9456 - acc: 0.6117 - val_loss: 0.9411 - val_acc: 0.6064\n",
            "Epoch 225/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 0.9478 - acc: 0.6118\n",
            " Val F1 score  0.6187002652519894\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9478 - acc: 0.6118 - val_loss: 0.9150 - val_acc: 0.6187\n",
            "Epoch 226/300\n",
            "26912/27144 [============================>.] - ETA: 0s - loss: 0.9452 - acc: 0.6142\n",
            " Val F1 score  0.6170424403183024\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9455 - acc: 0.6142 - val_loss: 0.9212 - val_acc: 0.6170\n",
            "Epoch 227/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 0.9460 - acc: 0.6110\n",
            " Val F1 score  0.6110742705570292\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 0.9460 - acc: 0.6109 - val_loss: 0.9307 - val_acc: 0.6111\n",
            "Epoch 228/300\n",
            "27104/27144 [============================>.] - ETA: 0s - loss: 0.9421 - acc: 0.6150\n",
            " Val F1 score  0.6226790450928382\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9418 - acc: 0.6152 - val_loss: 0.9330 - val_acc: 0.6227\n",
            "Epoch 229/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 0.9415 - acc: 0.6157\n",
            " Val F1 score  0.6286472148541115\n",
            "27144/27144 [==============================] - 5s 186us/sample - loss: 0.9419 - acc: 0.6156 - val_loss: 0.9152 - val_acc: 0.6286\n",
            "Epoch 230/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 0.9406 - acc: 0.6160\n",
            " Val F1 score  0.6143899204244032\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 0.9404 - acc: 0.6161 - val_loss: 0.9410 - val_acc: 0.6144\n",
            "Epoch 231/300\n",
            "26880/27144 [============================>.] - ETA: 0s - loss: 0.9413 - acc: 0.6150\n",
            " Val F1 score  0.6213527851458885\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9411 - acc: 0.6148 - val_loss: 0.9273 - val_acc: 0.6214\n",
            "Epoch 232/300\n",
            "27104/27144 [============================>.] - ETA: 0s - loss: 0.9429 - acc: 0.6155\n",
            " Val F1 score  0.6157161803713528\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9431 - acc: 0.6153 - val_loss: 0.9266 - val_acc: 0.6157\n",
            "Epoch 233/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 0.9426 - acc: 0.6141\n",
            " Val F1 score  0.6140583554376657\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9419 - acc: 0.6144 - val_loss: 0.9231 - val_acc: 0.6141\n",
            "Epoch 234/300\n",
            "26912/27144 [============================>.] - ETA: 0s - loss: 0.9421 - acc: 0.6129\n",
            " Val F1 score  0.620026525198939\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9432 - acc: 0.6122 - val_loss: 0.9213 - val_acc: 0.6200\n",
            "Epoch 235/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 0.9379 - acc: 0.6180\n",
            " Val F1 score  0.6206896551724138\n",
            "27144/27144 [==============================] - 5s 186us/sample - loss: 0.9381 - acc: 0.6180 - val_loss: 0.9194 - val_acc: 0.6207\n",
            "Epoch 236/300\n",
            "27040/27144 [============================>.] - ETA: 0s - loss: 0.9392 - acc: 0.6167\n",
            " Val F1 score  0.626657824933687\n",
            "27144/27144 [==============================] - 5s 181us/sample - loss: 0.9398 - acc: 0.6164 - val_loss: 0.9081 - val_acc: 0.6267\n",
            "Epoch 237/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 0.9408 - acc: 0.6140\n",
            " Val F1 score  0.616710875331565\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9404 - acc: 0.6142 - val_loss: 0.9174 - val_acc: 0.6167\n",
            "Epoch 238/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 0.9414 - acc: 0.6138\n",
            " Val F1 score  0.6303050397877984\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9415 - acc: 0.6138 - val_loss: 0.9137 - val_acc: 0.6303\n",
            "Epoch 239/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 0.9385 - acc: 0.6161\n",
            " Val F1 score  0.6173740053050398\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9384 - acc: 0.6160 - val_loss: 0.9185 - val_acc: 0.6174\n",
            "Epoch 240/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 0.9396 - acc: 0.6147\n",
            " Val F1 score  0.6259946949602122\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 0.9401 - acc: 0.6142 - val_loss: 0.9107 - val_acc: 0.6260\n",
            "Epoch 241/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 0.9401 - acc: 0.6152\n",
            " Val F1 score  0.6256631299734748\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 0.9401 - acc: 0.6151 - val_loss: 0.9152 - val_acc: 0.6257\n",
            "Epoch 242/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 0.9348 - acc: 0.6142\n",
            " Val F1 score  0.6236737400530504\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9344 - acc: 0.6145 - val_loss: 0.9097 - val_acc: 0.6237\n",
            "Epoch 243/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 0.9392 - acc: 0.6169\n",
            " Val F1 score  0.6180371352785146\n",
            "27144/27144 [==============================] - 5s 186us/sample - loss: 0.9399 - acc: 0.6166 - val_loss: 0.9210 - val_acc: 0.6180\n",
            "Epoch 244/300\n",
            "27104/27144 [============================>.] - ETA: 0s - loss: 0.9350 - acc: 0.6174\n",
            " Val F1 score  0.6256631299734748\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 0.9346 - acc: 0.6176 - val_loss: 0.9088 - val_acc: 0.6257\n",
            "Epoch 245/300\n",
            "26880/27144 [============================>.] - ETA: 0s - loss: 0.9389 - acc: 0.6143\n",
            " Val F1 score  0.6206896551724138\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9388 - acc: 0.6142 - val_loss: 0.9081 - val_acc: 0.6207\n",
            "Epoch 246/300\n",
            "26880/27144 [============================>.] - ETA: 0s - loss: 0.9372 - acc: 0.6161\n",
            " Val F1 score  0.6173740053050398\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9370 - acc: 0.6160 - val_loss: 0.9119 - val_acc: 0.6174\n",
            "Epoch 247/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 0.9348 - acc: 0.6174\n",
            " Val F1 score  0.6230106100795756\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9346 - acc: 0.6177 - val_loss: 0.9296 - val_acc: 0.6230\n",
            "Epoch 248/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 0.9339 - acc: 0.6184\n",
            " Val F1 score  0.6140583554376657\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 0.9341 - acc: 0.6183 - val_loss: 0.9067 - val_acc: 0.6141\n",
            "Epoch 249/300\n",
            "27104/27144 [============================>.] - ETA: 0s - loss: 0.9353 - acc: 0.6190\n",
            " Val F1 score  0.6170424403183024\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9354 - acc: 0.6189 - val_loss: 0.9148 - val_acc: 0.6170\n",
            "Epoch 250/300\n",
            "26880/27144 [============================>.] - ETA: 0s - loss: 0.9338 - acc: 0.6194\n",
            " Val F1 score  0.5991379310344828\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 0.9338 - acc: 0.6195 - val_loss: 0.9449 - val_acc: 0.5991\n",
            "Epoch 251/300\n",
            "27104/27144 [============================>.] - ETA: 0s - loss: 0.9350 - acc: 0.6181\n",
            " Val F1 score  0.6206896551724138\n",
            "27144/27144 [==============================] - 5s 187us/sample - loss: 0.9349 - acc: 0.6183 - val_loss: 0.9006 - val_acc: 0.6207\n",
            "Epoch 252/300\n",
            "27104/27144 [============================>.] - ETA: 0s - loss: 0.9296 - acc: 0.6206\n",
            " Val F1 score  0.6153846153846154\n",
            "27144/27144 [==============================] - 5s 197us/sample - loss: 0.9297 - acc: 0.6207 - val_loss: 0.9089 - val_acc: 0.6154\n",
            "Epoch 253/300\n",
            "26848/27144 [============================>.] - ETA: 0s - loss: 0.9296 - acc: 0.6202\n",
            " Val F1 score  0.6236737400530504\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 0.9302 - acc: 0.6203 - val_loss: 0.9019 - val_acc: 0.6237\n",
            "Epoch 254/300\n",
            "26912/27144 [============================>.] - ETA: 0s - loss: 0.9329 - acc: 0.6212\n",
            " Val F1 score  0.6130636604774535\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 0.9325 - acc: 0.6214 - val_loss: 0.9084 - val_acc: 0.6131\n",
            "Epoch 255/300\n",
            "26912/27144 [============================>.] - ETA: 0s - loss: 0.9328 - acc: 0.6183\n",
            " Val F1 score  0.625\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 0.9327 - acc: 0.6184 - val_loss: 0.8990 - val_acc: 0.6250\n",
            "Epoch 256/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 0.9282 - acc: 0.6229\n",
            " Val F1 score  0.6223474801061007\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9277 - acc: 0.6229 - val_loss: 0.8947 - val_acc: 0.6223\n",
            "Epoch 257/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 0.9304 - acc: 0.6199\n",
            " Val F1 score  0.623342175066313\n",
            "27144/27144 [==============================] - 5s 193us/sample - loss: 0.9303 - acc: 0.6199 - val_loss: 0.9026 - val_acc: 0.6233\n",
            "Epoch 258/300\n",
            "27040/27144 [============================>.] - ETA: 0s - loss: 0.9291 - acc: 0.6220\n",
            " Val F1 score  0.613395225464191\n",
            "27144/27144 [==============================] - 5s 199us/sample - loss: 0.9292 - acc: 0.6221 - val_loss: 0.9342 - val_acc: 0.6134\n",
            "Epoch 259/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 0.9282 - acc: 0.6201\n",
            " Val F1 score  0.6203580901856764\n",
            "27144/27144 [==============================] - 5s 187us/sample - loss: 0.9283 - acc: 0.6201 - val_loss: 0.9087 - val_acc: 0.6204\n",
            "Epoch 260/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 0.9259 - acc: 0.6223\n",
            " Val F1 score  0.6187002652519894\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 0.9259 - acc: 0.6223 - val_loss: 0.9279 - val_acc: 0.6187\n",
            "Epoch 261/300\n",
            "26912/27144 [============================>.] - ETA: 0s - loss: 0.9274 - acc: 0.6217\n",
            " Val F1 score  0.5891909814323607\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9275 - acc: 0.6219 - val_loss: 0.9592 - val_acc: 0.5892\n",
            "Epoch 262/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 0.9264 - acc: 0.6192\n",
            " Val F1 score  0.6273209549071618\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9265 - acc: 0.6191 - val_loss: 0.9113 - val_acc: 0.6273\n",
            "Epoch 263/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 0.9262 - acc: 0.6203\n",
            " Val F1 score  0.618368700265252\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9265 - acc: 0.6205 - val_loss: 0.8971 - val_acc: 0.6184\n",
            "Epoch 264/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 0.9300 - acc: 0.6181\n",
            " Val F1 score  0.6246684350132626\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9299 - acc: 0.6181 - val_loss: 0.9060 - val_acc: 0.6247\n",
            "Epoch 265/300\n",
            "27104/27144 [============================>.] - ETA: 0s - loss: 0.9294 - acc: 0.6209\n",
            " Val F1 score  0.6309681697612732\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9293 - acc: 0.6208 - val_loss: 0.9072 - val_acc: 0.6310\n",
            "Epoch 266/300\n",
            "26880/27144 [============================>.] - ETA: 0s - loss: 0.9246 - acc: 0.6241\n",
            " Val F1 score  0.6196949602122016\n",
            "27144/27144 [==============================] - 5s 186us/sample - loss: 0.9249 - acc: 0.6240 - val_loss: 0.9167 - val_acc: 0.6197\n",
            "Epoch 267/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 0.9314 - acc: 0.6177\n",
            " Val F1 score  0.6346153846153846\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 0.9311 - acc: 0.6179 - val_loss: 0.8973 - val_acc: 0.6346\n",
            "Epoch 268/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 0.9257 - acc: 0.6218\n",
            " Val F1 score  0.6213527851458885\n",
            "27144/27144 [==============================] - 5s 186us/sample - loss: 0.9254 - acc: 0.6221 - val_loss: 0.9106 - val_acc: 0.6214\n",
            "Epoch 269/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 0.9235 - acc: 0.6259\n",
            " Val F1 score  0.6339522546419099\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9232 - acc: 0.6260 - val_loss: 0.8924 - val_acc: 0.6340\n",
            "Epoch 270/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 0.9210 - acc: 0.6252\n",
            " Val F1 score  0.5772546419098143\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9210 - acc: 0.6252 - val_loss: 1.0171 - val_acc: 0.5773\n",
            "Epoch 271/300\n",
            "27104/27144 [============================>.] - ETA: 0s - loss: 0.9194 - acc: 0.6263\n",
            " Val F1 score  0.6389257294429708\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9193 - acc: 0.6262 - val_loss: 0.8827 - val_acc: 0.6389\n",
            "Epoch 272/300\n",
            "27104/27144 [============================>.] - ETA: 0s - loss: 0.9232 - acc: 0.6239\n",
            " Val F1 score  0.6375994694960212\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9229 - acc: 0.6241 - val_loss: 0.8884 - val_acc: 0.6376\n",
            "Epoch 273/300\n",
            "27040/27144 [============================>.] - ETA: 0s - loss: 0.9243 - acc: 0.6223\n",
            " Val F1 score  0.6259946949602122\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9243 - acc: 0.6223 - val_loss: 0.9071 - val_acc: 0.6260\n",
            "Epoch 274/300\n",
            "26912/27144 [============================>.] - ETA: 0s - loss: 0.9232 - acc: 0.6229\n",
            " Val F1 score  0.6369363395225465\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 0.9230 - acc: 0.6229 - val_loss: 0.9002 - val_acc: 0.6369\n",
            "Epoch 275/300\n",
            "26848/27144 [============================>.] - ETA: 0s - loss: 0.9254 - acc: 0.6197\n",
            " Val F1 score  0.6276525198938993\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9252 - acc: 0.6194 - val_loss: 0.9117 - val_acc: 0.6277\n",
            "Epoch 276/300\n",
            "26912/27144 [============================>.] - ETA: 0s - loss: 0.9304 - acc: 0.6189\n",
            " Val F1 score  0.6336206896551724\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 0.9302 - acc: 0.6190 - val_loss: 0.8894 - val_acc: 0.6336\n",
            "Epoch 277/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 0.9210 - acc: 0.6254\n",
            " Val F1 score  0.6349469496021221\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9210 - acc: 0.6254 - val_loss: 0.8902 - val_acc: 0.6349\n",
            "Epoch 278/300\n",
            "26880/27144 [============================>.] - ETA: 0s - loss: 0.9241 - acc: 0.6211\n",
            " Val F1 score  0.6306366047745358\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9244 - acc: 0.6208 - val_loss: 0.8904 - val_acc: 0.6306\n",
            "Epoch 279/300\n",
            "27040/27144 [============================>.] - ETA: 0s - loss: 0.9200 - acc: 0.6229\n",
            " Val F1 score  0.6153846153846154\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 0.9202 - acc: 0.6228 - val_loss: 0.9392 - val_acc: 0.6154\n",
            "Epoch 280/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 0.9167 - acc: 0.6250\n",
            " Val F1 score  0.6150530503978779\n",
            "27144/27144 [==============================] - 5s 187us/sample - loss: 0.9168 - acc: 0.6249 - val_loss: 0.9217 - val_acc: 0.6151\n",
            "Epoch 281/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 0.9189 - acc: 0.6245\n",
            " Val F1 score  0.6177055702917772\n",
            "27144/27144 [==============================] - 5s 185us/sample - loss: 0.9192 - acc: 0.6246 - val_loss: 0.9139 - val_acc: 0.6177\n",
            "Epoch 282/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 0.9174 - acc: 0.6278\n",
            " Val F1 score  0.621684350132626\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9172 - acc: 0.6280 - val_loss: 0.9315 - val_acc: 0.6217\n",
            "Epoch 283/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 0.9169 - acc: 0.6283\n",
            " Val F1 score  0.6326259946949602\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9174 - acc: 0.6281 - val_loss: 0.8931 - val_acc: 0.6326\n",
            "Epoch 284/300\n",
            "27040/27144 [============================>.] - ETA: 0s - loss: 0.9153 - acc: 0.6246\n",
            " Val F1 score  0.6196949602122016\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9147 - acc: 0.6247 - val_loss: 0.9174 - val_acc: 0.6197\n",
            "Epoch 285/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 0.9162 - acc: 0.6249\n",
            " Val F1 score  0.6196949602122016\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9160 - acc: 0.6250 - val_loss: 0.8993 - val_acc: 0.6197\n",
            "Epoch 286/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 0.9153 - acc: 0.6256\n",
            " Val F1 score  0.6256631299734748\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9153 - acc: 0.6257 - val_loss: 0.9130 - val_acc: 0.6257\n",
            "Epoch 287/300\n",
            "26848/27144 [============================>.] - ETA: 0s - loss: 0.9131 - acc: 0.6304\n",
            " Val F1 score  0.6061007957559682\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9140 - acc: 0.6302 - val_loss: 0.9647 - val_acc: 0.6061\n",
            "Epoch 288/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 0.9190 - acc: 0.6242\n",
            " Val F1 score  0.6405835543766578\n",
            "27144/27144 [==============================] - 5s 181us/sample - loss: 0.9188 - acc: 0.6243 - val_loss: 0.8794 - val_acc: 0.6406\n",
            "Epoch 289/300\n",
            "27008/27144 [============================>.] - ETA: 0s - loss: 0.9162 - acc: 0.6246\n",
            " Val F1 score  0.6223474801061007\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9160 - acc: 0.6248 - val_loss: 0.9165 - val_acc: 0.6223\n",
            "Epoch 290/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 0.9153 - acc: 0.6266\n",
            " Val F1 score  0.6203580901856764\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9153 - acc: 0.6266 - val_loss: 0.8918 - val_acc: 0.6204\n",
            "Epoch 291/300\n",
            "27104/27144 [============================>.] - ETA: 0s - loss: 0.9173 - acc: 0.6267\n",
            " Val F1 score  0.6356100795755968\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9175 - acc: 0.6265 - val_loss: 0.8880 - val_acc: 0.6356\n",
            "Epoch 292/300\n",
            "26848/27144 [============================>.] - ETA: 0s - loss: 0.9147 - acc: 0.6275\n",
            " Val F1 score  0.636604774535809\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9152 - acc: 0.6272 - val_loss: 0.8817 - val_acc: 0.6366\n",
            "Epoch 293/300\n",
            "26912/27144 [============================>.] - ETA: 0s - loss: 0.9109 - acc: 0.6291\n",
            " Val F1 score  0.6080901856763926\n",
            "27144/27144 [==============================] - 5s 186us/sample - loss: 0.9115 - acc: 0.6289 - val_loss: 0.9178 - val_acc: 0.6081\n",
            "Epoch 294/300\n",
            "27104/27144 [============================>.] - ETA: 0s - loss: 0.9126 - acc: 0.6282\n",
            " Val F1 score  0.5868700265251989\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9126 - acc: 0.6283 - val_loss: 0.9558 - val_acc: 0.5869\n",
            "Epoch 295/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 0.9105 - acc: 0.6313\n",
            " Val F1 score  0.6196949602122016\n",
            "27144/27144 [==============================] - 5s 183us/sample - loss: 0.9104 - acc: 0.6316 - val_loss: 0.9021 - val_acc: 0.6197\n",
            "Epoch 296/300\n",
            "26944/27144 [============================>.] - ETA: 0s - loss: 0.9101 - acc: 0.6279\n",
            " Val F1 score  0.64157824933687\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 0.9097 - acc: 0.6282 - val_loss: 0.8738 - val_acc: 0.6416\n",
            "Epoch 297/300\n",
            "27136/27144 [============================>.] - ETA: 0s - loss: 0.9105 - acc: 0.6275\n",
            " Val F1 score  0.621684350132626\n",
            "27144/27144 [==============================] - 5s 186us/sample - loss: 0.9105 - acc: 0.6275 - val_loss: 0.9044 - val_acc: 0.6217\n",
            "Epoch 298/300\n",
            "27072/27144 [============================>.] - ETA: 0s - loss: 0.9084 - acc: 0.6271\n",
            " Val F1 score  0.6352785145888594\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9084 - acc: 0.6271 - val_loss: 0.8835 - val_acc: 0.6353\n",
            "Epoch 299/300\n",
            "26848/27144 [============================>.] - ETA: 0s - loss: 0.9099 - acc: 0.6281\n",
            " Val F1 score  0.6329575596816976\n",
            "27144/27144 [==============================] - 5s 184us/sample - loss: 0.9099 - acc: 0.6287 - val_loss: 0.8824 - val_acc: 0.6330\n",
            "Epoch 300/300\n",
            "26912/27144 [============================>.] - ETA: 0s - loss: 0.9087 - acc: 0.6301\n",
            " Val F1 score  0.6501989389920424\n",
            "27144/27144 [==============================] - 5s 182us/sample - loss: 0.9082 - acc: 0.6302 - val_loss: 0.8714 - val_acc: 0.6502\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd73135bc18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    }
  ]
}