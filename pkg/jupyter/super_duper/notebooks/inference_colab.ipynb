{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inference_colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDbV9HrMYvtz"
      },
      "source": [
        "# Inference Demo for Mellotron on Google COLAB\n",
        "by [Hyungon Ryu](https://github.com/yhgon)  | NVAITC Sr. Data Scientist | Center Lead @ NVIDIA \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rjlhd5F1cZB-"
      },
      "source": [
        "modification from original inference.ipynb\n",
        "\n",
        "This notebook requires a GPU runtime to run.\n",
        "Please select the menu option **\"Runtime\"** -> **\"Change runtime type\"**, select **\"Hardware Accelerator\"** -> **\"GPU\"** and click **\"SAVE\"**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZMpc6xoWltS"
      },
      "source": [
        "## Model Description\n",
        "\n",
        "Mellotron: a multispeaker voice synthesis model based on Tacotron 2 GST that can make a voice emote and sing without emotive or singing training data.\n",
        "\n",
        "By explicitly conditioning on rhythm and continuous pitch contours from an audio signal or music score, Mellotron is able to generate speech in a variety of styles ranging from read speech to expressive speech, from slow drawls to rap and from monotonous voice to singing voice.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L_K6eXBY3Um"
      },
      "source": [
        "## DevOps for Google Colab\n",
        "install required python modules and APEX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbHliQpPVtyJ"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_uFcOSWY5Of",
        "cellView": "both"
      },
      "source": [
        "%%time\n",
        "%%bash\n",
        "pip install matplotlib==2.1.0 tensorflow==1.15 inflect==0.2.5 librosa==0.6.0 scipy==1.0.0 tensorboardX==1.1 Unidecode==1.0.22 pillow nltk==3.4.5 jamo==0.4.1  music21 vamp > /dev/null\n",
        "pip install matplotlib==2.1.0 tensorflow==1.15 inflect==0.2.5 librosa==0.6.0 scipy==1.0.0 tensorboardX==1.1 Unidecode==1.0.22 pillow nltk==3.4.5 jamo==0.4.1  music21 vamp "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxlBO28jbnXu"
      },
      "source": [
        "%%time\n",
        "%%bash\n",
        "git clone https://github.com/NVIDIA/apex\n",
        "cd apex\n",
        "pip install  --no-cache-dir ./ # only python  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KOxl0sAYoft"
      },
      "source": [
        "%%bash\n",
        "git clone https://github.com/NVIDIA/mellotron.git\n",
        "cd mellotron\n",
        "git submodule init\n",
        "git submodule update"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXVKnX_FbyGx"
      },
      "source": [
        "## download official checkpoint\n",
        "use google drive utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gD-5pMZCbyQB"
      },
      "source": [
        "%%bash\n",
        "wget -N  -q https://raw.githubusercontent.com/yhgon/colab_utils/master/gfile.py\n",
        "python gfile.py -u 'https://drive.google.com/open?id=1ZesPPyRRKloltRIuRnGZ2LIUEuMSVjkI' -f 'mellotron_libritts.pt'\n",
        "python gfile.py -u 'https://drive.google.com/open?id=1Rm5rV5XaWWiUbIpg5385l5sh68z2bVOE' -f 'waveglow_256channels_v4.pt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcAhlbiejC-4"
      },
      "source": [
        "## Patchs\n",
        "It's temporal solution for inference on COLAB, (TODO code clean )\n",
        "- modify CMUDict directories in `hparams.py` with `cmudict_path=\"/content/mellotron/data/cmu_dictionary\"`\n",
        "- ignore distributed module using `train_utils.py` instead of `train.py`\n",
        "- modify CMUDict directories for `CMUDICT_PATH`  `/content/mellotron/data/cmu_dictionary`  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkOC4HTMiJKS",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "%%file hparams.py\n",
        "import tensorflow as tf\n",
        "from text.symbols import symbols\n",
        "\n",
        "\n",
        "def create_hparams(hparams_string=None, verbose=False):\n",
        "    \"\"\"Create model hyperparameters. Parse nondefault from given string.\"\"\"\n",
        "\n",
        "    hparams = tf.contrib.training.HParams(\n",
        "        ################################\n",
        "        # Experiment Parameters        #\n",
        "        ################################\n",
        "        epochs=50000,\n",
        "        iters_per_checkpoint=500,\n",
        "        seed=1234,\n",
        "        dynamic_loss_scaling=True,\n",
        "        fp16_run=False,\n",
        "        distributed_run=False,\n",
        "        dist_backend=\"nccl\",\n",
        "        dist_url=\"tcp://localhost:54321\",\n",
        "        cudnn_enabled=True,\n",
        "        cudnn_benchmark=False,\n",
        "        ignore_layers=['speaker_embedding.weight'],\n",
        "\n",
        "        ################################\n",
        "        # Data Parameters             #\n",
        "        ################################\n",
        "        training_files='filelists/ljs_audiopaths_text_sid_train_filelist.txt',\n",
        "        validation_files='filelists/ljs_audiopaths_text_sid_val_filelist.txt',\n",
        "        text_cleaners=['english_cleaners'],\n",
        "        p_arpabet=1.0,\n",
        "        cmudict_path=\"/content/mellotron/data/cmu_dictionary\",\n",
        "\n",
        "        ################################\n",
        "        # Audio Parameters             #\n",
        "        ################################\n",
        "        max_wav_value=32768.0,\n",
        "        sampling_rate=22050,\n",
        "        filter_length=1024,\n",
        "        hop_length=256,\n",
        "        win_length=1024,\n",
        "        n_mel_channels=80,\n",
        "        mel_fmin=0.0,\n",
        "        mel_fmax=8000.0,\n",
        "        f0_min=80,\n",
        "        f0_max=880,\n",
        "        harm_thresh=0.25,\n",
        "\n",
        "        ################################\n",
        "        # Model Parameters             #\n",
        "        ################################\n",
        "        n_symbols=len(symbols),\n",
        "        symbols_embedding_dim=512,\n",
        "\n",
        "        # Encoder parameters\n",
        "        encoder_kernel_size=5,\n",
        "        encoder_n_convolutions=3,\n",
        "        encoder_embedding_dim=512,\n",
        "\n",
        "        # Decoder parameters\n",
        "        n_frames_per_step=1,  # currently only 1 is supported\n",
        "        decoder_rnn_dim=1024,\n",
        "        prenet_dim=256,\n",
        "        prenet_f0_n_layers=1,\n",
        "        prenet_f0_dim=1,\n",
        "        prenet_f0_kernel_size=1,\n",
        "        prenet_rms_dim=0,\n",
        "        prenet_rms_kernel_size=1,\n",
        "        max_decoder_steps=1000,\n",
        "        gate_threshold=0.5,\n",
        "        p_attention_dropout=0.1,\n",
        "        p_decoder_dropout=0.1,\n",
        "        p_teacher_forcing=1.0,\n",
        "\n",
        "        # Attention parameters\n",
        "        attention_rnn_dim=1024,\n",
        "        attention_dim=128,\n",
        "\n",
        "        # Location Layer parameters\n",
        "        attention_location_n_filters=32,\n",
        "        attention_location_kernel_size=31,\n",
        "\n",
        "        # Mel-post processing network parameters\n",
        "        postnet_embedding_dim=512,\n",
        "        postnet_kernel_size=5,\n",
        "        postnet_n_convolutions=5,\n",
        "\n",
        "        # Speaker embedding\n",
        "        n_speakers=123,\n",
        "        speaker_embedding_dim=128,\n",
        "\n",
        "        # Reference encoder\n",
        "        with_gst=True,\n",
        "        ref_enc_filters=[32, 32, 64, 64, 128, 128],\n",
        "        ref_enc_size=[3, 3],\n",
        "        ref_enc_strides=[2, 2],\n",
        "        ref_enc_pad=[1, 1],\n",
        "        ref_enc_gru_size=128,\n",
        "\n",
        "        # Style Token Layer\n",
        "        token_embedding_size=256,\n",
        "        token_num=10,\n",
        "        num_heads=8,\n",
        "\n",
        "        ################################\n",
        "        # Optimization Hyperparameters #\n",
        "        ################################\n",
        "        use_saved_learning_rate=False,\n",
        "        learning_rate=1e-3,\n",
        "        learning_rate_min=1e-5,\n",
        "        learning_rate_anneal=50000,\n",
        "        weight_decay=1e-6,\n",
        "        grad_clip_thresh=1.0,\n",
        "        batch_size=32,\n",
        "        mask_padding=True,  # set model's padded outputs to padded values\n",
        "\n",
        "    )\n",
        "\n",
        "    if hparams_string:\n",
        "        tf.compat.v1.logging.info('Parsing command line hparams: %s', hparams_string)\n",
        "        hparams.parse(hparams_string)\n",
        "\n",
        "    if verbose:\n",
        "        tf.compat.v1.logging.info('Final parsed hparams: %s', hparams.values())\n",
        "\n",
        "    return hparams\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9EMpbQ9eBWk",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "%%file train_utils.py\n",
        "import os\n",
        "import time\n",
        "import argparse\n",
        "import math\n",
        "from numpy import finfo\n",
        "\n",
        "import torch\n",
        "#from distributed import apply_gradient_allreduce\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from model import Tacotron2\n",
        "from data_utils import TextMelLoader, TextMelCollate\n",
        "from loss_function import Tacotron2Loss\n",
        "#from logger import Tacotron2Logger\n",
        "from hparams import create_hparams\n",
        "\n",
        "\n",
        "def reduce_tensor(tensor, n_gpus):\n",
        "    rt = tensor.clone()\n",
        "    dist.all_reduce(rt, op=dist.ReduceOp.SUM)\n",
        "    rt /= n_gpus\n",
        "    return rt\n",
        "\n",
        "\n",
        "def init_distributed(hparams, n_gpus, rank, group_name):\n",
        "    assert torch.cuda.is_available(), \"Distributed mode requires CUDA.\"\n",
        "    print(\"Initializing Distributed\")\n",
        "\n",
        "    # Set cuda device so everything is done on the right GPU.\n",
        "    torch.cuda.set_device(rank % torch.cuda.device_count())\n",
        "\n",
        "    # Initialize distributed communication\n",
        "    dist.init_process_group(\n",
        "        backend=hparams.dist_backend, init_method=hparams.dist_url,\n",
        "        world_size=n_gpus, rank=rank, group_name=group_name)\n",
        "\n",
        "    print(\"Done initializing distributed\")\n",
        "\n",
        "\n",
        "def prepare_dataloaders(hparams):\n",
        "    # Get data, data loaders and collate function ready\n",
        "    trainset = TextMelLoader(hparams.training_files, hparams)\n",
        "    valset = TextMelLoader(hparams.validation_files, hparams,\n",
        "                           speaker_ids=trainset.speaker_ids)\n",
        "    collate_fn = TextMelCollate(hparams.n_frames_per_step)\n",
        "\n",
        "    if hparams.distributed_run:\n",
        "        train_sampler = DistributedSampler(trainset)\n",
        "        shuffle = False\n",
        "    else:\n",
        "        train_sampler = None\n",
        "        shuffle = True\n",
        "\n",
        "    train_loader = DataLoader(trainset, num_workers=1, shuffle=shuffle,\n",
        "                              sampler=train_sampler,\n",
        "                              batch_size=hparams.batch_size, pin_memory=False,\n",
        "                              drop_last=True, collate_fn=collate_fn)\n",
        "    return train_loader, valset, collate_fn, train_sampler\n",
        "\n",
        "\n",
        "def prepare_directories_and_logger(output_directory, log_directory, rank):\n",
        "    if rank == 0:\n",
        "        if not os.path.isdir(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "            os.chmod(output_directory, 0o775)\n",
        "        logger = None    \n",
        "        #logger = Tacotron2Logger(os.path.join(output_directory, log_directory))\n",
        "    else:\n",
        "        logger = None\n",
        "    return logger\n",
        "\n",
        "\n",
        "def load_model(hparams):\n",
        "    model = Tacotron2(hparams).cuda()\n",
        "    if hparams.fp16_run:\n",
        "        model.decoder.attention_layer.score_mask_value = finfo('float16').min\n",
        "\n",
        "    if hparams.distributed_run:\n",
        "        model = apply_gradient_allreduce(model)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def warm_start_model(checkpoint_path, model, ignore_layers):\n",
        "    assert os.path.isfile(checkpoint_path)\n",
        "    print(\"Warm starting model from checkpoint '{}'\".format(checkpoint_path))\n",
        "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model_dict = checkpoint_dict['state_dict']\n",
        "    if len(ignore_layers) > 0:\n",
        "        model_dict = {k: v for k, v in model_dict.items()\n",
        "                      if k not in ignore_layers}\n",
        "        dummy_dict = model.state_dict()\n",
        "        dummy_dict.update(model_dict)\n",
        "        model_dict = dummy_dict\n",
        "    model.load_state_dict(model_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer):\n",
        "    assert os.path.isfile(checkpoint_path)\n",
        "    print(\"Loading checkpoint '{}'\".format(checkpoint_path))\n",
        "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint_dict['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
        "    learning_rate = checkpoint_dict['learning_rate']\n",
        "    iteration = checkpoint_dict['iteration']\n",
        "    print(\"Loaded checkpoint '{}' from iteration {}\" .format(\n",
        "        checkpoint_path, iteration))\n",
        "    return model, optimizer, learning_rate, iteration\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, learning_rate, iteration, filepath):\n",
        "    print(\"Saving model and optimizer state at iteration {} to {}\".format(\n",
        "        iteration, filepath))\n",
        "    torch.save({'iteration': iteration,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'learning_rate': learning_rate}, filepath)\n",
        "\n",
        "\n",
        "def validate(model, criterion, valset, iteration, batch_size, n_gpus,\n",
        "             collate_fn, logger, distributed_run, rank):\n",
        "    \"\"\"Handles all the validation scoring and printing\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_sampler = DistributedSampler(valset) if distributed_run else None\n",
        "        val_loader = DataLoader(valset, sampler=val_sampler, num_workers=1,\n",
        "                                shuffle=False, batch_size=batch_size,\n",
        "                                pin_memory=False, collate_fn=collate_fn)\n",
        "\n",
        "        val_loss = 0.0\n",
        "        for i, batch in enumerate(val_loader):\n",
        "            x, y = model.parse_batch(batch)\n",
        "            y_pred = model(x)\n",
        "            loss = criterion(y_pred, y)\n",
        "            if distributed_run:\n",
        "                reduced_val_loss = reduce_tensor(loss.data, n_gpus).item()\n",
        "            else:\n",
        "                reduced_val_loss = loss.item()\n",
        "            val_loss += reduced_val_loss\n",
        "        val_loss = val_loss / (i + 1)\n",
        "\n",
        "    model.train()\n",
        "    if rank == 0:\n",
        "        print(\"Validation loss {}: {:9f}  \".format(iteration, reduced_val_loss))\n",
        "        logger.log_validation(val_loss, model, y, y_pred, iteration)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEoYpG37gWd2",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "%%file /content/mellotron/mellotron_utils.py\n",
        "import re\n",
        "import numpy as np\n",
        "import music21 as m21\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from text import text_to_sequence, get_arpabet, cmudict\n",
        "\n",
        "\n",
        "CMUDICT_PATH = \"/content/mellotron/data/cmu_dictionary\"\n",
        "CMUDICT = cmudict.CMUDict(CMUDICT_PATH)\n",
        "PHONEME2GRAPHEME = {\n",
        "    'AA': ['a', 'o', 'ah'],\n",
        "    'AE': ['a', 'e'],\n",
        "    'AH': ['u', 'e', 'a', 'h', 'o'],\n",
        "    'AO': ['o', 'u', 'au'],\n",
        "    'AW': ['ou', 'ow'],\n",
        "    'AX': ['a'],\n",
        "    'AXR': ['er'],\n",
        "    'AY': ['i'],\n",
        "    'EH': ['e', 'ae'],\n",
        "    'EY': ['a', 'ai', 'ei', 'e', 'y'],\n",
        "    'IH': ['i', 'e'],\n",
        "    'IX': ['e', 'i'],\n",
        "    'IY': ['ea', 'ey', 'y', 'i'],\n",
        "    'OW': ['oa'],\n",
        "    'OY': ['oy'],\n",
        "    'UH': ['oo'],\n",
        "    'UW': ['oo', 'u', 'o'],\n",
        "    'UX': ['u'],\n",
        "    'B': ['b'],\n",
        "    'CH': ['ch', 'tch'],\n",
        "    'D': ['d', 'e', 'de'],\n",
        "    'DH': ['th'],\n",
        "    'DX': ['tt'],\n",
        "    'EL': ['le'],\n",
        "    'EM': ['m'],\n",
        "    'EN': ['on'],\n",
        "    'ER': ['i', 'er'],\n",
        "    'F': ['f'],\n",
        "    'G': ['g'],\n",
        "    'HH': ['h'],\n",
        "    'JH': ['j'],\n",
        "    'K': ['k', 'c', 'ch'],\n",
        "    'KS': ['x'],\n",
        "    'L': ['ll', 'l'],\n",
        "    'M': ['m'],\n",
        "    'N': ['n', 'gn'],\n",
        "    'NG': ['ng'],\n",
        "    'NX': ['nn'],\n",
        "    'P': ['p'],\n",
        "    'Q': ['-'],\n",
        "    'R': ['wr', 'r'],\n",
        "    'S': ['s', 'ce'],\n",
        "    'SH': ['sh'],\n",
        "    'T': ['t'],\n",
        "    'TH': ['th'],\n",
        "    'V': ['v', 'f', 'e'],\n",
        "    'W': ['w'],\n",
        "    'WH': ['wh'],\n",
        "    'Y': ['y', 'j'],\n",
        "    'Z': ['z', 's'],\n",
        "    'ZH': ['s']\n",
        "}\n",
        "\n",
        "########################\n",
        "#  CONSONANT DURATION  #\n",
        "########################\n",
        "PHONEMEDURATION = {\n",
        "    'B': 0.05,\n",
        "    'CH': 0.1,\n",
        "    'D': 0.075,\n",
        "    'DH': 0.05,\n",
        "    'DX': 0.05,\n",
        "    'EL': 0.05,\n",
        "    'EM': 0.05,\n",
        "    'EN': 0.05,\n",
        "    'F': 0.1,\n",
        "    'G': 0.05,\n",
        "    'HH': 0.05,\n",
        "    'JH': 0.05,\n",
        "    'K': 0.05,\n",
        "    'L': 0.05,\n",
        "    'M': 0.15,\n",
        "    'N': 0.15,\n",
        "    'NG': 0.15,\n",
        "    'NX': 0.05,\n",
        "    'P': 0.05,\n",
        "    'Q': 0.075,\n",
        "    'R': 0.05,\n",
        "    'S': 0.1,\n",
        "    'SH': 0.05,\n",
        "    'T': 0.075,\n",
        "    'TH': 0.1,\n",
        "    'V': 0.05,\n",
        "    'Y': 0.05,\n",
        "    'W': 0.05,\n",
        "    'WH': 0.05,\n",
        "    'Z': 0.05,\n",
        "    'ZH': 0.05\n",
        "}\n",
        "\n",
        "\n",
        "def add_space_between_events(events, connect=False):\n",
        "    new_events = []\n",
        "    for i in range(1, len(events)):\n",
        "        token_a, freq_a, start_time_a, end_time_a = events[i-1][-1]\n",
        "        token_b, freq_b, start_time_b, end_time_b = events[i][0]\n",
        "\n",
        "        if token_a in (' ', '') and len(events[i-1]) == 1:\n",
        "            new_events.append(events[i-1])\n",
        "        elif token_a not in (' ', '') and token_b not in (' ', ''):\n",
        "            new_events.append(events[i-1])\n",
        "            if connect:\n",
        "                new_events.append([[' ', 0, end_time_a, start_time_b]])\n",
        "            else:\n",
        "                new_events.append([[' ', 0, end_time_a, end_time_a]])\n",
        "        else:\n",
        "            new_events.append(events[i-1])\n",
        "\n",
        "    if new_events[-1][0][0] != ' ':\n",
        "        new_events.append([[' ', 0, end_time_a, end_time_a]])\n",
        "    new_events.append(events[-1])\n",
        "\n",
        "    return new_events\n",
        "\n",
        "\n",
        "def adjust_words(events):\n",
        "    new_events = []\n",
        "    for event in events:\n",
        "        if len(event) == 1 and event[0][0] == ' ':\n",
        "            new_events.append(event)\n",
        "        else:\n",
        "            for e in event:\n",
        "                if e[0][0].isupper():\n",
        "                    new_events.append([e])\n",
        "                else:\n",
        "                    new_events[-1].extend([e])\n",
        "    return new_events\n",
        "\n",
        "\n",
        "def adjust_extensions(events, phoneme_durations):\n",
        "    if len(events) == 1:\n",
        "        return events\n",
        "\n",
        "    idx_last_vowel = None\n",
        "    n_consonants_after_last_vowel = 0\n",
        "    target_ids = np.arange(len(events))\n",
        "    for i in range(len(events)):\n",
        "        token = re.sub('[0-9{}]', '', events[i][0])\n",
        "        if idx_last_vowel is None and token not in phoneme_durations:\n",
        "            idx_last_vowel = i\n",
        "            n_consonants_after_last_vowel = 0\n",
        "        else:\n",
        "            if token == '_' and not n_consonants_after_last_vowel:\n",
        "                events[i][0] = events[idx_last_vowel][0]\n",
        "            elif token == '_' and n_consonants_after_last_vowel:\n",
        "                events[i][0] = events[idx_last_vowel][0]\n",
        "                start = idx_last_vowel + 1\n",
        "                target_ids[start:start+n_consonants_after_last_vowel] += 1\n",
        "                target_ids[i] -= n_consonants_after_last_vowel\n",
        "            elif token in phoneme_durations:\n",
        "                n_consonants_after_last_vowel += 1\n",
        "            else:\n",
        "                n_consonants_after_last_vowel = 0\n",
        "                idx_last_vowel = i\n",
        "\n",
        "    new_events = [0] * len(events)\n",
        "    for i in range(len(events)):\n",
        "        new_events[target_ids[i]] = events[i]\n",
        "\n",
        "    # adjust time of consonants that were repositioned\n",
        "    for i in range(1, len(new_events)):\n",
        "        if new_events[i][2] < new_events[i-1][2]:\n",
        "            new_events[i][2] = new_events[i-1][2]\n",
        "            new_events[i][3] = new_events[i-1][3]\n",
        "\n",
        "    return new_events\n",
        "\n",
        "\n",
        "def adjust_consonant_lengths(events, phoneme_durations):\n",
        "    t_init = events[0][2]\n",
        "\n",
        "    idx_last_vowel = None\n",
        "    for i in range(len(events)):\n",
        "        task = re.sub('[0-9{}]', '', events[i][0])\n",
        "        if task in phoneme_durations:\n",
        "            duration = phoneme_durations[task]\n",
        "            if idx_last_vowel is None:  # consonant comes before any vowel\n",
        "                events[i][2] = t_init\n",
        "                events[i][3] = t_init + duration\n",
        "            else:  # consonant comes after a vowel, must offset\n",
        "                events[idx_last_vowel][3] -= duration\n",
        "                for k in range(idx_last_vowel+1, i):\n",
        "                    events[k][2] -= duration\n",
        "                    events[k][3] -= duration\n",
        "                events[i][2] = events[i-1][3]\n",
        "                events[i][3] = events[i-1][3] + duration\n",
        "        else:\n",
        "            events[i][2] = t_init\n",
        "            events[i][3] = events[i][3]\n",
        "            t_init = events[i][3]\n",
        "            idx_last_vowel = i\n",
        "        t_init = events[i][3]\n",
        "\n",
        "    return events\n",
        "\n",
        "\n",
        "def adjust_consonants(events, phoneme_durations):\n",
        "    if len(events) == 1:\n",
        "        return events\n",
        "\n",
        "    start = 0\n",
        "    split_ids = []\n",
        "    t_init = events[0][2]\n",
        "\n",
        "    # get each substring group\n",
        "    for i in range(1, len(events)):\n",
        "        if events[i][2] != t_init:\n",
        "            split_ids.append((start, i))\n",
        "            start = i\n",
        "            t_init = events[i][2]\n",
        "    split_ids.append((start, len(events)))\n",
        "\n",
        "    for (start, end) in split_ids:\n",
        "        events[start:end] = adjust_consonant_lengths(\n",
        "            events[start:end], phoneme_durations)\n",
        "\n",
        "    return events\n",
        "\n",
        "\n",
        "def adjust_event(event, hop_length=256, sampling_rate=22050):\n",
        "    tokens, freq, start_time, end_time = event\n",
        "\n",
        "    if tokens == ' ':\n",
        "        return [event] if freq == 0 else [['_', freq, start_time, end_time]]\n",
        "\n",
        "    return [[token, freq, start_time, end_time] for token in tokens]\n",
        "\n",
        "\n",
        "def musicxml2score(filepath, bpm=60):\n",
        "    track = {}\n",
        "    beat_length_seconds = 60/bpm\n",
        "    data = m21.converter.parse(filepath)\n",
        "    for i in range(len(data.parts)):\n",
        "        part = data.parts[i].flat\n",
        "        events = []\n",
        "        for k in range(len(part.notesAndRests)):\n",
        "            event = part.notesAndRests[k]\n",
        "            if isinstance(event, m21.note.Note):\n",
        "                freq = event.pitch.frequency\n",
        "                token = event.lyrics[0].text if len(event.lyrics) > 0 else ' '\n",
        "                start_time = event.offset * beat_length_seconds\n",
        "                end_time = start_time + event.duration.quarterLength * beat_length_seconds\n",
        "                event = [token, freq, start_time, end_time]\n",
        "            elif isinstance(event, m21.note.Rest):\n",
        "                freq = 0\n",
        "                token = ' '\n",
        "                start_time = event.offset * beat_length_seconds\n",
        "                end_time = start_time + event.duration.quarterLength * beat_length_seconds\n",
        "                event = [token, freq, start_time, end_time]\n",
        "\n",
        "            if token == '_':\n",
        "                raise Exception(\"Unexpected token {}\".format(token))\n",
        "\n",
        "            if len(events) == 0:\n",
        "                events.append(event)\n",
        "            else:\n",
        "                if token == ' ':\n",
        "                    if freq == 0:\n",
        "                        if events[-1][1] == 0:\n",
        "                            events[-1][3] = end_time\n",
        "                        else:\n",
        "                            events.append(event)\n",
        "                    elif freq == events[-1][1]:  # is event duration extension ?\n",
        "                        events[-1][-1] = end_time\n",
        "                    else:  # must be different note on same syllable\n",
        "                        events.append(event)\n",
        "                else:\n",
        "                    events.append(event)\n",
        "        track[part.partName] = events\n",
        "    return track\n",
        "\n",
        "\n",
        "def track2events(track):\n",
        "    events = []\n",
        "    for e in track:\n",
        "        events.extend(adjust_event(e))\n",
        "    group_ids = [i for i in range(len(events))\n",
        "                 if events[i][0] in [' '] or events[i][0].isupper()]\n",
        "\n",
        "    events_grouped = []\n",
        "    for i in range(1, len(group_ids)):\n",
        "        start, end = group_ids[i-1], group_ids[i]\n",
        "        events_grouped.append(events[start:end])\n",
        "\n",
        "    if events[-1][0] != ' ':\n",
        "        events_grouped.append(events[group_ids[-1]:])\n",
        "\n",
        "    return events_grouped\n",
        "\n",
        "\n",
        "def events2eventsarpabet(event):\n",
        "    if event[0][0] == ' ':\n",
        "        return event\n",
        "\n",
        "    # get word and word arpabet\n",
        "    word = ''.join([e[0] for e in event if e[0] not in('_', ' ')])\n",
        "    word_arpabet = get_arpabet(word, CMUDICT)\n",
        "    if word_arpabet[0] != '{':\n",
        "        return event\n",
        "\n",
        "    word_arpabet = word_arpabet.split()\n",
        "\n",
        "    # align tokens to arpabet\n",
        "    i, k = 0, 0\n",
        "    new_events = []\n",
        "    while i < len(event) and k < len(word_arpabet):\n",
        "        # single token\n",
        "        token_a, freq_a, start_time_a, end_time_a = event[i]\n",
        "\n",
        "        if token_a == ' ':\n",
        "            new_events.append([token_a, freq_a, start_time_a, end_time_a])\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        if token_a == '_':\n",
        "            new_events.append([token_a, freq_a, start_time_a, end_time_a])\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        # two tokens\n",
        "        if i < len(event) - 1:\n",
        "            j = i + 1\n",
        "            token_b, freq_b, start_time_b, end_time_b = event[j]\n",
        "            between_events = []\n",
        "            while j < len(event) and event[j][0] == '_':\n",
        "                between_events.append([token_b, freq_b, start_time_b, end_time_b])\n",
        "                j += 1\n",
        "                if j < len(event):\n",
        "                    token_b, freq_b, start_time_b, end_time_b = event[j]\n",
        "\n",
        "            token_compound_2 = (token_a + token_b).lower()\n",
        "\n",
        "        # single arpabet\n",
        "        arpabet = re.sub('[0-9{}]', '', word_arpabet[k])\n",
        "\n",
        "        if k < len(word_arpabet) - 1:\n",
        "            arpabet_compound_2 = ''.join(word_arpabet[k:k+2])\n",
        "            arpabet_compound_2 = re.sub('[0-9{}]', '', arpabet_compound_2)\n",
        "\n",
        "        if i < len(event) - 1 and token_compound_2 in PHONEME2GRAPHEME[arpabet]:\n",
        "            new_events.append([word_arpabet[k], freq_a, start_time_a, end_time_a])\n",
        "            if len(between_events):\n",
        "                new_events.extend(between_events)\n",
        "            if start_time_a != start_time_b:\n",
        "                new_events.append([word_arpabet[k], freq_b, start_time_b, end_time_b])\n",
        "            i += 2 + len(between_events)\n",
        "            k += 1\n",
        "        elif token_a.lower() in PHONEME2GRAPHEME[arpabet]:\n",
        "            new_events.append([word_arpabet[k], freq_a, start_time_a, end_time_a])\n",
        "            i += 1\n",
        "            k += 1\n",
        "        elif arpabet_compound_2 in PHONEME2GRAPHEME and token_a.lower() in PHONEME2GRAPHEME[arpabet_compound_2]:\n",
        "            new_events.append([word_arpabet[k], freq_a, start_time_a, end_time_a])\n",
        "            new_events.append([word_arpabet[k+1], freq_a, start_time_a, end_time_a])\n",
        "            i += 1\n",
        "            k += 2\n",
        "        else:\n",
        "            k += 1\n",
        "\n",
        "    if len(event) == 1:\n",
        "        if len(word_arpabet) > 1:\n",
        "            raise Exception(\"More characters in {} than in {}\".format(\n",
        "                word, word_arpabet))\n",
        "        token_a, freq_a, start_time_a, end_time_a = event[i]\n",
        "        new_events.append([word_arpabet[0], freq_a, start_time_a, end_time_a])\n",
        "\n",
        "    # add extensions and pauses at end of words\n",
        "    while i < len(event):\n",
        "        token_a, freq_a, start_time_a, end_time_a = event[i]\n",
        "\n",
        "        if token_a in (' ', '_'):\n",
        "            new_events.append([token_a, freq_a, start_time_a, end_time_a])\n",
        "        i += 1\n",
        "\n",
        "    return new_events\n",
        "\n",
        "\n",
        "def event2alignment(events, hop_length=256, sampling_rate=22050):\n",
        "    frame_length = float(hop_length) / float(sampling_rate)\n",
        "\n",
        "    n_frames = int(events[-1][-1][-1] / frame_length)\n",
        "    n_tokens = np.sum([len(e) for e in events])\n",
        "    alignment = np.zeros((n_tokens, n_frames))\n",
        "\n",
        "    cur_event = -1\n",
        "    for event in events:\n",
        "        for i in range(len(event)):\n",
        "            if len(event) == 1 or cur_event == -1 or event[i][0] != event[i-1][0]:\n",
        "                cur_event += 1\n",
        "            token, freq, start_time, end_time = event[i]\n",
        "            alignment[cur_event, int(start_time/frame_length):int(end_time/frame_length)] = 1\n",
        "\n",
        "    return alignment[:cur_event+1]\n",
        "\n",
        "\n",
        "def event2f0(events, hop_length=256, sampling_rate=22050):\n",
        "    frame_length = float(hop_length) / float(sampling_rate)\n",
        "    n_frames = int(events[-1][-1][-1] / frame_length)\n",
        "    f0s = np.zeros((1, n_frames))\n",
        "\n",
        "    for event in events:\n",
        "        for i in range(len(event)):\n",
        "            token, freq, start_time, end_time = event[i]\n",
        "            f0s[0, int(start_time/frame_length):int(end_time/frame_length)] = freq\n",
        "\n",
        "    return f0s\n",
        "\n",
        "\n",
        "def event2text(events, convert_stress, cmudict=None):\n",
        "    text_clean = ''\n",
        "    for event in events:\n",
        "        for i in range(len(event)):\n",
        "            if i > 0 and event[i][0] == event[i-1][0]:\n",
        "                continue\n",
        "            if event[i][0] == ' ' and len(event) > 1:\n",
        "                if text_clean[-1] != \"}\":\n",
        "                    text_clean = text_clean[:-1] + '} {'\n",
        "                else:\n",
        "                    text_clean += ' {'\n",
        "            else:\n",
        "                if event[i][0][-1] in ('}', ' '):\n",
        "                    text_clean += event[i][0]\n",
        "                else:\n",
        "                    text_clean += event[i][0] + ' '\n",
        "\n",
        "    if convert_stress:\n",
        "        text_clean = re.sub('[0-9]', '1', text_clean)\n",
        "\n",
        "    text_encoded = text_to_sequence(text_clean, [], cmudict)\n",
        "    return text_encoded, text_clean\n",
        "\n",
        "\n",
        "def remove_excess_frames(alignment, f0s):\n",
        "    excess_frames = np.sum(alignment.sum(0) == 0)\n",
        "    alignment = alignment[:, :-excess_frames] if excess_frames > 0 else alignment\n",
        "    f0s = f0s[:, :-excess_frames] if excess_frames > 0 else f0s\n",
        "    return alignment, f0s\n",
        "\n",
        "\n",
        "def get_data_from_musicxml(filepath, bpm, phoneme_durations=None,\n",
        "                           convert_stress=False):\n",
        "    if phoneme_durations is None:\n",
        "        phoneme_durations = PHONEMEDURATION\n",
        "    score = musicxml2score(filepath, bpm)\n",
        "    data = {}\n",
        "    for k, v in score.items():\n",
        "        # ignore empty tracks\n",
        "        if len(v) == 1 and v[0][0] == ' ':\n",
        "            continue\n",
        "\n",
        "        events = track2events(v)\n",
        "        events = adjust_words(events)\n",
        "        events_arpabet = [events2eventsarpabet(e) for e in events]\n",
        "\n",
        "        # make adjustments\n",
        "        events_arpabet = [adjust_extensions(e, phoneme_durations)\n",
        "                          for e in events_arpabet]\n",
        "        events_arpabet = [adjust_consonants(e, phoneme_durations)\n",
        "                          for e in events_arpabet]\n",
        "        events_arpabet = add_space_between_events(events_arpabet)\n",
        "\n",
        "        # convert data to alignment, f0 and text encoded\n",
        "        alignment = event2alignment(events_arpabet)\n",
        "        f0s = event2f0(events_arpabet)\n",
        "        alignment, f0s = remove_excess_frames(alignment, f0s)\n",
        "        text_encoded, text_clean = event2text(events_arpabet, convert_stress)\n",
        "\n",
        "        # convert data to torch\n",
        "        alignment = torch.from_numpy(alignment).permute(1, 0)[:, None].float()\n",
        "        f0s = torch.from_numpy(f0s)[None].float()\n",
        "        text_encoded = torch.LongTensor(text_encoded)[None]\n",
        "        data[k] = {'rhythm': alignment,\n",
        "                   'pitch_contour': f0s,\n",
        "                   'text_encoded': text_encoded}\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "    # Get defaults so it can work with no Sacred\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-f', \"--filepath\", required=True)\n",
        "    args = parser.parse_args()\n",
        "    get_data_from_musicxml(args.filepath, 60)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYJX2NQ8cLyL"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rUry_AYcbMj"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "\n",
        "import sys\n",
        "################################################################################################\n",
        "sys.path.append('/content/mellotron')             #####  modified for  colab ######\n",
        "sys.path.append('/content/mellotron/waveglow/')   #####  modified for  colab ######\n",
        "################################################################################################\n",
        "\n",
        "from itertools import cycle\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from scipy.io.wavfile import write\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import torch\n",
        "\n",
        "from hparams import create_hparams\n",
        "from model import Tacotron2\n",
        "from waveglow.denoiser import Denoiser\n",
        "from layers import TacotronSTFT\n",
        "################################################################################################\n",
        "from train_utils import load_model #####  modified for inference  on colab #####################\n",
        "################################################################################################\n",
        "from data_utils import TextMelLoader, TextMelCollate\n",
        "from text import cmudict, text_to_sequence\n",
        "from mellotron_utils import get_data_from_musicxml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfxC85xxdbiL"
      },
      "source": [
        "def panner(signal, angle):\n",
        "    angle = np.radians(angle)\n",
        "    left = np.sqrt(2)/2.0 * (np.cos(angle) - np.sin(angle)) * signal\n",
        "    right = np.sqrt(2)/2.0 * (np.cos(angle) + np.sin(angle)) * signal\n",
        "    return np.dstack((left, right))[0]\n",
        "\n",
        "def plot_mel_f0_alignment(mel_source, mel_outputs_postnet, f0s, alignments, figsize=(16, 16)):\n",
        "    fig, axes = plt.subplots(4, 1, figsize=figsize)\n",
        "    axes = axes.flatten()\n",
        "    axes[0].imshow(mel_source, aspect='auto', origin='bottom', interpolation='none')\n",
        "    axes[1].imshow(mel_outputs_postnet, aspect='auto', origin='bottom', interpolation='none')\n",
        "    axes[2].scatter(range(len(f0s)), f0s, alpha=0.5, color='red', marker='.', s=1)\n",
        "    axes[2].set_xlim(0, len(f0s))\n",
        "    axes[3].imshow(alignments, aspect='auto', origin='bottom', interpolation='none')\n",
        "    axes[0].set_title(\"Source Mel\")\n",
        "    axes[1].set_title(\"Predicted Mel\")\n",
        "    axes[2].set_title(\"Source pitch contour\")\n",
        "    axes[3].set_title(\"Source rhythm\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "def load_mel(path):\n",
        "    audio, sampling_rate = librosa.core.load(path, sr=hparams.sampling_rate)\n",
        "    audio = torch.from_numpy(audio)\n",
        "    if sampling_rate != hparams.sampling_rate:\n",
        "        raise ValueError(\"{} SR doesn't match target {} SR\".format(\n",
        "            sampling_rate, stft.sampling_rate))\n",
        "    audio_norm = audio / hparams.max_wav_value\n",
        "    audio_norm = audio_norm.unsqueeze(0)\n",
        "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
        "    melspec = stft.mel_spectrogram(audio_norm)\n",
        "    melspec = melspec.cuda()\n",
        "    return melspec\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ppld4DYgmPr"
      },
      "source": [
        "hparams = create_hparams()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjOLIgswgpYQ"
      },
      "source": [
        "stft = TacotronSTFT(hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
        "                    hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
        "                    hparams.mel_fmax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmeHMMX_5Y7S"
      },
      "source": [
        "## Load Models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omAernnggsMD"
      },
      "source": [
        "checkpoint_path = \"/content/mellotron_libritts.pt\"\n",
        "tacotron = load_model(hparams).cuda().eval()\n",
        "tacotron.load_state_dict(torch.load(checkpoint_path)['state_dict'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWfAkxQ3g1L3"
      },
      "source": [
        "waveglow_path = '/content/waveglow_256channels_v4.pt'\n",
        "waveglow = torch.load(waveglow_path)['model'].cuda().eval()\n",
        "denoiser = Denoiser(waveglow).cuda().eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2odPOEu5dTK"
      },
      "source": [
        "## Setup dataloaders for Google colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOB9U_Hbhv_E"
      },
      "source": [
        "%%file /content/mellotron/data/examples_filelist.txt\n",
        "/content/mellotron/data/example1.wav|exploring the expanses of space to keep our planet safe|1\n",
        "/content/mellotron/data/example2.wav|and all the species that call it home|1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZLRcTyxhUF-"
      },
      "source": [
        "arpabet_dict = cmudict.CMUDict('/content/mellotron/data/cmu_dictionary')\n",
        "audio_paths = '/content/mellotron/data/examples_filelist.txt'\n",
        "dataloader = TextMelLoader(audio_paths, hparams)\n",
        "datacollate = TextMelCollate(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V41TqC8R5iO8"
      },
      "source": [
        "## Load data for reference voice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP5xaf2sio60"
      },
      "source": [
        "file_idx = 0\n",
        "audio_path, text, sid = dataloader.audiopaths_and_text[file_idx]\n",
        "\n",
        "# get audio path, encoded text, pitch contour and mel for gst\n",
        "text_encoded = torch.LongTensor(text_to_sequence(text, hparams.text_cleaners, arpabet_dict))[None, :].cuda()    \n",
        "pitch_contour = dataloader[file_idx][3][None].cuda()\n",
        "mel = load_mel(audio_path)\n",
        "print(audio_path, text)\n",
        "\n",
        "# load source data to obtain rhythm using tacotron 2 as a forced aligner\n",
        "x, y = tacotron.parse_batch(datacollate([dataloader[file_idx]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bdyR9g6ipfy"
      },
      "source": [
        "ipd.Audio(audio_path, rate=hparams.sampling_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEarL2P_6bdP"
      },
      "source": [
        "## Define Speakers Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4ITADu4irh0"
      },
      "source": [
        "speaker_ids = TextMelLoader(\"/content/mellotron/filelists/libritts_train_clean_100_audiopath_text_sid_atleast5min_val_filelist.txt\", hparams).speaker_ids\n",
        "speakers = pd.read_csv('/content/mellotron/filelists/libritts_speakerinfo.txt', engine='python',header=None, comment=';', sep=' *\\| *', \n",
        "                       names=['ID', 'SEX', 'SUBSET', 'MINUTES', 'NAME'])\n",
        "speakers['MELLOTRON_ID'] = speakers['ID'].apply(lambda x: speaker_ids[x] if x in speaker_ids else -1)\n",
        "female_speakers = cycle(\n",
        "    speakers.query(\"SEX == 'F' and MINUTES > 20 and MELLOTRON_ID >= 0\")['MELLOTRON_ID'].sample(frac=1).tolist())\n",
        "male_speakers = cycle(\n",
        "    speakers.query(\"SEX == 'M' and MINUTES > 20 and MELLOTRON_ID >= 0\")['MELLOTRON_ID'].sample(frac=1).tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNqlaig86fcK"
      },
      "source": [
        "## Style Transfer (Rhythm and Pitch Contour)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_WTEzJqiurH"
      },
      "source": [
        "with torch.no_grad():\n",
        "    # get rhythm (alignment map) using tacotron 2\n",
        "    mel_outputs, mel_outputs_postnet, gate_outputs, rhythm = tacotron.forward(x)\n",
        "    rhythm = rhythm.permute(1, 0, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rE5qEaBJiwmp"
      },
      "source": [
        "speaker_id = next(female_speakers) if np.random.randint(2) else next(male_speakers)\n",
        "speaker_id = torch.LongTensor([speaker_id]).cuda()\n",
        "\n",
        "with torch.no_grad():\n",
        "    mel_outputs, mel_outputs_postnet, gate_outputs, _ = tacotron.inference_noattention(\n",
        "        (text_encoded, mel, speaker_id, pitch_contour, rhythm))\n",
        "\n",
        "plot_mel_f0_alignment(x[2].data.cpu().numpy()[0],\n",
        "                      mel_outputs_postnet.data.cpu().numpy()[0],\n",
        "                      pitch_contour.data.cpu().numpy()[0, 0],\n",
        "                      rhythm.data.cpu().numpy()[:, 0].T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhRLnjNy-X3d"
      },
      "source": [
        "synthesis voice voice with sigma = 0.8 (default)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnl0ba5oizE_"
      },
      "source": [
        "with torch.no_grad():\n",
        "    audio = denoiser(waveglow.infer(mel_outputs_postnet, sigma=0.8), 0.01)[:, 0]\n",
        "ipd.Audio(audio[0].data.cpu().numpy(), rate=hparams.sampling_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srw4VwP06TjT"
      },
      "source": [
        "## try other random voice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMmw_05w6MiA"
      },
      "source": [
        "speaker_id = next(female_speakers) if np.random.randint(2) else next(male_speakers)\n",
        "speaker_id = torch.LongTensor([speaker_id]).cuda()\n",
        "\n",
        "with torch.no_grad():\n",
        "    mel_outputs, mel_outputs_postnet, gate_outputs, _ = tacotron.inference_noattention(\n",
        "        (text_encoded, mel, speaker_id, pitch_contour, rhythm))\n",
        "\n",
        "plot_mel_f0_alignment(x[2].data.cpu().numpy()[0],\n",
        "                      mel_outputs_postnet.data.cpu().numpy()[0],\n",
        "                      pitch_contour.data.cpu().numpy()[0, 0],\n",
        "                      rhythm.data.cpu().numpy()[:, 0].T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSNukuaa-lQH"
      },
      "source": [
        "synthesis voice voice with sigma =0.9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y95Yqp2X6Rt6"
      },
      "source": [
        "with torch.no_grad():\n",
        "    audio = denoiser(waveglow.infer(mel_outputs_postnet, sigma=0.9), 0.01)[:, 0]\n",
        "ipd.Audio(audio[0].data.cpu().numpy(), rate=hparams.sampling_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wMtWCEe6lud"
      },
      "source": [
        "## Singing Voice from Music Score\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B78EPSRi1hN"
      },
      "source": [
        "data = get_data_from_musicxml('/content/mellotron/data/haendel_hallelujah.musicxml', 132, convert_stress=True)\n",
        "panning = {'Soprano': [-60, -30], 'Alto': [-40, -10], 'Tenor': [30, 60], 'Bass': [10, 40]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_V5NLsxi3cj"
      },
      "source": [
        "n_speakers_per_part = 4\n",
        "frequency_scaling = 0.4\n",
        "n_seconds = 90\n",
        "audio_stereo = np.zeros((hparams.sampling_rate*n_seconds, 2), dtype=np.float32)\n",
        "for i, (part, v) in enumerate(data.items()):\n",
        "    rhythm = data[part]['rhythm'].cuda()\n",
        "    pitch_contour = data[part]['pitch_contour'].cuda()\n",
        "    text_encoded = data[part]['text_encoded'].cuda()\n",
        "    \n",
        "    for k in range(n_speakers_per_part):\n",
        "        pan = np.random.randint(panning[part][0], panning[part][1])\n",
        "        if any(x in part.lower() for x in ('soprano', 'alto', 'female')):\n",
        "            speaker_id = torch.LongTensor([next(female_speakers)]).cuda()\n",
        "        else:\n",
        "            speaker_id = torch.LongTensor([next(male_speakers)]).cuda()\n",
        "        print(\"{} MellotronID {} pan {}\".format(part, speaker_id.item(), pan))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            mel_outputs, mel_outputs_postnet, gate_outputs, alignments_transfer = tacotron.inference_noattention(\n",
        "                (text_encoded, mel, speaker_id, pitch_contour*frequency_scaling, rhythm))\n",
        "\n",
        "            audio = denoiser(waveglow.infer(mel_outputs_postnet, sigma=0.8), 0.01)[0, 0]\n",
        "            audio = audio.cpu().numpy()\n",
        "            audio = panner(audio, pan)\n",
        "            audio_stereo[:audio.shape[0]] += audio            \n",
        "            write(\"{} {}.wav\".format(part, speaker_id.item()), hparams.sampling_rate, audio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwtiK55Pi5Vd"
      },
      "source": [
        "audio_stereo = audio_stereo / np.max(np.abs(audio_stereo))\n",
        "write(\"audio_stereo.wav\", hparams.sampling_rate, audio_stereo)\n",
        "ipd.Audio([audio_stereo[:,0], audio_stereo[:,1]], rate=hparams.sampling_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c7iJ0kuXULe"
      },
      "source": [
        "## end of notebook"
      ]
    }
  ]
}