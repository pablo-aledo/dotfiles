{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8umvDNC4pEbc"
      },
      "source": [
        "#BERT demo notebook\n",
        "###by Eyal Gruss\n",
        "###Hebrew support: Doron Adler\n",
        "###⭐ New: Hebrew poetry glitcher - ShirBert! ⭐\n",
        "###Based on https://huggingface.co/transformers\n",
        "<img src='https://i.pinimg.com/originals/1a/38/8d/1a388d9b1e1ce42f424e60ce5b9d88ff.png' width=\"400px\"/>\n",
        "\n",
        "###Image credit: Doron Adler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swp4_fNxl813"
      },
      "source": [
        "!pip install transformers\n",
        "!rm -f /content/shirbert_poems.txt\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
        "import re\n",
        "\n",
        "def run_model(text, add_special_tokens=True, embedding=False, use_cls=False):\n",
        "  # Tokenize input\n",
        "  tokenized_text = tokenizer.tokenize(text)\n",
        "  #print(tokenized_text)\n",
        "\n",
        "  # Convert token to vocabulary indices\n",
        "  indexed_tokens = tokenizer.encode(text, add_special_tokens=add_special_tokens or use_cls)\n",
        "\n",
        "  # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
        "  segments_ids = [0]*len(indexed_tokens)\n",
        "\n",
        "  # Convert inputs to PyTorch tensors\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "  # If you have a GPU, put everything on cuda\n",
        "  tokens_tensor = tokens_tensor.to('cuda')\n",
        "  segments_tensors = segments_tensors.to('cuda')\n",
        "\n",
        "  if not embedding:\n",
        "    # Predict all tokens\n",
        "    with torch.no_grad():\n",
        "        outputs = masked_model(tokens_tensor, token_type_ids=segments_tensors)\n",
        "    if add_special_tokens:\n",
        "      return indexed_tokens[1:-1], outputs[0][0][1:-1]  \n",
        "    return indexed_tokens, outputs[0][0]\n",
        "\n",
        "  with torch.no_grad():\n",
        "      outputs = bert_model(tokens_tensor, token_type_ids=segments_tensors)\n",
        "  encoded_layers = outputs.last_hidden_state[0].cpu()\n",
        "  if use_cls:\n",
        "    return encoded_layers[0] \n",
        "  return encoded_layers.mean(axis=0)\n",
        "\n",
        "def predict_missing_word(text, topn=10, add_special_tokens=True):\n",
        "  indexed_tokens, predictions = run_model(text, add_special_tokens=add_special_tokens)\n",
        "  \n",
        "  # Mask a token that we will try to predict back with `BertForMaskedLM`\n",
        "  masked_index = indexed_tokens.index(tokenizer.convert_tokens_to_ids('[MASK]'))\n",
        "\n",
        "  predicted_inds = torch.argsort(-predictions[masked_index])\n",
        "  predicted_probs = [round(p.item(), 4) for p in torch.softmax(predictions[masked_index], 0)[predicted_inds]]\n",
        "  predicted_tokens = tokenizer.convert_ids_to_tokens([ind.item() for ind in predicted_inds])\n",
        "  return list(zip(predicted_tokens, predicted_probs))[:topn]\n",
        "\n",
        "def complete_missing_word(text, add_special_tokens=True):\n",
        "  word = predict_missing_word(text, topn=1, add_special_tokens=add_special_tokens)[0][0]\n",
        "  return text.replace('[MASK]', word)\n",
        "\n",
        "def join_clean(tokens):\n",
        "  text = ' '.join(tokens)\n",
        "  text = text.replace('ך ##', 'כ').replace('ם ##', 'מ').replace('ן ##', 'נ').replace('ף ##', 'פ').replace(\"ץ' ##\", \"'צ\").replace('ץ ##', 'צ')\n",
        "  text = text.replace(' ##', '').strip()\n",
        "  text = tokenizer.clean_up_tokenization(text)\n",
        "  text = text.replace(' :', ':').replace(' ;', ';').replace(' )', ')').replace('( ', '(')\n",
        "  return text\n",
        "\n",
        "def get_word_probs(text, add_special_tokens=True, with_masking=False, skip_punct=False):\n",
        "  if not text:\n",
        "    return []\n",
        "  tokenized_text = tokenizer.tokenize(text)\n",
        "  if with_masking:\n",
        "    indexed_tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "    predicted_probs = []\n",
        "    for i in range(len(tokenized_text)):\n",
        "      if skip_punct and not re.search(r'\\w',tokenized_text[i]):\n",
        "        predicted_probs.append(1.001)\n",
        "        continue\n",
        "      masked_tokens = tokenized_text.copy()\n",
        "      masked_tokens[i] = '[MASK]'\n",
        "      text = join_clean(masked_tokens)\n",
        "      _, predictions = run_model(text, add_special_tokens=add_special_tokens)\n",
        "      predicted_probs.append(round(torch.softmax(predictions[i], 0)[indexed_tokens[i]].item(), 4))\n",
        "  else:\n",
        "    indexed_tokens, predictions = run_model(text, add_special_tokens=add_special_tokens)\n",
        "    predicted_probs = [round(torch.softmax(predictions[i], 0)[j].item(), 4) for i,j in enumerate(indexed_tokens)]\n",
        "  return list(zip(tokenized_text, predicted_probs))\n",
        "\n",
        "vav = 'ו'\n",
        "heb_prefixes = 'מ|ש|ה|כ|ל|ב|כש'\n",
        "fix_heb_prefix_pattern = re.compile(r'\\b('+vav+'?(?:'+heb_prefixes+')|'+vav+') ')\n",
        "is_heb_prefix_pattern = re.compile(vav+'?('+heb_prefixes+')?')\n",
        "\n",
        "def fix_hebrew_prefix(text):\n",
        "  return fix_heb_prefix_pattern.sub(r'\\1', text)\n",
        "\n",
        "def is_hebrew_prefix(text):\n",
        "  return text and is_heb_prefix_pattern.fullmatch(text)\n",
        "\n",
        "alpha = re.compile(r'\\w(?<!(\\d|_))')\n",
        "hebchar = re.compile('[א-ת]')\n",
        "bad_punct = '\"()'\n",
        "long_punct = ['...','?!','!?']\n",
        "struct = re.compile(r\"[\\w']+|[^\\w'\\s]+\")\n",
        "\n",
        "class History:\n",
        "  def __init__(self):\n",
        "    self.hist = []\n",
        "    self.struct_hist = []\n",
        "    self.stuck_structs = set()\n",
        "    self.stuck_tokens = []\n",
        "\n",
        "def fix_one_word(text, add_special_tokens=True, with_masking=False, join_subwords=True, add_period=False, prevent_nonword=False, prefer_word=False, prevent_repeat=False, hist_to_prevent=None, freeze_punct=False, freeze_struct=False, max_local_changes=None, fix_heb_prefix=True, allow_heb_prefix=True):\n",
        "  if not text:\n",
        "    return text\n",
        "  period_added = False\n",
        "  if add_period and text and text[-1] not in '!?,.:;':\n",
        "    period_added = True\n",
        "    text += '.'\n",
        "  tokenized_text = tokenizer.tokenize(text)\n",
        "  if '[MASK]' in tokenized_text:\n",
        "    ind = tokenized_text.index('[MASK]')\n",
        "    bad_word = '[MASK]'\n",
        "  else:\n",
        "    probs = [p[1] for p in get_word_probs(text, add_special_tokens=add_special_tokens, with_masking=with_masking, skip_punct=freeze_punct)]\n",
        "    if period_added:\n",
        "      probs = probs[:-1]\n",
        "    inds = torch.argsort(torch.tensor(probs))\n",
        "    if freeze_punct:\n",
        "      inds = [i for i in inds if re.search(r'\\w',tokenized_text[i])]\n",
        "    save_tokenized = tokenized_text.copy()\n",
        "    for j in range(len(inds)):\n",
        "      ind = inds[j]\n",
        "      if join_subwords:\n",
        "        while ind and tokenized_text[ind].startswith('##'):\n",
        "          ind -= 1\n",
        "        bad_word = tokenized_text[ind]\n",
        "        i = ind+1  \n",
        "        while i<len(tokenized_text) and tokenized_text[i].startswith('##'):\n",
        "          bad_word += ' '+tokenized_text[ind]\n",
        "          del tokenized_text[i]\n",
        "      else:\n",
        "        bad_word = tokenized_text[ind]\n",
        "      tokenized_text[ind] = '[MASK]'\n",
        "      text = join_clean(tokenized_text)\n",
        "      if hist_to_prevent and max_local_changes:\n",
        "        local_struct = struct.sub('_',text.split('[MASK]',1)[0])+'[MASK]'+struct.sub('_',text.split('[MASK]',1)[1])\n",
        "      if j==len(inds)-1 or not hist_to_prevent or (text not in hist_to_prevent.hist and (not max_local_changes or hist_to_prevent.struct_hist.count(local_struct)<max_local_changes)):\n",
        "        break\n",
        "      if text not in hist_to_prevent.hist and local_struct not in hist_to_prevent.stuck_structs:\n",
        "        hist_to_prevent.stuck_structs.add(local_struct)\n",
        "        hist_to_prevent.stuck_tokens.append(bad_word)\n",
        "      tokenized_text = save_tokenized.copy()\n",
        "    if hist_to_prevent:\n",
        "      hist_to_prevent.hist.append(text)\n",
        "      if max_local_changes:\n",
        "        hist_to_prevent.struct_hist.append(local_struct)  \n",
        "  candidates = predict_missing_word(text, topn=None, add_special_tokens=add_special_tokens)\n",
        "  fix = bad_word\n",
        "  for word, _ in candidates:\n",
        "    word_alphanum = re.search(r'\\w', word)\n",
        "    bad_alphanum = re.search(r'\\w', bad_word)\n",
        "    if ((word.lower() != bad_word.lower() or not bad_alphanum) and\n",
        "       (not freeze_punct or word_alphanum) and \n",
        "       (not freeze_struct or is_hebrew_prefix(word)==is_hebrew_prefix(bad_word) and word.startswith('##')==bad_word.startswith('##') and bool(hebchar.fullmatch(word))==bool(hebchar.fullmatch(bad_word)) and bool(word_alphanum)==bool(bad_alphanum)) and\n",
        "       (not prevent_nonword or word != '[UNK]' and (word not in bad_punct or word==bad_word) and\n",
        "             (alpha.search(word) and (word[-1] not in 'ךםןףץ' or len(tokenized_text) == ind+1 or not tokenized_text[ind+1].startswith('##')) or not bad_alphanum and (not prefer_word or freeze_struct) and (bad_word not in bad_punct or word==bad_word)) and\n",
        "             (not is_hebrew_prefix(word) and (not hebchar.fullmatch(word) or not prefer_word and hebchar.fullmatch(bad_word)) or is_hebrew_prefix(word) and allow_heb_prefix and len(tokenized_text) > ind+1 and hebchar.search(tokenized_text[ind+1])) and\n",
        "             (not word.startswith('##') or ind and alpha.search(tokenized_text[ind-1])))):\n",
        "      fix = word\n",
        "      if not prevent_repeat or len(word.strip(' #')) == 1 or not re.search(r'\\b'+re.escape(word)+r'\\b', text, re.IGNORECASE):\n",
        "        break\n",
        "  text = text.replace('[MASK]', fix)\n",
        "  text = join_clean([text])\n",
        "  if fix_heb_prefix:\n",
        "    text = fix_hebrew_prefix(text)\n",
        "  if period_added:\n",
        "    text = text[:-1]\n",
        "  return text\n",
        "\n",
        "def cosim(vec1, vec2):\n",
        "  return np.dot(vec1,vec2)/np.linalg.norm(vec1)/np.linalg.norm(vec2)\n",
        "\n",
        "def sent_sim(base_sent, compare_to, add_special_tokens=True, use_cls=False):\n",
        "  results = []\n",
        "  if type(compare_to) == str:\n",
        "    compare_to = [compare_to]\n",
        "  e1 = run_model(base_sent, add_special_tokens=add_special_tokens, embedding=True, use_cls=use_cls)\n",
        "  for s in compare_to:\n",
        "    e2 = run_model(s, add_special_tokens=add_special_tokens, embedding=True, use_cls=use_cls)\n",
        "    results.append(cosim(e1,e2))\n",
        "  if len(results) == 1:\n",
        "    return results[0]\n",
        "  return results\n",
        "\n",
        "def mask_join(part1, part2, add_period=False):\n",
        "  s = part1 + ' [MASK] ' + part2  \n",
        "  if add_period and s[-1] not in '!?,.:;':\n",
        "    s += '.'\n",
        "  return s\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyuTQ1VZbtaY",
        "cellView": "form"
      },
      "source": [
        "#@title Choose model { run: \"auto\" }\n",
        "\n",
        "model = 'bert-base-uncased' #@param ['bert-base-uncased', 'bert-large-uncased', 'bert-large-uncased-whole-word-masking', 'bert-base-multilingual-cased']\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained(model)\n",
        "\n",
        "# Load pre-trained model weights and change to evaluation mode\n",
        "masked_model = BertForMaskedLM.from_pretrained(model)\n",
        "masked_model.eval()\n",
        "masked_model.to('cuda')\n",
        "\n",
        "bert_model = BertModel.from_pretrained(model)\n",
        "bert_model.eval()\n",
        "bert_model.to('cuda')\n",
        "\n",
        "print('\\nhttps://huggingface.co/'+model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X_j9L7hbxt2"
      },
      "source": [
        "predict_missing_word('The boy [MASK] to his school.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZQWt5nHpHuQ"
      },
      "source": [
        "predict_missing_word('Alex likes to have [MASK] with his best friend.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wxz1yC4iuO5a"
      },
      "source": [
        "get_word_probs('The boy want to his school.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nVlZtLIbLw-"
      },
      "source": [
        "get_word_probs('The boy want to his school.', with_masking=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQBnd7nKuQaB"
      },
      "source": [
        "fix_one_word('The boy want to his school.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO1wCIIE9NjH"
      },
      "source": [
        "get_word_probs('The boy want to his school')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqi-IBCgc8zO"
      },
      "source": [
        "get_word_probs('The boy want to his school', with_masking=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfYBa7LJOgI7"
      },
      "source": [
        "get_word_probs('The boy returned to his school.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhi5wBgNdHit"
      },
      "source": [
        "get_word_probs('The boy returned to his school.', with_masking=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqaKEf9WJwoi"
      },
      "source": [
        "get_word_probs('he said')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_8m7skhdavd"
      },
      "source": [
        "get_word_probs('he said', with_masking=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cpm0trhj9O6x"
      },
      "source": [
        "get_word_probs('he said.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gXeubd_dk-x"
      },
      "source": [
        "get_word_probs('he said.', with_masking=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdM53FBhNcgt"
      },
      "source": [
        "predict_missing_word('The prime minister [MASK]')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd6EJq0qQRfx"
      },
      "source": [
        "predict_missing_word('The prime minister [MASK].') #added period in the end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8a3AOMwQWst"
      },
      "source": [
        "complete_missing_word('The prime minister [MASK].')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBe6t-4sSjOl"
      },
      "source": [
        "get_word_probs('The crime minister resigned.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm6PeFkNdw4Z"
      },
      "source": [
        "get_word_probs('The crime minister resigned.', with_masking=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uudb8uwXTABM"
      },
      "source": [
        "get_word_probs('. The crime minister resigned.') #add period in beginning "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7BcVeMBd4_c"
      },
      "source": [
        "get_word_probs('. The crime minister resigned.', with_masking=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bakSezpnTSnl"
      },
      "source": [
        "fix_one_word('. The crime minister resigned.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sUcnoUm_J39"
      },
      "source": [
        "base_sent = 'she told me she loved me before she passed away'\n",
        "compare_to = [\n",
        "              'he told me he loved me before she passed away',\n",
        "              'he told me that you loved her before i passed away',\n",
        "              'i was very sad when my love died',\n",
        "              'you are my one and only love for eternity',\n",
        "              'i love pizza more than i love sex',\n",
        "              'we must have some pizza with onions',\n",
        "              'sieg heil',\n",
        "              'יאללה ביי'\n",
        "              ]   \n",
        "sorted(zip(sent_sim(base_sent, compare_to), compare_to),reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guvdS_IO-t8n",
        "cellView": "form"
      },
      "source": [
        "#@title Choose model { run: \"auto\" }\n",
        "\n",
        "model = 'TurkuNLP/wikibert-base-he-cased' #@param ['TurkuNLP/wikibert-base-he-cased', 'bert-base-multilingual-cased', 'avichr/heBERT', 'onlplab/alephbert-base']\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained(model)\n",
        "\n",
        "# Load pre-trained model weights and change to evaluation mode\n",
        "masked_model = BertForMaskedLM.from_pretrained(model)\n",
        "masked_model.eval()\n",
        "masked_model.to('cuda')\n",
        "\n",
        "bert_model = BertModel.from_pretrained(model)\n",
        "bert_model.eval()\n",
        "bert_model.to('cuda')\n",
        "\n",
        "print('\\nhttps://huggingface.co/'+model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KyIkg466Pbq"
      },
      "source": [
        "s = 'ישראל [MASK] ולתפארת'\n",
        "print(s+'\\n')\n",
        "predict_missing_word(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SOJ1W0GzRU_"
      },
      "source": [
        "s = 'ולתפארת [MASK] ישראל' #fixed order\n",
        "print(s+'\\n')\n",
        "predict_missing_word(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE43KB1sAQId"
      },
      "source": [
        "s = 'ולתפארת [MASK] ישראל' + '.' #added period\n",
        "print(s+'\\n')\n",
        "predict_missing_word(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkcmqNYj9X-e"
      },
      "source": [
        "get_word_probs('לא רוצה')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lyg_S_LqeNqe"
      },
      "source": [
        "get_word_probs('לא רוצה', with_masking=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duyf_c2M9aeG"
      },
      "source": [
        "get_word_probs('לא רוצה.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NlkoE_JeSnY"
      },
      "source": [
        "get_word_probs('לא רוצה.', with_masking=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf1xg6uV_4k-"
      },
      "source": [
        "#פרדי מרקורי מאסק זמר ומוזיקאי\n",
        "\n",
        "p1 = 'פרדי מרקורי'\n",
        "p2 = 'זמר ומוזיקאי'\n",
        "s = mask_join(p1,p2,add_period=True)\n",
        "print(s+'\\n')\n",
        "predict_missing_word(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1Z8wRcdAUt8"
      },
      "source": [
        "#פרדי מרקורי היה מאסק ומוזיקאי\n",
        "\n",
        "p1 = 'פרדי מרקורי היה'\n",
        "p2 = 'ומוזיקאי'\n",
        "s = mask_join(p1,p2,add_period=True)\n",
        "print(s+'\\n')\n",
        "predict_missing_word(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yI5X7wNwJvdv"
      },
      "source": [
        "#שירברט - משבש (משברט) שירה עברית\n",
        "#Hebrew poetry glitcher - ShirBert\n",
        "###Best used with TurkuNLP/wikibert-base-he-cased model\n",
        "###Warning: poetic experimentation below this line\n",
        "<img src='https://i.pinimg.com/originals/e2/8d/08/e28d0819c670b164eceb38d7e5acb466.jpg' width=\"400px\"/>\n",
        "\n",
        "###Image credit: [thismuppetdoesnotexist.com](https://thismuppetdoesnotexist.com) by Doron Adler and EG\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh6KJn4s16bR"
      },
      "source": [
        "nikud = re.compile('[\\u0591-\\u05bd\\u05bf-\\u05c2\\u05c4\\u05c5\\u05c7]') # remove all nikud and teamim except makaf, sof-pasuk, nun-hafukha\n",
        "stopwords = []\n",
        "\n",
        "def get_tokens(line, min_token=1, max_token=None, stopwords=[], stuck_tokens=[], orig=False, ret_cnt=False):\n",
        "  if orig:\n",
        "    tokens = re.findall(r\"(?:\\w'?)+\", line)\n",
        "    collection = dict.fromkeys(tokens) # preserve order\n",
        "  else:\n",
        "    line = nikud.sub('',line).lower()\n",
        "    tokens = re.findall(r\"(?:\\w'?){\"+str(min_token)+r\",}\", line)\n",
        "    unstuck_tokens = tokens.copy()\n",
        "    for stuck_token in stuck_tokens:\n",
        "      if stuck_token in unstuck_tokens:\n",
        "        unstuck_tokens.remove(stuck_token)\n",
        "    unstuck_tokens = [x[:(None if orig else max_token)].replace('ך', 'כ').replace('ם', 'מ').replace('ן', 'נ').replace('ף', 'פ').replace('ץ', 'צ') for x in unstuck_tokens if x not in stopwords]\n",
        "    collection = set(unstuck_tokens)\n",
        "  if ret_cnt:\n",
        "    return collection, len(tokens)\n",
        "  return collection\n",
        "\n",
        "def glitch_line(line, min_token=2, max_token=5, short_line=2, intersect=1, with_masking=False, add_period=False, prevent_nonword=True, prefer_word=True, prevent_repeat=False, hist_to_prevent=None, freeze_punct=False, freeze_struct=False, max_local_changes=None, fix_heb_prefix=False, allow_heb_prefix=True, stopwords=[], stateful_stop_criteria=False, backtrack_stop_criteria=True, verbose=False):\n",
        "  hist = []\n",
        "  hist_lower = []\n",
        "  hist_intersect = []\n",
        "  hist_log = set()\n",
        "  tokens = get_tokens(line, min_token=min_token, max_token=max_token, stopwords=stopwords)\n",
        "  if verbose:\n",
        "    print('>'*verbose+line)\n",
        "  line_intersect = tokens\n",
        "  while len(line_intersect) > (intersect if len(tokens) > short_line else 0):\n",
        "    hist.append(line)\n",
        "    hist_lower.append(line.lower())\n",
        "    hist_intersect.append(len(line_intersect))\n",
        "    hist_len = None if not stateful_stop_criteria or hist_to_prevent is None else len(set(hist_to_prevent.hist))\n",
        "    hist_log.add((line.lower(), hist_len))\n",
        "    line = fix_one_word(line, with_masking=with_masking, add_period=add_period, prevent_nonword=prevent_nonword, prefer_word=prefer_word, prevent_repeat=prevent_repeat, hist_to_prevent=hist_to_prevent, freeze_punct=freeze_punct, freeze_struct=freeze_struct, max_local_changes=max_local_changes, fix_heb_prefix=fix_heb_prefix, allow_heb_prefix=allow_heb_prefix)\n",
        "    line_intersect = tokens & get_tokens(line, min_token=min_token, max_token=max_token, stuck_tokens=hist_to_prevent.stuck_tokens if hist_to_prevent else [])\n",
        "    if verbose:\n",
        "      line_prob = 1 if not line else np.mean([p[1] for p in get_word_probs(line, with_masking=True)])\n",
        "      print('>'*verbose+'intersect=%d prob=%.3f %s'%(len(line_intersect), line_prob, line), line_intersect)\n",
        "    hist_len = None if not stateful_stop_criteria or hist_to_prevent is None else len(set(hist_to_prevent.hist))\n",
        "    if (line.lower(),hist_len) in hist_log:\n",
        "      len_intersect = len(line_intersect)\n",
        "      min_intersect = min(hist_intersect)\n",
        "      while backtrack_stop_criteria and (line.lower() in hist_lower or len_intersect>min_intersect):\n",
        "        line = hist[-1]\n",
        "        len_intersect = hist_intersect[-1]\n",
        "        del hist[-1]\n",
        "        del hist_lower[-1]\n",
        "        del hist_intersect[-1]\n",
        "      break\n",
        "  return line\n",
        "\n",
        "def shirbert(text, remove_nikud=True, split_eos=False, min_token=2, max_token=5, short_token=2, short_line=2, all_intersect=1, long_intersect=2, with_masking=False, add_period_logic=True, add_period_to_short=False, prevent_nonword=True, prefer_word=True, prevent_repeat=False, prevent_hist=False, freeze_punct=False, freeze_struct=False, max_local_changes=None, allow_heb_prefix=True, stopwords=stopwords, stateful_stop_criteria=False, backtrack_stop_criteria=True, last_level=None, save=True, verbose=False):\n",
        "  output = ''\n",
        "  min_sim = 1\n",
        "  levels = set()\n",
        "  orig_text = text\n",
        "  if remove_nikud:\n",
        "    text = nikud.sub('',text)\n",
        "  if split_eos:\n",
        "    text = text.replace('. ','.\\n').replace('! ','!\\n').replace('? ','?\\n')\n",
        "  for line in text.strip().splitlines():\n",
        "    hist_to_prevent = History() if prevent_hist else None\n",
        "    line = line.strip()\n",
        "    orig_line = line\n",
        "    all_tokens, num_tokens = get_tokens(line, min_token=min_token, max_token=max_token, stopwords=stopwords, ret_cnt=True)\n",
        "    long_tokens = get_tokens(line, min_token=max(min_token,short_token+1), max_token=max_token, stopwords=stopwords)\n",
        "    add_period = add_period_logic and (num_tokens > short_line or add_period_to_short and line and line[-1] not in '!?,.:;')\n",
        "    line = glitch_line(line, min_token=min_token, max_token=max_token, short_line=short_line, intersect=all_intersect, with_masking=with_masking, add_period=add_period, prevent_nonword=prevent_nonword, prefer_word=prefer_word, prevent_repeat=prevent_repeat, hist_to_prevent=hist_to_prevent, freeze_punct=freeze_punct, freeze_struct=freeze_struct, max_local_changes=max_local_changes, allow_heb_prefix=allow_heb_prefix, stopwords=stopwords, stateful_stop_criteria=stateful_stop_criteria, backtrack_stop_criteria=backtrack_stop_criteria, verbose=1 if verbose else 0)\n",
        "    sim = sent_sim(orig_line, line)\n",
        "    min_sim = min(min_sim,sim)\n",
        "    levels.add(1)\n",
        "    if verbose:\n",
        "      print(f'>sim={sim:.2f}')\n",
        "    tokens = get_tokens(line, min_token=min_token, max_token=max_token)\n",
        "    if (not last_level or last_level>1) and add_period and (len(long_tokens&tokens) > long_intersect or all_tokens&tokens and len(all_tokens) <= short_line):\n",
        "      line = glitch_line(orig_line, min_token=min_token, max_token=max_token, short_line=short_line, intersect=all_intersect, with_masking=with_masking, add_period=False, prevent_nonword=prevent_nonword, prefer_word=prefer_word, prevent_repeat=prevent_repeat, hist_to_prevent=hist_to_prevent, freeze_punct=freeze_punct, freeze_struct=freeze_struct, max_local_changes=max_local_changes, allow_heb_prefix=allow_heb_prefix, stopwords=stopwords, stateful_stop_criteria=stateful_stop_criteria, backtrack_stop_criteria=backtrack_stop_criteria, verbose=2 if verbose else 0)\n",
        "      sim = sent_sim(orig_line, line)\n",
        "      min_sim = min(min_sim,sim)\n",
        "      levels.add(2)\n",
        "      if verbose:\n",
        "        print(f'>>sim={sim:.2f}')\n",
        "    tokens = get_tokens(line, min_token=min_token, max_token=max_token)\n",
        "    if (not last_level or last_level>2) and len(long_tokens&tokens) > long_intersect or all_tokens&tokens and len(all_tokens) <= short_line:\n",
        "      orig_tokens = get_tokens(orig_line, orig=True)\n",
        "      untrunced_long_tokens = get_tokens(orig_line, min_token=max(min_token,short_token+1), stopwords=stopwords)\n",
        "      mask_line = orig_line.replace(sorted(orig_tokens,key=lambda x: (nikud.sub('',x).lower().replace('ך', 'כ').replace('ם', 'מ').replace('ן', 'נ').replace('ף', 'פ').replace('ץ', 'צ') not in untrunced_long_tokens, -len(nikud.sub('',x))))[0], '[MASK]', 1)\n",
        "      add_period = num_tokens > short_line or add_period_to_short and mask_line and mask_line[-1] not in '!?,.:;'\n",
        "      line = glitch_line(mask_line, min_token=min_token, max_token=max_token, short_line=short_line, intersect=all_intersect, with_masking=with_masking, add_period=add_period, prevent_nonword=prevent_nonword, prefer_word=prefer_word, prevent_repeat=prevent_repeat, hist_to_prevent=hist_to_prevent, freeze_punct=freeze_punct, freeze_struct=freeze_struct, max_local_changes=max_local_changes, allow_heb_prefix=allow_heb_prefix, stopwords=stopwords, stateful_stop_criteria=stateful_stop_criteria, backtrack_stop_criteria=backtrack_stop_criteria, verbose=3 if verbose else 0)\n",
        "      sim = sent_sim(orig_line, line)\n",
        "      min_sim = min(min_sim,sim)\n",
        "      levels.add(3)\n",
        "      if verbose:\n",
        "        print(f'>>>sim={sim:.2f}')\n",
        "      tokens = get_tokens(line, min_token=min_token, max_token=max_token)\n",
        "      if (not last_level or last_level>3) and add_period and (all_tokens&tokens and len(all_tokens) <= short_line):\n",
        "        line = glitch_line(mask_line, min_token=min_token, max_token=max_token, short_line=short_line, intersect=all_intersect, with_masking=with_masking, add_period=False, prevent_nonword=prevent_nonword, prefer_word=prefer_word, prevent_repeat=prevent_repeat, hist_to_prevent=hist_to_prevent, freeze_punct=freeze_punct, freeze_struct=freeze_struct, max_local_changes=max_local_changes, allow_heb_prefix=allow_heb_prefix, stopwords=stopwords, stateful_stop_criteria=stateful_stop_criteria, backtrack_stop_criteria=backtrack_stop_criteria, verbose=4 if verbose else 0)\n",
        "        sim = sent_sim(orig_line, line)\n",
        "        min_sim = min(min_sim,sim)\n",
        "        levels.add(4)\n",
        "        if verbose:\n",
        "          print(f'>>>>sim={sim:.2f}')\n",
        "    line = fix_hebrew_prefix(line)\n",
        "    print(line)\n",
        "    output += line+'\\n'\n",
        "  if verbose:\n",
        "    print(f'====min_sim={min_sim:.2f} levels={sorted(levels)}')\n",
        "  if save:\n",
        "    with open('/content/shirbert_poems.txt', 'a', encoding='utf8') as f:\n",
        "      f.write('input:\\n')\n",
        "      f.write(orig_text.strip()+'\\n\\n')\n",
        "      f.write('output:\\n')\n",
        "      f.write(output.strip()+'\\n\\n')\n",
        "  return output\n",
        "\n",
        "poem = shirbert('''\n",
        "התקוה\n",
        "נפתלי ה. אימבר\n",
        "\n",
        "כל עוד בלבב פנימה\n",
        "נפש יהודי הומיה,\n",
        "ולפאתי מזרח קדימה\n",
        "עין לציון צופיה,\n",
        "עוד לא אבדה תקותנו,\n",
        "התקוה בת שנות אלפים,\n",
        "להיות עם חפשי בארצנו,\n",
        "ארץ ציון וירושלים.\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWR8GX5zw57o"
      },
      "source": [
        "poem = shirbert('''\n",
        "_מעילי הפשוט ופנס על הגשר\n",
        "ליל הסתיו ושפתי הלחות מני גשם\n",
        "כך ראית אותי ראשונה התזכור\n",
        "והיה לי ברור כמו שתיים ושתיים\n",
        "כי אהיה בשבילך כמו לחם ומים\n",
        "וכאל מים ולחם אלי תחזו\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oFL15KoO0rU"
      },
      "source": [
        "poem = shirbert('''\n",
        "מהפכה, מתהפכים כמו 69\n",
        "היום כן משתלם הפשע\n",
        "אני מסתובב ברחובות מחפש מהות\n",
        "אבל הכל חוזר חביבי כמו תקליט שרוט\n",
        "ואינפלציה כבר מיפלציה מכאיבה כמו אפילציה\n",
        "מה זה אינטגרציה?! השיטה מניפולציה\n",
        "ואתה מחכה שמשהו יפסיק ת'כאב\n",
        "כמו כדור 9 מילימטר בתוך הלב\n",
        "אנחנו מטרות נעות\n",
        "אין שטרות - אין בעיות\n",
        "שטויות, הבטחות במים נכתבות\n",
        "חולמים בעיניים פתוחות - לרווחה\n",
        "פאק משרד הרווחה - לחם, עבודה\n",
        "תמונות, צרחות, סירנות ויריות\n",
        "אפילו בשכונת התקווה אבדו כל התקוות\n",
        "יורד על הברכיים ''אלי אנא''\n",
        "הנה זה בא (מי בא?) זינזאנה!\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNL24tWvhVOm"
      },
      "source": [
        "poem = shirbert('''\n",
        "שתדעו.\n",
        "מאת אודיה רוזנק\n",
        "\n",
        "אני ועוד מיליון מובטלים\n",
        "רואים את התמונות שאתם מעלים לאינסטגרם,\n",
        "שתדעו.\n",
        "אני ועוד מיליון מובטלים רואים את עוגות הקצפת\n",
        "הלבנות שלכם, את המקררים העמוסים\n",
        "יוגורטים אפס אחוז שומן,\n",
        "שתדעו.\n",
        "אני ועוד מיליון מובטלים רואים אתכם מורחים חמאה\n",
        "על חלות תוצרת בית מקמח כוסמין\n",
        "ומוסיפים פרוסת סלמון,\n",
        "שתדעו.\n",
        "אני ועוד מיליון מובטלים בלענו שעונים מעוררים\n",
        "טיק טק\n",
        "טיק טק\n",
        "טיק טק\n",
        "אתם שומעים?\n",
        "אני ועוד מיליון מובטלים מתכוננים לצאת\n",
        "לרחובות, לשבור לכם את החלונות\n",
        "לשרוף לכם את האסמים\n",
        "לתלוש לכם את הפנים, ולגלות את המסכות.\n",
        "שתדעו.\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA4qaw36Ag5a"
      },
      "source": [
        "poem = shirbert('''\n",
        "האגם הגדול.\n",
        "מאת רועי צ. ארד\n",
        " \n",
        "שוחה לבד בתוך האגם הגדול\n",
        "שוחה על בטני באגם הגדול\n",
        "שוחה על גבי באגם הגדול\n",
        "על צדי שוחה באגם הגדול\n",
        "מדוע איש לא מצטרף אלי באגם הגדול?\n",
        "אין גדר סביב האגם הגדול\n",
        "משתכשך בתוך אגם הגדול\n",
        "צולל בתוך אגם הגדול\n",
        "הדרך לדפוק את השיטה: האגם הגדול\n",
        "הצטרפי אלי אל האגם הגדול\n",
        "הצטרף אלי אל האגם הגדול\n",
        "מדוע אני לבד באגם הגדול?\n",
        "דבר לא מונע מכם לבוא אל האגם הגדול\n",
        "למשל אתה הקורא,\n",
        "אל נא תאמר \"אני רק הקורא\",\n",
        "הפשל המכנס, השלך החזיה,\n",
        "בוא עכשיו אל\n",
        "האגם הגדול!\n",
        "שחה עמוק בתוך האגם הגדול!\n",
        "שחה מהר בתוך האגם הגדול!\n",
        "שחה על גחונך בהאגם הגדול!\n",
        "שחה על העורף בהאגם הגדול!\n",
        "בוא עכשיו לכאן.\n",
        "פעם היו כאן רבים באגם הגדול\n",
        "אני היחיד שטבל באגם הגדול\n",
        "אפשר לטבוע בהאגם הגדול\n",
        "(אבל) אפשר למות מצחצוח יתר במרידול\n",
        "אז בואו בואו בואו אל האגם הגדול\n",
        "נצוף נצוף נצוף באגם הגדול\n",
        "אין כאן מים, רק קול\n",
        "נתחכך בתוך האגם הגדול\n",
        "בשרכם יוטח בבשרי באגם הגדול\n",
        "בוא עכשיו לכאן.\n",
        "מדוע אני לבד בתוך האגם הגדול\n",
        "מדוע אני לבד בתוך האגם הגדול\n",
        "כי אני לבד בתוך האגם הגדול\n",
        "כן, אני לבד באגם הגדול.\n",
        "אני לבד לבד לבד באגם הגדול\n",
        "לפעמים עם עוד כמה חברים\n",
        "מדוע אינכם מבינים שהכי סבבי באגם הגדול\n",
        "שהכי חינמי באגם הגדול\n",
        "שזה המקום היחיד בעיירה בלי גדרות, האגם הגדול\n",
        "ולא איזה אשד הפכפך, האגם הגדול\n",
        "והוא לא ממש גדול, האגם הגדול\n",
        "אפשר לשים אותו בבגאז' של פג'ו,\n",
        "בתא מטבעות מעור שסק\n",
        "בפנקסון סגול\n",
        "האגם הגדול האגם הגדול האגם הגדול\n",
        "הצטרפו אלי עכשיו לאגם הגדול\n",
        "הצטרפתו עמי באגם הגדול\n",
        "יש מקום לכולם באגם הגדול\n",
        "יש מקום לכולן באגם הגדול\n",
        "יש מקום לקולר באגם הגדול\n",
        "האגם הגד גד גד גדול\n",
        "האגם הגדול דול דול דול דול\n",
        "בואו אל האגם הגדול\n",
        "בואו אל האגם הגדול\n",
        "למה אתם נכנסים אל תוך האגם הגדול רק\n",
        "כשאני יוצא מהמים להתייבש?\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvXNbOxV9K4t"
      },
      "source": [
        "poem = shirbert('''\n",
        "שני ארזי\n",
        "\n",
        "מיקרוס פר בנרה הסינדוק משלם\n",
        "קולו דנצירסמי אוטונומאנטילן\n",
        "שוקולרצח בעד דם לא לוחשת מיד!\n",
        "הציבני לגוף אהיה אל אחד!\n",
        "הצילני מדם וזוועות משיחין\n",
        "\n",
        "סעודת רשעים בגבולות היומרין\n",
        "לא תראה בי אח מוות משתנה וזהה\n",
        "מה עסוק בי רום דע את גבולותיך בשן!\n",
        "אל תנשך אל תרצח אל תגנוב מתחתי\n",
        "לא תלויה בכוחו עבדך אשריי \n",
        "\n",
        "לא נתלה בעור מוות ודום עומק גופך\n",
        "מה שלי ונגזר עת דפך באפך!\n",
        "לא נשבר ורועד השימני בצל\n",
        "אשתנה אתגלד בתנועות יד ואל\n",
        "ובשמי אתרשם ואראה חיסורים\n",
        "\n",
        "לא תרביץ כשתשמע חיבורים אסורים!\n",
        "מה נגזר בעול מוות משתנה ונכנס\n",
        "היונה התפשטה במוחו הדרדס\n",
        "ואנס ותם רוח מתעקש לעוד דם\n",
        "לא שלום אדוני לא ירצח אף פעם\n",
        "\n",
        "לא ארשה שתרצח אדוני את דמי\n",
        "מה יקרה אם תיקח נשרים מבשרי\n",
        "והרצח ינגע באורות שארית\n",
        "אנדרגראונד נס ובא מתקשר לאורית\n",
        "מי זה כאן הפולש בשיחת הבירור\n",
        "\n",
        "מה אציל מתבייש רוטב אש הסיפור\n",
        "לא מקריח יודע בגד דמות שם צמוד\n",
        "שערות ופשרות רקדנית על עמוד\n",
        "לא נכנסת פואנטה בהרגל אפשרי\n",
        "לא יוצא אל ומטא חידוש השני\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86PZ2bOD5HBF"
      },
      "source": [
        "poem = shirbert('''\n",
        "גן נעול\n",
        "\n",
        "מִי אַתָּה? מַדּוּעַ יָד מוּשֶׁטֶת\n",
        "לֹא פּוֹגֶשֶׁת יַד אָחוֹת?\n",
        "וְעֵינַיִם אַךְ תַּמְתֵּנָּה רֶגַע\n",
        "וְהִנֵה שָׁפְלוּ כְּבָר נְבוֹכוֹת.\n",
        "\n",
        "גַּן נָעוּל. לֹא שְׁבִיל אֵלָיו, לֹא דֶרֶךְ.\n",
        "גַּן נָעוּל – אָדָם.\n",
        "הַאֵלֵךְ לִי? אוֹ אַכֶּה בַּסֶּלַע\n",
        "עַד זוֹב דָּם? \n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DIq9ATR89sa"
      },
      "source": [
        "# גרסה מאת דורון אדלר\n",
        "poem = shirbert('''\n",
        "רחל - גן נעול\n",
        "\n",
        "מי אתה? מדוע יד מושטת?\n",
        "לא פוגשת יד אחות\n",
        "ועיניים, אך תמתנה רגע\n",
        "והנה שפלו כבר נבוכות.\n",
        "\n",
        "גן נעול,\n",
        "לא שביל אליו, לא דרך.\n",
        "גם נעול, אדם, גן נעול.\n",
        "\n",
        "ההלך לי? \n",
        "או אכה בסלע?\n",
        "\n",
        "עד זוב דם\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umLERjig_UYN"
      },
      "source": [
        "poem=shirbert('''\n",
        "רק על עצמי\n",
        "\n",
        "רַק עַל עַצְמִי לְסַפֵּר יָדַעְתִּי.\n",
        "צַר עוֹלָמִי כְּעוֹלַם נְמָלָה,\n",
        "גַּם מַשָּׂאִי עָמַסְתִּי כָּמוֹהָ\n",
        "רַב וְכָבֵד מִכְּתֵפִי הַדַּלָּה.\n",
        "\n",
        "גַּם אֶת דַרְכִּי – כְּדַרְכָּהּ אֶל צַמֶּרֶת –\n",
        "דֶּרֶך מַכְאוֹב וְדֶרֶךְ עָמָל,\n",
        "יַד עֲנָקִים זְדוֹנָה וּבוֹטַחַת,\n",
        "יַד מִתְבַּדַּחַת שָׂמָה לְאַל.\n",
        "\n",
        "כָּל אָרְחוֹתַי הִלִּיז וְהִדְמִיע\n",
        "פַּחַד טָמִיר מִיַּד עֲנָקִים.\n",
        "לָמָּה קְרָאתֶם לִי, חוֹפֵי הַפֶּלֶא?\n",
        "לָמָּה כְּזַבְתֶּם, אוֹרוֹת רְחוֹקִים? \n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O-g8rxQuWnp"
      },
      "source": [
        "poem=shirbert('''\n",
        "כְּצִפּוֹר אֲשֶׁר יַשִּׂיגֶנָּה הַחֵץ\n",
        "בְּעוּפָהּ בַּמְּרוֹמִים, וְרֹאשָׁהּ כְּבָר שַׁח,\n",
        "וְהִיא, אַף כִּי-תֵדַע כִּי נוֹעַד הַקֵּץ,\n",
        "עוֹד אַחַת בִּכְנָפָהּ הִיא תַךְ;\n",
        "וּבִרְפוֹת כְּנָפֶיהָ — עוֹד רֶגַע כִּמְעָט\n",
        "הִיא תְלוּיָה כִקְסוּמָה מֵרוּם וּמֵרֶדֶת —\n",
        "לֹא נִיד! אִישׁוֹן-עֵינָהּ מוּל שַׁחַק רַק שָׁט,\n",
        "וְאוֹר בּוֹ, אוֹר נֶפֶשׁ נִפְרֶדֶת, —\n",
        "הָהּ, כֵּן גַּם אֲנִי עֵת נֶחְפַּזְתִּי לִמְרוֹם\n",
        "כָּל גִּבְעוֹת הַנֹּעַר — הִדְבִּיקַנִי יוֹם-שְׁכוֹל,\n",
        "וּבְהִמּוֹט כְּבָר רַגְלִי עַל עֶבְרֵי פִּי-תְהוֹם —\n",
        "אָמַרְתִּי עוֹד אַהֲבָה שְׁאֹל.\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRkDwXb2vkB5"
      },
      "source": [
        "poem=shirbert('''\n",
        "עין טובה\n",
        "\n",
        "איל על צוק עומד\n",
        "ובין שני קרני שנהביו\n",
        "כוכב זהב נוצץ\n",
        "גופו צמרי וחום\n",
        "וחזו לבן\n",
        "ועין רעה\n",
        "מצד שמאל תבוא לפתע\n",
        "מזנקת מתוך התהו\n",
        "והשחור כפסת שמים\n",
        "כמו יד\n",
        "אין כאן שום יפי\n",
        "רק פחד רב\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}