{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bbc-text-classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2ZhK6jC9Nnv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f009920-eb25-4a9a-cd07-72779ea87993"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoYqnsOI9gP-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "45a33afd-f894-40f3-d480-64caa6957b4c"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejIJG128-dVX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "outputId": "9fdf5f05-39dc-4ada-95a9-12330a33c7bb"
      },
      "source": [
        "# Read data frame\n",
        "drive_url = '/content/drive/My Drive/Colab Notebooks/bbc-text-classification/'\n",
        "df = pd.read_csv(drive_url+'bbc-data.csv')\n",
        "\n",
        "# Description of the dataset\n",
        "print('SHAPE OF DATASET: ', df.shape, '\\n\\nCOLUMNS IN DATASET: ', df.columns, '\\n\\nCATEGORIES: ', df.category.unique(), '\\n\\nDATA SAMPLE: \\n\\n', df.sample(n=5), '\\n\\n')\n",
        "\n",
        "# Plotting number of samples within each category\n",
        "print('NUMBER OF SAMPLES IN EACH CATEGORY: \\n')\n",
        "sns.countplot(df.category)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SHAPE OF DATASET:  (2225, 2) \n",
            "\n",
            "COLUMNS IN DATASET:  Index(['category', 'text'], dtype='object') \n",
            "\n",
            "CATEGORIES:  ['tech' 'business' 'sport' 'entertainment' 'politics'] \n",
            "\n",
            "DATA SAMPLE: \n",
            "\n",
            "            category                                               text\n",
            "742           sport  ref stands by scotland decisions the referee f...\n",
            "9     entertainment  last star wars  not for children  the sixth an...\n",
            "1456       business  french wine gets 70m euro top-up the french go...\n",
            "166   entertainment  springer criticises opera musical talk show ho...\n",
            "43            sport  disappointed scott in solid start allan scott ... \n",
            "\n",
            "\n",
            "NUMBER OF SAMPLES IN EACH CATEGORY: \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0e5c806b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wexlZDEABsafwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22L7TrqYtFiz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "2988f642-699a-41bd-e255-f1417019b625"
      },
      "source": [
        "# DATA CLEANING\n",
        "print('Data cleaning in progress...')\n",
        "\n",
        "# Tokenize\n",
        "df['text_clean'] = df['text'].apply(nltk.word_tokenize)\n",
        "print('Tokenization complete.')\n",
        "\n",
        "# Remove stop words\n",
        "stop_words=set(nltk.corpus.stopwords.words(\"english\"))\n",
        "df['text_clean'] = df['text_clean'].apply(lambda x: [item for item in x if item not in stop_words])\n",
        "print('Stop words removed.')\n",
        "\n",
        "# Remove numbers, punctuation and special characters (only keep words)\n",
        "regex = '[a-z]+'\n",
        "df['text_clean'] = df['text_clean'].apply(lambda x: [item for item in x if re.match(regex, item)])\n",
        "print('Numbers, punctuation and special characters removed.')\n",
        "\n",
        "# Lemmatization\n",
        "lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "df['text_clean'] = df['text_clean'].apply(lambda x: [lem.lemmatize(item, pos='v') for item in x])\n",
        "print('Lemmatization complete.\\nData cleaning complete.\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data cleaning in progress...\n",
            "Tokenization complete.\n",
            "Stop words removed.\n",
            "Numbers, punctuation and special characters removed.\n",
            "Lemmatization complete.\n",
            "Data cleaning complete.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiqczAO5OHM0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "b33714a9-337b-4a90-ae68-a61c95ee572e"
      },
      "source": [
        "# Classification using word2vec vectorizer\n",
        "\n",
        "vec_model = Word2Vec(df['text_clean'])\n",
        "w2v = dict(zip(vec_model.wv.index2word, vec_model.wv.syn0))\n",
        "\n",
        "class Vectorizer(object):\n",
        "    \n",
        "    def __init__(self, vec):\n",
        "        self.vec = vec\n",
        "        self.dim = len(vec.values())\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([np.mean([self.vec[w] for w in words if w in self.vec] or [np.zeros(self.dim)], axis=0) for words in X])\n",
        "\n",
        "class Classifier(object):\n",
        "    \n",
        "    def __init__(self, model, param):\n",
        "        self.model = model\n",
        "        self.param = param\n",
        "        self.gs = GridSearchCV(self.model, self.param, cv=5, error_score=0, refit=True)        \n",
        "\n",
        "    def fit(self, X, y):        \n",
        "        return self.gs.fit(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.gs.predict(X)\n",
        "\n",
        "clf_models = {\n",
        "    'Naive Bayes': GaussianNB(), \n",
        "    'SVC': SVC(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),  \n",
        "    'Perceptron': MLPClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier()\n",
        "}\n",
        "\n",
        "clf_params = {\n",
        "    'Naive Bayes': { }, \n",
        "    'SVC': { 'kernel': ['linear', 'rbf'] },\n",
        "    'Decision Tree': { 'min_samples_split': [2, 5] }, \n",
        "    'Perceptron': { 'activation': ['tanh', 'relu'] },\n",
        "    'Gradient Boosting': { 'min_samples_split': [2, 5] }\n",
        "}\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['category'], test_size=0.2, shuffle=True)\n",
        "\n",
        "for key in clf_models.keys():\n",
        "    \n",
        "    clf = Pipeline([('Word2Vec vectorizer', Vectorizer(w2v)), ('Classifier', Classifier(clf_models[key], clf_params[key]))])\n",
        "    \n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    \n",
        "    print(key, ':')\n",
        "    print(\"Accuracy: %1.3f \\tPrecision: %1.3f \\tRecall: %1.3f \\t\\tF1: %1.3f\\n\" % (accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='macro'), recall_score(y_test, y_pred, average='macro'), f1_score(y_test, y_pred, average='macro')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive Bayes :\n",
            "Accuracy: 0.348 \tPrecision: 0.373 \tRecall: 0.329 \t\tF1: 0.312\n",
            "\n",
            "SVC :\n",
            "Accuracy: 0.294 \tPrecision: 0.119 \tRecall: 0.264 \t\tF1: 0.163\n",
            "\n",
            "Decision Tree :\n",
            "Accuracy: 0.353 \tPrecision: 0.351 \tRecall: 0.351 \t\tF1: 0.350\n",
            "\n",
            "Perceptron :\n",
            "Accuracy: 0.303 \tPrecision: 0.134 \tRecall: 0.273 \t\tF1: 0.172\n",
            "\n",
            "Gradient Boosting :\n",
            "Accuracy: 0.425 \tPrecision: 0.416 \tRecall: 0.415 \t\tF1: 0.412\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTfHetcHmoaM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "9c1a2a1c-2c91-4df3-d0f2-880f1faefcb1"
      },
      "source": [
        "# Classification using TFIDF vectorizer\n",
        "\n",
        "# Vectorize training and testing data\n",
        "def Vectorize(vec, X_train, X_test):    \n",
        "    \n",
        "    X_train_vec = vec.fit_transform(X_train)\n",
        "    X_test_vec = vec.transform(X_test)\n",
        "    \n",
        "    print('Vectorization complete.\\n')\n",
        "    \n",
        "    return X_train_vec, X_test_vec\n",
        "\n",
        "# Use multiple classifiers and grid search for prediction\n",
        "def ML_modeling(models, params, X_train, X_test, y_train, y_test):    \n",
        "    \n",
        "    if not set(models.keys()).issubset(set(params.keys())):\n",
        "        raise ValueError('Some estimators are missing parameters')\n",
        "\n",
        "    for key in models.keys():\n",
        "    \n",
        "        model = models[key]\n",
        "        param = params[key]\n",
        "        gs = GridSearchCV(model, param, cv=5, error_score=0, refit=True)\n",
        "        gs.fit(X_train, y_train)\n",
        "        y_pred = gs.predict(X_test)\n",
        "        \n",
        "        # Print scores for the classifier\n",
        "        print(key, ':', gs.best_params_)\n",
        "        print(\"Precision: %1.3f \\tRecall: %1.3f \\t\\tF1: %1.3f\\n\" % (precision_score(y_test, y_pred, average='macro'), recall_score(y_test, y_pred, average='macro'), f1_score(y_test, y_pred, average='macro')))\n",
        "    \n",
        "    return\n",
        "\n",
        "models = {\n",
        "    'Naive Bayes': MultinomialNB(), \n",
        "    'Decision Tree': DecisionTreeClassifier(),  \n",
        "    'Perceptron': MLPClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier()\n",
        "}\n",
        "\n",
        "params = {\n",
        "    'Naive Bayes': { 'alpha': [0.5, 1], 'fit_prior': [True, False] }, \n",
        "    'Decision Tree': { 'min_samples_split': [1, 2, 5] }, \n",
        "    'Perceptron': { 'alpha': [0.0001, 0.001], 'activation': ['tanh', 'relu'] },\n",
        "    'Gradient Boosting': { 'learning_rate': [0.05, 0.1], 'min_samples_split': [2, 5] }\n",
        "}\n",
        "\n",
        "# Encode label categories to numbers\n",
        "enc = LabelEncoder()\n",
        "df['category'] = enc.fit_transform(df['category'])\n",
        "labels = list(enc.classes_)\n",
        "\n",
        "# Train-test split and vectorize\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['category'], test_size=0.2, shuffle=True)\n",
        "X_train_vec, X_test_vec = Vectorize(TfidfVectorizer(), X_train, X_test)\n",
        "\n",
        "ML_modeling(models, params, X_train_vec, X_test_vec, y_train, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectorization complete.\n",
            "\n",
            "Naive Bayes : {'alpha': 0.5, 'fit_prior': False}\n",
            "Precision: 0.940 \tRecall: 0.943 \t\tF1: 0.939\n",
            "\n",
            "Decision Tree : {'min_samples_split': 2}\n",
            "Precision: 0.806 \tRecall: 0.808 \t\tF1: 0.806\n",
            "\n",
            "Perceptron : {'activation': 'relu', 'alpha': 0.0001}\n",
            "Precision: 0.969 \tRecall: 0.971 \t\tF1: 0.970\n",
            "\n",
            "Gradient Boosting : {'learning_rate': 0.1, 'min_samples_split': 2}\n",
            "Precision: 0.954 \tRecall: 0.954 \t\tF1: 0.953\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}