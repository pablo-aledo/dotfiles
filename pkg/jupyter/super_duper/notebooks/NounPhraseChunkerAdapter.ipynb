{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "NounPhraseChunkerAdapter.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xi29suMHZX2g"
      },
      "source": [
        "# **AdapterHub** quickstart example for **chunk phrase** extraction. \n",
        "\n",
        "###In this particular notebook, we'll focus on extracting **noun** phrases. However, given that the model was trained on all chunk phrase types, one could easily extract other chunks as well. See the available labels below.\n",
        "\n",
        "Let's install adapter-transformers from github/master, import the required modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2Zf8VWV93qQ"
      },
      "source": [
        "!pip install git+https://github.com/adapter-hub/adapter-transformers.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mc8xdqyK9wsK"
      },
      "source": [
        "from typing import Dict\n",
        "import string\n",
        "import numpy as np\n",
        "from transformers import AutoConfig, AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbXJ55nrbOoW"
      },
      "source": [
        "Here are the chunk labels in IOB format.    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5FKrAY49wso"
      },
      "source": [
        "labels = [\"O\", \"B-ADVP\", \"B-INTJ\", \"B-LST\", \"B-PRT\", \"B-NP\", \"B-SBAR\", \"B-VP\", \"B-ADJP\", \"B-CONJP\", \"B-PP\",\n",
        "               \"I-ADVP\", \"I-INTJ\", \"I-LST\", \"I-PRT\", \"I-NP\", \"I-SBAR\", \"I-VP\", \"I-ADJP\", \"I-CONJP\", \"I-PP\"]\n",
        "label_map: Dict[int, str] = {i: label for i, label in enumerate(labels)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0XV6DnegF9l"
      },
      "source": [
        "Next, we load a standard Bert model and its tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrFLCe229wsx"
      },
      "source": [
        "model_name = \"bert-base-uncased\"\n",
        "config = AutoConfig.from_pretrained(model_name,\n",
        "                                    num_labels=len(labels),\n",
        "                                    id2label=label_map,\n",
        "                                    label2id={label: i for i, label in enumerate(labels)})\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name, config=config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKvf1-itgotc"
      },
      "source": [
        "Now, we'll load the chunking adapter. It's light-weight and appx 3MB! The F1 accuracy of this model was 91.3. We can now leverage adapter to predict the chunking tags of words in sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFz6GRPygnjw"
      },
      "source": [
        "model.load_adapter(\"chunk/conll2003@vblagoje\", \"text_task\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlttRsoYgQiO"
      },
      "source": [
        "We'll also need a helper function to wrap model inferencing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAaXghiv9wsb"
      },
      "source": [
        "def predict(sentence):\n",
        "    tokens = tokenizer.encode(\n",
        "        sentence,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation= \"only_first\",\n",
        "        max_length=tokenizer.max_len,\n",
        "    )\n",
        "    preds = model(tokens, adapter_names=['chunk'])[0]\n",
        "    preds = preds.detach().numpy()\n",
        "    preds = np.argmax(preds, axis=2)\n",
        "    return tokenizer.tokenize(sentence), preds.squeeze()[1:-1] # chop of CLS and SEP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWVMznsngS6e"
      },
      "source": [
        "And a filtering function to clean up the resulting list of noun chunks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJpHpcI776S1"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def filter_chunk(s):    \n",
        "    # Isolate and remove punctuations except '?'\n",
        "    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\/\\,])', r' \\1 ', s)\n",
        "    s = re.sub(r'[^\\w\\s\\?]', ' ', s)\n",
        "    # Remove some special characters\n",
        "    s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n",
        "    # Remove stopwords except 'not' and 'can'\n",
        "    s = \" \".join([word for word in s.split()\n",
        "                  if word not in stopwords.words('english')\n",
        "                  or word in ['not', 'can']])\n",
        "    # Remove trailing whitespace\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmqRyj9egcRG"
      },
      "source": [
        "Next, we'll need to extract noun phrase chunks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIOGNzXXmOyk"
      },
      "source": [
        "def decode(chunk):\n",
        "  return tokenizer.convert_tokens_to_string(chunk)\n",
        "\n",
        "def extract_chunks(sentence):\n",
        "  all_chunks = []\n",
        "  chunks = []\n",
        "  tokens, labels = predict(sentence)  \n",
        "  for w, l in zip(tokens, labels):\n",
        "    l = label_map[l]\n",
        "    #print(f\"-{w}-{l}\")\n",
        "    # is this a new noun phrase?\n",
        "    if l == 'B-NP':      \n",
        "        if len(chunks) > 0:\n",
        "            all_chunks.append(\"\".join(decode(chunks)))      \n",
        "        chunks = [w] \n",
        "    # or another word of some compound noun phrase \n",
        "    elif l == 'I-NP':\n",
        "      chunks.append(w)      \n",
        "\n",
        "  #last noun phrase\n",
        "  if len(chunks) > 0:\n",
        "      all_chunks.append(\"\".join(decode(chunks)))\n",
        "\n",
        "  all_chunks = [filter_chunk(chunk) for chunk in all_chunks]  \n",
        "  all_chunks = [chunk for chunk in all_chunks if len(chunk)>0]  \n",
        "  return all_chunks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlCjMY-ctXFT"
      },
      "source": [
        "print(extract_chunks(\"Autonomous cars move insurance liability toward manufacturers.\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XSYCHmMl30J"
      },
      "source": [
        "print(extract_chunks(\"Norges Bank’s Supervisory Council told key lawmakers gathered for a rare parliamentary hearing that risks remain for conflicts of interest and that rules were broken when manager Nicolai Tangen was hired to head the fund.\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poQur5QhtuDR"
      },
      "source": [
        "print(extract_chunks(\"The opposition Labor Party, the parliament’s biggest group, has yet to decide on whether it will push for the committee to get the government involved, its deputy leader Hadia Tajik said by phone before the hearing\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}