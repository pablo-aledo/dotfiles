{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Custom classifier on top of Transformers Language Models - example with PolEmo2.0 sentiment classification.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxQFlXbomk_0"
      },
      "source": [
        "Last time I wrote about training the language models from scratch, you can find this post <a href=\"https://zablo.net/blog/post/training-roberta-from-scratch-the-missing-guide-polish-language-model/?utm_source=blog&utm_medium=link&utm_campaign=roberta_from_scratch\">here</a>. Now it's time to take your pre-trained lamnguage model at put it into good use by fine-tuning it for real world problem, i.e text classification or sentiment analysis. In this post I will show how to take pre-trained language model and build **custom** classifier on top of it. As in the previous post - I cover all of the important parts in details and I point out possible mistakes.\n",
        "\n",
        "Why custom model? There are two answers. First is that the fun in deep learning begins only when you can do something custom with your model. Experiments, experiments and more experiments! Second thing is that by implmenting some parts on your own, you gain better understaing of different parts of the modeling itself, but also the whole training/fine-tuning process.\n",
        "\n",
        "## TL;DR\n",
        "This post covers:\n",
        "1. taking existing pre-trained language model and understanding it's output - here I use PolBERTa trained for Polish language\n",
        "1. building **custom classification head** on top of the LM\n",
        "1. using fast tokenizers to efficiently tokenize and pad input text as well as prepare attention masks \n",
        "1. preparing reproducible training code with PyTorch Lightning\n",
        "1. finding good starting learning rate for the model\n",
        "1. validating the trained model on PolEmo 2.0 dataset (benchmark for Polish language sentiment analysis with 4 classes)\n",
        "\n",
        "The whole post is available as a Colab Notebook, linked below in *Additional links and resources* section.\n",
        "\n",
        "## Libraries and preparation\n",
        "In this post I will use pretty standard set of ML-related libraries, including: transformers & tokenizers from HuggingFace, PyTorch Lightning, pandas, scikit-learn and LR Finder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M42o768Hmk_4",
        "outputId": "2250bf36-3562-4e09-fe2d-d7e3c659c758"
      },
      "source": [
        "!pip install transformers tokenizers pytorch-lightning"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in ./.conda/envs/pytorch/lib/python3.7/site-packages (2.5.1)\n",
            "Requirement already satisfied: tokenizers in ./.conda/envs/pytorch/lib/python3.7/site-packages (0.6.0)\n",
            "Requirement already satisfied: pytorch-lightning in ./.conda/envs/pytorch/lib/python3.7/site-packages (0.7.1)\n",
            "Requirement already satisfied: filelock in ./.conda/envs/pytorch/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in ./.conda/envs/pytorch/lib/python3.7/site-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./.conda/envs/pytorch/lib/python3.7/site-packages (from transformers) (2020.2.20)\n",
            "Requirement already satisfied: sacremoses in ./.conda/envs/pytorch/lib/python3.7/site-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: boto3 in ./.conda/envs/pytorch/lib/python3.7/site-packages (from transformers) (1.12.26)\n",
            "Requirement already satisfied: tqdm>=4.27 in ./.conda/envs/pytorch/lib/python3.7/site-packages (from transformers) (4.43.0)\n",
            "Requirement already satisfied: numpy in ./.conda/envs/pytorch/lib/python3.7/site-packages (from transformers) (1.18.1)\n",
            "Requirement already satisfied: sentencepiece in ./.conda/envs/pytorch/lib/python3.7/site-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: future>=0.17.1 in ./.conda/envs/pytorch/lib/python3.7/site-packages (from pytorch-lightning) (0.18.2)\n",
            "Requirement already satisfied: torch>=1.1 in ./.conda/envs/pytorch/lib/python3.7/site-packages (from pytorch-lightning) (1.4.0)\n",
            "Requirement already satisfied: tensorboard>=1.14 in ./.conda/envs/pytorch/lib/python3.7/site-packages (from pytorch-lightning) (2.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in ./.conda/envs/pytorch/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./.conda/envs/pytorch/lib/python3.7/site-packages (from requests->transformers) (1.25.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in ./.conda/envs/pytorch/lib/python3.7/site-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/envs/pytorch/lib/python3.7/site-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: joblib in ./.conda/envs/pytorch/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in ./.conda/envs/pytorch/lib/python3.7/site-packages (from sacremoses->transformers) (1.14.0)\n",
            "Requirement already satisfied: click in ./.conda/envs/pytorch/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in ./.conda/envs/pytorch/lib/python3.7/site-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in ./.conda/envs/pytorch/lib/python3.7/site-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.26 in ./.conda/envs/pytorch/lib/python3.7/site-packages (from boto3->transformers) (1.15.26)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in ./.conda/envs/pytorch/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (3.11.3)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in ./.conda/envs/pytorch/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (0.34.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./.conda/envs/pytorch/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in ./.conda/envs/pytorch/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in ./.conda/envs/pytorch/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (3.2.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in ./.conda/envs/pytorch/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (1.11.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in ./.conda/envs/pytorch/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (0.9.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in ./.conda/envs/pytorch/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (46.0.0.post20200309)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in ./.conda/envs/pytorch/lib/python3.7/site-packages (from tensorboard>=1.14->pytorch-lightning) (1.27.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./.conda/envs/pytorch/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.26->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in ./.conda/envs/pytorch/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.26->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./.conda/envs/pytorch/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in ./.conda/envs/pytorch/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (4.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in ./.conda/envs/pytorch/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (4.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.conda/envs/pytorch/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in ./.conda/envs/pytorch/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in ./.conda/envs/pytorch/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2rNCC-wmk_-"
      },
      "source": [
        "After running the cell below, you will probably have to restart the notebook (one time only)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2OlAXXwmk__"
      },
      "source": [
        "!git clone https://github.com/davidtvs/pytorch-lr-finder.git && cd pytorch-lr-finder && python setup.py install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZH3-uQbomlAC",
        "outputId": "0041aa7a-ecca-4ae9-c167-99f7adbcd09a"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from typing import List\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead, pipeline, AdamW, get_linear_schedule_with_warmup\n",
        "import logging\n",
        "import os\n",
        "from functools import lru_cache\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "from argparse import Namespace\n",
        "from sklearn.metrics import classification_report\n",
        "torch.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZrAsMZGmlAG"
      },
      "source": [
        "## Understanding pre-trained language model input / outputs\n",
        "In order to build your classifier on top of pre-trained language model you must first understand it outputs. Usually it requries reading the related paper or (if possible) reading the documentation / github code. Fortunately, it's easy for Transformers library - as the models are documented and the return value is described well. For my case (RoBERTa) the docs are here: https://huggingface.co/transformers/model_doc/roberta.html#robertamodel. \n",
        "\n",
        "We're interested what `RobertaForMaskedLM` returns. It this case, output format is the following:\n",
        "\n",
        "```python\n",
        "sequence_output, pooled_output, (hidden_states), (attentions)\n",
        "```\n",
        "\n",
        "Let's take a look at it ourselves:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4rKSuc0mlAH"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"marrrcin/PolBERTa-base-polish-cased-v1\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"marrrcin/PolBERTa-base-polish-cased-v1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3BHdT-mmlAL"
      },
      "source": [
        "We are actually interested only in base model, without the LM head which is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB1V-ZIhmlAM"
      },
      "source": [
        "base_model = model.roberta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSLbmL8HmlAQ"
      },
      "source": [
        "Let's see what does output look like by running the model forward pass with some input. We need to tokenize the input first:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owPFe6QBmlAR",
        "outputId": "de2bed15-fd67-4b49-9f97-1e960a041eab"
      },
      "source": [
        "text = \"Mówiąc otwarcie jesteście reliktami. Ilu wiedźminów jeszcze żyje? Kilku? Góra kilkunastu. Nie stanowicie k<mask>. Poza tym, Zakon nie każe sobie płacić za ratowanie ludzi od złego.\"\n",
        "enc = tokenizer.encode_plus(text)\n",
        "enc.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAB4lA9zmlAU"
      },
      "source": [
        "Tokenizer returns `input_ids` which are the tokens of your input text. Then `token_type_ids` and `attention_mask` - first one is used when you use LM for next-sentence prediction task, second one is to tell the model which tokens are from your input text and which are just padding tokens.\n",
        "\n",
        "For classification, you only need `input_ids` and `attention_mask` as token types are created on the fly by the internal model's forward function (<a href=\"https://github.com/huggingface/transformers/blob/f7dcf8fcea4d486544f221032625a97ad7dc5405/src/transformers/modeling_bert.py#L701\" target=\"blank\">source</a>).\n",
        "\n",
        "Let's run the model. The `.unsqueeze(0)` is just to simulate batch processing (in this case, it will be batch size of 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2XE5wxdmlAV",
        "outputId": "61214b36-d616-4dbf-ec8c-ab198f077d90"
      },
      "source": [
        "# sequence_output, pooled_output, (hidden_states), (attentions)\n",
        "out = base_model(torch.tensor(enc[\"input_ids\"]).unsqueeze(0), torch.tensor(enc[\"attention_mask\"]).unsqueeze(0))\n",
        "out[0].shape, out[1].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 12, 768]), torch.Size([1, 768]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 228
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWeld9D7mlAZ"
      },
      "source": [
        "We are interested only in first element of the tuple while building classifier as the second element is used for next-sentence prediction task and **it is literally said in the Transformers docs not to use it**:\n",
        "> This output is usually *not* a good summary\n",
        "of the semantic content of the input, you're often better with averaging or pooling\n",
        "the sequence of hidden-states for the whole input sequence. [<a href=\"https://github.com/huggingface/transformers/blob/f7dcf8fcea4d486544f221032625a97ad7dc5405/src/transformers/modeling_bert.py#L658\" target=\"blank\">source</a>]\n",
        "\n",
        "Output shape description:\n",
        "<img src=\"./model-output.png\"/>\n",
        "\n",
        "Keep in mind that for bigger batch sizes you have to pad to either longest text's lenght or to model's max supported length (which usually for BERT-like models is `512`).\n",
        "\n",
        "The second dimension contains internal representation of each of the input tokens, so for properly tokenized data, i.e:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOwyBlfmmlAa",
        "outputId": "b8ae8461-940f-49c6-b08a-a4cd351e492d"
      },
      "source": [
        "t = \"Mówiąc otwarcie jesteście reliktami.\"\n",
        "enc = tokenizer.encode_plus(t)\n",
        "token_representations = base_model(torch.tensor(enc[\"input_ids\"]).unsqueeze(0))[0][0]\n",
        "print(enc[\"input_ids\"])\n",
        "print(tokenizer.decode(enc[\"input_ids\"]))\n",
        "print(f\"Length: {len(enc['input_ids'])}\")\n",
        "print(token_representations.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 355, 304, 1748, 71, 12029, 28398, 807, 2578, 19022, 18, 2]\n",
            "<s> Mówiąc otwarcie jesteście reliktami.</s>\n",
            "Length: 12\n",
            "torch.Size([12, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOq6yEVymlAe"
      },
      "source": [
        "Representation for each token is as follows:\n",
        "* `0` = \"&lt;s&gt;\" token is represented by `token_representations[0]`\n",
        "* `355` = \" M\" token is represented by `token_representations[1]`\n",
        "\n",
        "and so on..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOF-IAq5mlAf"
      },
      "source": [
        "## Build custom classification head on top of LM\n",
        "OK, we understand the model's output and sizes, now we can build custom classification head on top of it.\n",
        "Have you found new awesome activation function? Or maybe you want to try-out different pooling methods to better fit your case? You're free to experiment. In this example, I will try out custom `Mish` activation function on top of straightforward classification head inspired by RoBERTa's original one (<a href=\"https://github.com/huggingface/transformers/blob/f7dcf8fcea4d486544f221032625a97ad7dc5405/src/transformers/modeling_roberta.py#L563\" target=\"blank\">source</a>) and DistilBERT's custom initialization. Because why not!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsLz1oo8mlAg"
      },
      "source": [
        "# from https://github.com/digantamisra98/Mish/blob/b5f006660ac0b4c46e2c6958ad0301d7f9c59651/Mish/Torch/mish.py\n",
        "@torch.jit.script\n",
        "def mish(input):\n",
        "    return input * torch.tanh(F.softplus(input))\n",
        "  \n",
        "class Mish(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return mish(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCepGeMPmlAk"
      },
      "source": [
        "class PolBERTaSentimentModel(nn.Module):\n",
        "    def __init__(self, base_model, n_classes, base_model_output_size=768, dropout=0.05):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(base_model_output_size, base_model_output_size),\n",
        "            Mish(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(base_model_output_size, n_classes)\n",
        "        )\n",
        "        \n",
        "        for layer in self.classifier:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                layer.weight.data.normal_(mean=0.0, std=0.02)\n",
        "                if layer.bias is not None:\n",
        "                    layer.bias.data.zero_()\n",
        "\n",
        "    def forward(self, input_, *args):\n",
        "        X, attention_mask = input_\n",
        "        hidden_states, _ = self.base_model(X, attention_mask=attention_mask)\n",
        "        \n",
        "        # here I use only representation of <s> token, but you can easily use more tokens,\n",
        "        # maybe do some pooling / RNNs... go crazy here!\n",
        "        return self.classifier(hidden_states[:, 0, :])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9cUflDbmlAn"
      },
      "source": [
        "<span style=\"color:red;font-weight:bold;\">Hint</span>\n",
        "> As a sanity check, I encourage you to run forward pass of the model on dummy data to validate the code. It also better to do this on CPU, as the stack traces of potential exceptions will be more meaningful. Mysterious CUDA erros very often are caused by simple Python-level errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2442pYnmlAo",
        "outputId": "e94118ba-f6ca-4433-934f-a0ee5dbc2065"
      },
      "source": [
        "classifier = PolBERTaSentimentModel(AutoModelWithLMHead.from_pretrained(\"marrrcin/PolBERTa-base-polish-cased-v1\").roberta, 3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3206ede66de644e9aae8c267d44605ec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=955.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9ZFeuGqmlAr"
      },
      "source": [
        "X = torch.tensor(enc[\"input_ids\"]).unsqueeze(0)\n",
        "attn = torch.tensor(enc[\"attention_mask\"]).unsqueeze(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b46bNRVAmlAu",
        "outputId": "4c59c14c-423b-4c19-d026-4ae9f83173af"
      },
      "source": [
        "classifier((X, attn))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5643, 0.2237, 0.1142]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g71UvZqcmlAx"
      },
      "source": [
        "The output is correct - we've run the model with batch size and expected 3 classes output (for CrossEntropyLoss)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd53ujsDmlAy"
      },
      "source": [
        "## Preparing fast tokenizers to use with text dataset\n",
        "When you prepare your dataset you have two options while implementing `Dataset` class: \n",
        "\n",
        "* return tokenized text and class in `__getitem__` function\n",
        "* return just text and class in `__getitem__` function and implement `Collate` function - this function will be used by PyTorch DataLoader to prepare batches\n",
        "\n",
        "Here I've chosen the second approach.\n",
        "\n",
        "Default tokenizer loaded above (as for Transformers v2.5.1) uses Python implementation. In order to leverage full potential of parallel Rust tokenizers, we need to save the tokenizer's internal data and then create instance of fast tokenizer with it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc-qjcgCmlAz"
      },
      "source": [
        "!mkdir -p tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8wCQHssmlA1",
        "outputId": "bd869f70-fe65-4451-e1f5-a9d7a6ef27b9"
      },
      "source": [
        "tokenizer.save_pretrained(\"tokenizer\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('tokenizer/vocab.json',\n",
              " 'tokenizer/merges.txt',\n",
              " 'tokenizer/special_tokens_map.json',\n",
              " 'tokenizer/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 275
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3EYQMjpmlA4",
        "outputId": "6ca2ab9e-0ddc-4485-cc84-27c61e239076"
      },
      "source": [
        "!ls tokenizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "merges.txt  special_tokens_map.json  tokenizer_config.json  vocab.json\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmDK5EJQmlA6"
      },
      "source": [
        "Once saved, implement `CollateFn` using fast tokenizers. As you can see, the whole batch can be tokenized in parallel and the tokenizer will also automatically return attention mask and pad input to max length of 512.\n",
        "\n",
        "<span style=\"color:red;font-weight:bold;\">Important!</span>\n",
        "> Ensure that all of the special tokens are in appropriate places and their IDs come from the actual tokenizer you're using - they should not be hardcoded, as different models will have different IDs for padding, BOS, EOS and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oz0xO_7cmlA7"
      },
      "source": [
        "class TokenizersCollateFn:\n",
        "    def __init__(self, max_tokens=512):\n",
        "        t = ByteLevelBPETokenizer(\n",
        "            \"tokenizer/vocab.json\",\n",
        "            \"tokenizer/merges.txt\"\n",
        "        )\n",
        "        t._tokenizer.post_processor = BertProcessing(\n",
        "            (\"</s>\", t.token_to_id(\"</s>\")),\n",
        "            (\"<s>\", t.token_to_id(\"<s>\")),\n",
        "        )\n",
        "        t.enable_truncation(max_tokens)\n",
        "        t.enable_padding(max_length=max_tokens, pad_id=t.token_to_id(\"<pad>\"))\n",
        "        self.tokenizer = t\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        encoded = self.tokenizer.encode_batch([x[0] for x in batch])\n",
        "        sequences_padded = torch.tensor([enc.ids for enc in encoded])\n",
        "        attention_masks_padded = torch.tensor([enc.attention_mask for enc in encoded])\n",
        "        labels = torch.tensor([x[1] for x in batch])\n",
        "        \n",
        "        return (sequences_padded, attention_masks_padded), labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN_RLWOzmlA9"
      },
      "source": [
        "## Getting data for classification\n",
        "Here I use PolEmo 2.0 dataset, which is a benchmark for sentiment analysis task in Polish language."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIMgJW32mlA-"
      },
      "source": [
        "!wget -O dataset_conll.zip https://clarin-pl.eu/dspace/bitstream/handle/11321/710/dataset_conll.zip?sequence=1&isAllowed=y "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn8gFkEXmlBA",
        "outputId": "15ee9cbc-2c8c-4e35-f723-2693a3091101"
      },
      "source": [
        "!unzip -n dataset_conll.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  dataset_conll.zip\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea2S3v6SmlBC",
        "outputId": "2552f176-b9a0-4f10-8c88-54ff053b816c"
      },
      "source": [
        "train_path, val_path, test_path = [f\"dataset_conll/all.text.{d}.txt\" for d in (\"train\", \"dev\", \"test\")]\n",
        "train_path, val_path, test_path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('dataset_conll/all.text.train.txt',\n",
              " 'dataset_conll/all.text.dev.txt',\n",
              " 'dataset_conll/all.text.test.txt')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 278
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9fA20nMmlBE"
      },
      "source": [
        "This dataset has CSV-like format with `__label__` text being the separator. Don't ask.\n",
        "Let's take a quick look at the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MK6--OgOmlBF",
        "outputId": "92c60f0b-ba8f-4031-a1b3-f9adbe4437b5"
      },
      "source": [
        "df = pd.read_csv(train_path, sep=\"__label__\", header=None, names=[\"text\", \"label\"], engine=\"python\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Na samym wejściu hotel śmierdzi . W pokojach j...</td>\n",
              "      <td>meta_minus_m</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ze swoimi dziećmi ( 10 lat i 2 latka ) chodzę ...</td>\n",
              "      <td>meta_plus_m</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Chciał em zestaw do podstawowych prac coś przy...</td>\n",
              "      <td>meta_minus_m</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Drogie Mamy . Mam o tym laktatorze takie same ...</td>\n",
              "      <td>meta_minus_m</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>własciwie nic się nie dowidział em , podczas b...</td>\n",
              "      <td>meta_minus_m</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text         label\n",
              "0  Na samym wejściu hotel śmierdzi . W pokojach j...  meta_minus_m\n",
              "1  Ze swoimi dziećmi ( 10 lat i 2 latka ) chodzę ...   meta_plus_m\n",
              "2  Chciał em zestaw do podstawowych prac coś przy...  meta_minus_m\n",
              "3  Drogie Mamy . Mam o tym laktatorze takie same ...  meta_minus_m\n",
              "4  własciwie nic się nie dowidział em , podczas b...  meta_minus_m"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 282
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edkG0sItmlBI",
        "outputId": "a4ff782a-5b1c-446c-a8c3-4feec877928e"
      },
      "source": [
        "df.label.unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['meta_minus_m', 'meta_plus_m', 'meta_zero', 'meta_amb'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 283
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0i-YVVtmlBK"
      },
      "source": [
        "You will need mapping between text label and integer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZayNBaCQmlBL"
      },
      "source": [
        "label2int = {\n",
        "  \"meta_minus_m\": 0,\n",
        "  \"meta_plus_m\": 1,\n",
        "  \"meta_zero\": 2,\n",
        "  \"meta_amb\": 3\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhC0mCwImlBN"
      },
      "source": [
        "As you can see below, the Dataset itself is really simple as it returns only the text and label. The rest is handled by the `TokenizersCollateFn` above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njN9QGy2mlBO"
      },
      "source": [
        "class PolEmo2Dataset(Dataset):\n",
        "    def __init__(self, path):\n",
        "        super().__init__()\n",
        "        self.data_column = \"text\"\n",
        "        self.class_column = \"class\"\n",
        "        self.data = pd.read_csv(path, sep=\"__label__\", header=None, names=[self.data_column, self.class_column],\n",
        "                               engine=\"python\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data.loc[idx, self.data_column], label2int[self.data.loc[idx, self.class_column]]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt4sttgomlBR"
      },
      "source": [
        "Sanity check:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQyWdtwcmlBS",
        "outputId": "89f87c44-c902-4000-be3b-fffcd4c8e89d"
      },
      "source": [
        "ds = PolEmo2Dataset(train_path)\n",
        "ds[13]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Była m przekonana , że w Sopocie jest jeden hotel , który może zapewnić najlepszą jakość , tym większe było moje zaskoczenie krótkim pobytem służbowym w Małej Anglii . Miała m okazję mieszkać w pięknym i wygodnym pokoju o bardzo wysokim standardzie . Przemiła obsługa , co jest kluczowe i często niesieciowe hotele nie potrafią sobie z tym poradzić , tutaj nie było takiego problemu . Bardzo przyjemna strefa wellness oraz spa . Doskonała restauracja śniadaniowa , przepięknie urządzona , śniadanie bez zarzutu i kawa . . doskonała , każdy kawosz zrozumie . Brak widoku na morze został zupełnie zrekompensowany . Polecam i mam nadzieję , że taki poziom już pozostanie . Dziękuję za miłe wspomnienia . ',\n",
              " 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 292
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWxhu1aSmlBU"
      },
      "source": [
        "## Preparing reproducible training code with PyTorch Lightning\n",
        "Most recently, PyTorch Lightning is my *go to* library for quick prototyping with PyTorch. You have clear API that is actually extension of the original PyTorch `nn.Module` one with all of the repeatable parts like training loop, validation loop, using GPUs, learning rate schedulers, gradient accumulation, tensorboard, checkpointing and many others handled for you.\n",
        "\n",
        "There is only a thin API you have to implement, which is `pl.LightningModule`. Required parts are:\n",
        "1. `__init__` to initialize custom variables in your model\n",
        "1. `forward` same as you would do in `nn.Module` - here I wrap my model inside of the `pl.LightningModule` but you could as well implement it directly here\n",
        "1. `training_step` - take care of the forward pass for the model and return loss\n",
        "1. `validation_step` and `test_step` - same as above but for the different learning phases\n",
        "1. `train_dataloader` - `DataLoader` for your training dataset\n",
        "1. `val_dataloader` and `test_dataloader` - same as above but for the different learning phases\n",
        "1. `configure_optimizers` - prepare optimizer and (optionally) LR scheudles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8E3qWmDmlBV"
      },
      "source": [
        "class TrainingModule(pl.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super().__init__()\n",
        "        self.model = PolBERTaSentimentModel(AutoModelWithLMHead.from_pretrained(\"marrrcin/PolBERTa-base-polish-cased-v1\").roberta, 4)\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "        self.hparams = hparams\n",
        "\n",
        "    def step(self, batch, step_name=\"train\"):\n",
        "        X, y = batch\n",
        "        loss = self.loss(self.forward(X), y)\n",
        "        loss_key = f\"{step_name}_loss\"\n",
        "        tensorboard_logs = {loss_key: loss}\n",
        "\n",
        "        return { (\"loss\" if step_name == \"train\" else loss_key): loss, 'log': tensorboard_logs,\n",
        "               \"progress_bar\": {loss_key: loss}}\n",
        "\n",
        "    def forward(self, X, *args):\n",
        "        return self.model(X, *args)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self.step(batch, \"train\")\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        return self.step(batch, \"val\")\n",
        "\n",
        "    def validation_end(self, outputs: List[dict]):\n",
        "        loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
        "        return {\"val_loss\": loss}\n",
        "        \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        return self.step(batch, \"test\")\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        return self.create_data_loader(self.hparams.train_path, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return self.create_data_loader(self.hparams.val_path)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return self.create_data_loader(self.hparams.test_path)\n",
        "                \n",
        "    def create_data_loader(self, ds_path: str, shuffle=False):\n",
        "        return DataLoader(\n",
        "                    PolEmo2Dataset(ds_path),\n",
        "                    batch_size=self.hparams.batch_size,\n",
        "                    shuffle=shuffle,\n",
        "                    collate_fn=TokenizersCollateFn()\n",
        "        )\n",
        "        \n",
        "    @lru_cache()\n",
        "    def total_steps(self):\n",
        "        return len(self.train_dataloader()) // self.hparams.accumulate_grad_batches * self.hparams.epochs\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = AdamW(self.model.parameters(), lr=self.hparams.lr)\n",
        "        lr_scheduler = get_linear_schedule_with_warmup(\n",
        "                    optimizer,\n",
        "                    num_warmup_steps=self.hparams.warmup_steps,\n",
        "                    num_training_steps=self.total_steps(),\n",
        "        )\n",
        "        return [optimizer], [{\"scheduler\": lr_scheduler, \"interval\": \"step\"}]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za7eXst2mlBa"
      },
      "source": [
        "### Hints and comments\n",
        "* `hparams` from `__init__` is a `Namespace`-type object, which holds all of the models hyperparameters; PyTorch Lightning takes good care of them when checkpointing the model\n",
        "* `*_step` functions can be implemented with the same method with argument\n",
        "* `*_dataloader` functions can also be implemented in this way\n",
        "* in `configure_optimizers` the line: `[{\"scheduler\": lr_scheduler, \"interval\": \"step\"}]` takes care of the step-based LR scheduler - it does LR warmup and then linear decay, implementation comes from Transformers library\n",
        "\n",
        "The code above is fairly simple and we don't need anything more here. For more robust examples I send you to PyTorch Lightning docs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_D9evoFmlBa"
      },
      "source": [
        "## Finding good starting learning rate for the model\n",
        "There is a method of approximating \"good\" learning rate for the training. I first encountered it in Fast.ai Deep Learning course. This approach is called `LR-Finder` and boils down to plotting loss of the training batch (starting from really small LR) while increasing learning rate up to the poit where the loss blows up. Then, the \"good\" LR is one of the values that appears when the Loss/Learning Rate curve is decreasing.\n",
        "\n",
        "<span style=\"color:red;font-weight:bold;\">Important!</span>\n",
        "> Keep in mind that this method does not guarantee to provide good learning rate for every dataset. It's just a method discovered to often work in practice.\n",
        "\n",
        "Here I use implementation for PyTorch from <a href=\"https://github.com/davidtvs/pytorch-lr-finder\" target=\"blank\">here</a> (installed at the top of this notebook).\n",
        "\n",
        "You need to specify initial learning rate and number of steps. Other parameters are optional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvcIVa_WmlBb",
        "outputId": "a608e226-34a6-4536-c645-61e04248c76f"
      },
      "source": [
        "from torch_lr_finder import LRFinder\n",
        "hparams_tmp = Namespace(\n",
        "    train_path=train_path,\n",
        "    val_path=val_path,\n",
        "    test_path=test_path,\n",
        "    batch_size=8,\n",
        "    warmup_steps=100,\n",
        "    epochs=3,\n",
        "    lr=lr,\n",
        "    accumulate_grad_batches=1,\n",
        ")\n",
        "module = TrainingModule(hparams_tmp)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(module.parameters(), lr=5e-7)\n",
        "lr_finder = LRFinder(module, optimizer, criterion, device=\"cuda\")\n",
        "lr_finder.range_test(module.train_dataloader(), end_lr=100, num_iter=100, accumulation_steps=hparams_tmp.accumulate_grad_batches)\n",
        "lr_finder.plot()\n",
        "lr_finder.reset()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "633b4b6e0d96462a8c0a4fec24de3dc1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Stopping early, the loss has diverged\n",
            "Learning rate search finished. See the graph with {finder_name}.plot()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0a7Wp7lmlBd"
      },
      "source": [
        "From the plot above we can guess that something between 1e-5 and 1e-4 would be a good learning rate, as everyhing higher results in increased loss.\n",
        "\n",
        "To ensure the right spot was chosen, you can plot it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB7uvFwsmlBe",
        "outputId": "f55daa37-822d-460d-a218-d34aed8957d8"
      },
      "source": [
        "lr = 4e-5\n",
        "lr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4e-05"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 307
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hd6JO7PmmlBg",
        "outputId": "3c0607a5-ef96-4b3d-9dd8-0051e1d9307b"
      },
      "source": [
        "lr_finder.plot(show_lr=lr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7efb82597650>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 308
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqCaP8sxmlBi"
      },
      "source": [
        "## Training the sentiment analysis classifier on PolEmo2.0\n",
        "We know the learning rate, we have the code for the model. Now it's time to run it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrabBnYymlBj"
      },
      "source": [
        "hparams = Namespace(\n",
        "    train_path=train_path,\n",
        "    val_path=val_path,\n",
        "    test_path=test_path,\n",
        "    batch_size=8,\n",
        "    warmup_steps=100,\n",
        "    epochs=3,\n",
        "    lr=lr,\n",
        "    accumulate_grad_batches=1\n",
        ")\n",
        "module = TrainingModule(hparams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMSWB44YmlBl"
      },
      "source": [
        "# just to get rid of any leftovers before the training - useful when you run different cells multiple times\n",
        "import gc; gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKohIVjAmlBo"
      },
      "source": [
        "### Using Trainer from PyTorch Lightning\n",
        "The `pl.Trainer` provides all-in-one solution for training, validation and testing. There is a lot of useful parameters for almost every aspect of training you can think of, everything is well documented <a href=\"https://pytorch-lightning.readthedocs.io/en/0.7.1/trainer.html\" target=\"blank\">here</a>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJYIZfXwmlBo"
      },
      "source": [
        "trainer = pl.Trainer(gpus=1, max_epochs=hparams.epochs, progress_bar_refresh_rate=10,\n",
        "                     accumulate_grad_batches=hparams.accumulate_grad_batches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq0uIxD8mlBq",
        "outputId": "0bee7be0-b3e8-40c4-a3b6-003e74d24f00"
      },
      "source": [
        "trainer.fit(module)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Validation sanity check', layout=Layout(flex='2'), max=5.…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ebb127a15de44656b58d35697f0de282",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=1.0), HTML(value='')), …"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Validating', layout=Layout(flex='2'), max=103.0, style=Pr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Validating', layout=Layout(flex='2'), max=103.0, style=Pr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Validating', layout=Layout(flex='2'), max=103.0, style=Pr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 313
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNvRlkNDmlBs"
      },
      "source": [
        "## Evaluating the trained model\n",
        "As this dataset is small, I've used scikit-learn to get the overall classification report from the model.\n",
        "\n",
        "<span style=\"color:red;font-weight:bold;\">Important!</span>\n",
        "> Be aware that for big datasets this approach might not be optimal as it transfers the data from GPU to CPU for the whole dataset and puts every prediction in memory. For big datasets it's better to implement metrics inside of the `pl.LightningModule` when the dataset is processed in batches.\n",
        "\n",
        "> Ensure that you disable gradients and put model into evaluation state (`module.eval()`) to disable dropouts during prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRbNPBdFmlBt",
        "outputId": "e56bcaf5-5245-4109-bcf3-784c62f15768"
      },
      "source": [
        "with torch.no_grad():\n",
        "    progress = [\"/\", \"-\", \"\\\\\", \"|\", \"/\", \"-\", \"\\\\\", \"|\"]\n",
        "    module.eval()\n",
        "    true_y, pred_y = [], []\n",
        "    for i, batch_ in enumerate(module.test_dataloader()):\n",
        "        (X, attn), y = batch_\n",
        "        batch = (X.cuda(), attn.cuda())\n",
        "        print(progress[i % len(progress)], end=\"\\r\")\n",
        "        y_pred = torch.argmax(module(batch), dim=1)\n",
        "        true_y.extend(y.cpu())\n",
        "        pred_y.extend(y_pred.cpu())\n",
        "print(\"\\n\" + \"_\" * 80)\n",
        "print(classification_report(true_y, pred_y, target_names=label2int.keys(), digits=6))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\\\n",
            "________________________________________________________________________________\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "meta_minus_m   0.880878  0.828909  0.854103       339\n",
            " meta_plus_m   0.867521  0.894273  0.880694       227\n",
            "   meta_zero   0.957983  0.966102  0.962025       118\n",
            "    meta_amb   0.574324  0.625000  0.598592       136\n",
            "\n",
            "    accuracy                       0.832927       820\n",
            "   macro avg   0.820177  0.828571  0.823854       820\n",
            "weighted avg   0.837433  0.832927  0.834617       820\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hixs_RUomlBx"
      },
      "source": [
        "## Additional links and resources\n",
        "* this notebook in Google Colab - \n",
        "* this notebook on GitHub - \n",
        "* PyTorch Lightning documentation - <a href=\"https://pytorch-lightning.readthedocs.io/en/0.7.1/\" target=\"blank\">https://pytorch-lightning.readthedocs.io/en/0.7.1/</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcdv7wQQmlBy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}