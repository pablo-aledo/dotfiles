# Awesome colab notebooks collection for ML experiments
## Research
| name | description | authors | links | colaboratory | update |
|------|-------------|:--------|:------|:------------:|:------:|
| VideoGPT | A conceptually simple architecture for scaling likelihood based generative modeling to natural videos | <ul><li>[Wilson Yan](https://wilson1yan.github.io/)</li> <li>[Yunzhi Zhang](https://zzyunzhi.github.io/)</li> <li>[Pieter Abbeel](https://people.eecs.berkeley.edu/~pabbeel/)</li> <li>[Aravind Srinivas](https://people.eecs.berkeley.edu/~aravind/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.10157), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1904.10509)</li><li>[data](https://www.crcv.ucf.edu/data/UCF101.php)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/wilson1yan/VideoGPT)</li><li>[project](https://wilson1yan.github.io/videogpt/index.html)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/wilson1yan/VideoGPT/blob/master/notebooks/Using_VideoGPT.ipynb) | 27.06.2021 |
| YOLOv5 | You Only Look Once | [Glenn Jocher](https://github.com/glenn-jocher) | <ul><li>[data](http://cocodataset.org/#upload)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/ultralytics/yolov5)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/ultralytics/yolov5), [<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/ultralytics/coco128)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb) | 24.06.2021 |
| Parallel WaveGAN | State-of-the-art non-autoregressive models to build your own great vocoder | [Tomoki Hayashi](https://kan-bayashi.github.io/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1910.11480), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1910.06711), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2005.05106)</li><li>[demo](https://kan-bayashi.github.io/ParallelWaveGAN/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/kan-bayashi/ParallelWaveGAN), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVIDIA/tacotron2), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/espnet/espnet)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/espnet2_tts_realtime_demo.ipynb) | 22.06.2021 |
| SimSwap | An efficient framework, called Simple Swap, aiming for generalized and high fidelity face swapping | <ul><li>[Xuanhong Chen](https://github.com/neuralchen)</li> <li>[Bingbing Ni](https://scholar.google.com.sg/citations?user=eUbmKwYAAAAJ)</li> <li>[Yanhao Ge](https://scholar.google.com/citations?user=h6tuBAcAAAAJ)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2106.06340)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/neuralchen/SimSwap)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/neuralchen/SimSwap/blob/master/SimSwap%20colab.ipynb) | 21.06.2021 |
| Customizing a Transformer Encoder | We will learn how to customize the encoder to employ new network architectures | [Chen Chen](https://github.com/chenGitHuber) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1706.03762)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/tensorflow/models/tree/master/official/nlp/modeling), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/tensorflow/models/blob/master/official/nlp/modeling/networks/encoder_scaffold.py)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/models/blob/master/official/colab/nlp/customize_encoder.ipynb) | 06.06.2021 |
| DFL-Colab | This project provides you IPython Notebook to use DeepFaceLab | [chervonij](https://github.com/chervonij) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2005.05535)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/iperov/DeepFaceLab)</li><li>[guide](https://mrdeepfakes.com/forums/thread-guide-deepfacelab-google-colab-tutorial)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/channel/UCTKBl8kB6DJ_qLnk1NGDGbQ)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/chervonij/DFL-Colab/blob/master/DFL_Colab.ipynb) | 02.06.2021 |
| Pixel2Style2Pixel | Encoding in Style: A StyleGAN Encoder for Image-to-Image Translation | <ul><li>[Elad Richardson](https://github.com/eladrich)</li> <li>[Yuval Alaluf](https://yuval-alaluf.github.io/)</li> <li>[Yotam Nitzan](https://yotamnitzan.github.io/)</li> <li>[Daniel Cohen-Or](https://www.cs.tau.ac.il/~dcor/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2008.00951)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/eladrich/pixel2style2pixel), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/HuangYG123/CurricularFace)</li><li>[project](https://eladrich.github.io/pixel2style2pixel/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/eladrich/pixel2style2pixel/blob/master/notebooks/inference_playground.ipynb) | 01.06.2021 |
| First Order Motion Model for Image Animation | Transferring facial movements from video to image | [Aliaksandr Siarohin](https://aliaksandrsiarohin.github.io/aliaksandr-siarohin-website/) | <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/AliaksandrSiarohin/first-order-model)</li><li>[<img src="images/neurips.svg" alt="neurips" height=20/>](https://papers.nips.cc/paper/2019/hash/31c0b36aef265d9221af80872ceb62f9-Abstract.html)</li><li>[project](https://aliaksandrsiarohin.github.io/first-order-model-website/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=u-0cQ-grXBQ)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/AliaksandrSiarohin/first-order-model/blob/master/demo.ipynb) | 28.05.2021 |
| YOLOv3 | You Only Look Once | [Glenn Jocher](https://github.com/glenn-jocher) | <ul><li>[data](http://cocodataset.org/#upload)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/ultralytics/yolov3)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/ultralytics/yolov3), [<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/ultralytics/coco128)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/ultralytics/yolov3/blob/master/tutorial.ipynb) | 27.05.2021 |
| Fine-tuning a BERT | We will work through fine-tuning a BERT model using the tensorflow-models PIP package | <ul><li>[Chen Chen](https://github.com/chenGitHuber)</li> <li>[Claire Yao](https://github.com/claireyao-fen)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1810.04805)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://tensorflow.org/hub)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/models/blob/master/official/colab/fine_tuning_bert.ipynb) | 24.05.2021 |
| ReStyle | A Residual-Based StyleGAN Encoder via Iterative Refinement | <ul><li>[Yuval Alaluf](https://yuval-alaluf.github.io/)</li> <li>[Or Patashnik](https://orpatashnik.github.io/)</li> <li>[Daniel Cohen-Or](https://danielcohenor.com/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.02699), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2008.00951), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2102.02766)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/yuval-alaluf/restyle-encoder), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/TreB1eN/InsightFace_Pytorch)</li><li>[project](https://yuval-alaluf.github.io/restyle-encoder/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/yuval-alaluf/restyle-encoder/blob/master/notebooks/inference_playground.ipynb) | 21.05.2021 |
| Motion Representations for Articulated Animation | Novel motion representations for animating articulated objects consisting of distinct parts | <ul><li>[Aliaksandr Siarohin](https://aliaksandrsiarohin.github.io/aliaksandr-siarohin-website/)</li> <li>[Oliver Woodford](https://ojwoodford.github.io/)</li> <li>[Jian Ren](https://alanspike.github.io/)</li> <li>[Menglei Chai](https://mlchai.com/)</li> <li>[Sergey Tulyakov](http://www.stulyakov.com/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.11280)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/snap-research/articulated-animation)</li><li>[project](https://snap-research.github.io/articulated-animation/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=gpBYN8t8_yY)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/AliaksandrSiarohin/articulated-animation/blob/master/demo.ipynb) | 29.04.2021 |
| EasyNMT | Easy to use, state-of-the-art machine translation for more than 100+ languages | [Nils Reimers](https://www.nils-reimers.de/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2008.00401), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2010.11125)</li><li>[demo](http://easynmt.net/demo/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/UKPLab/EasyNMT), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/Helsinki-NLP/Opus-MT)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1X47vgSiOphpxS5w_LPtjQgJmiSTNfRNC) | 26.04.2021 |
| SAM | Age Transformation Using a Style-Based Regression Model | <ul><li>[Yuval Alaluf](https://yuval-alaluf.github.io/)</li> <li>[Or Patashnik](https://orpatashnik.github.io/)</li> <li>[Daniel Cohen-Or](https://danielcohenor.com/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2102.02754)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/yuval-alaluf/SAM), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/eladrich/pixel2style2pixel), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch)</li><li>[project](https://yuval-alaluf.github.io/SAM/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/X_pYC_LtBFw)</li></ul> | [![Open In Colab](images/colab.svg)](http://colab.research.google.com/github/yuval-alaluf/SAM/blob/master/notebooks/animation_inference_playground.ipynb) | 26.04.2021 |
| SkinDeep | Remove Body Tattoo Using Deep Learning | [Vijish Madhavan](https://github.com/vijishmadhavan) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1805.08318), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1710.10196), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1707.02921), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1603.08155)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/vijishmadhavan/SkinDeep), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/jantic/DeOldify)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/vijishmadhavan/SkinDeep/blob/master/SkinDeep_good.ipynb) | 24.04.2021 |
| TediGAN | Framework for multi-modal image generation and manipulation with textual descriptions | <ul><li>[Weihao Xia](https://github.com/weihaox)</li> <li>[Yujiu Yang](http://www.fiesta.tsinghua.edu.cn/pi/3/24)</li> <li>[Jing-Hao Xue](http://www.homepages.ucl.ac.uk/~ucakjxu/)</li> <li>[Baoyuan Wu](https://sites.google.com/site/baoyuanwu2015/home)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2012.03308), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.08910)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/IIGROUP/TediGAN), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/weihaox/Multi-Modal-CelebA-HQ), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/ffhq-dataset), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/rosinality/stylegan2-pytorch/), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/fyu/lsun)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/L8Na2f5viAM)</li></ul> | [![Open In Colab](images/colab.svg)](http://colab.research.google.com/github/weihaox/TediGAN/blob/master/playground.ipynb) | 23.04.2021 |
| Geometry-Free View Synthesis | Is a geometric model required to synthesize novel views from a single image? | <ul><li>[Robin Rombach](https://github.com/rromb)</li> <li>[Patrick Esser](https://github.com/pesser)</li> <li>[Björn Ommer](https://hci.iwr.uni-heidelberg.de/people/bommer)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2104.07652)</li><li>[data](https://google.github.io/realestate10k/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/CompVis/geometry-free-view-synthesis), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/colmap/colmap)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/CompVis/geometry-free-view-synthesis/blob/master/scripts/braindance.ipynb) | 22.04.2021 |
| NeRViS | An algorithm for full-frame video stabilization by first estimating dense warp fields | <ul><li>[Yu-Lun Liu](http://www.cmlab.csie.ntu.edu.tw/~yulunliu/)</li> <li>[Wei-Sheng Lai](https://www.wslai.net/)</li> <li>[Ming-Hsuan Yang](https://faculty.ucmerced.edu/mhyang/)</li> <li>[Yung-Yu Chuang](https://www.csie.ntu.edu.tw/~cyy/)</li> <li>[Jia-Bin Huang](https://filebox.ece.vt.edu/~jbhuang/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2102.06205)</li><li>[data](http://liushuaicheng.org/SIGGRAPH2013/database.html)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/alex04072000/NeRViS), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/cxjyxxme/deep-online-video-stabilization), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/jinsc37/DIFRINT)</li><li>[project](https://alex04072000.github.io/NeRViS/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/KO3sULs4hso)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1l-fUzyM38KJMZyKMBWw_vu7ZUyDwgdYH) | 11.04.2021 |
| NeX | View synthesis based on enhancements of multiplane image that can reproduce NeXt-level view-dependent effects in real time | <ul><li>[Suttisak Wizadwongsa](linkedin.com/in/suttisak-wizadwongsa-763a931a5)</li> <li>[Pakkapon Phongthawee](http://pureexe.github.io/)</li> <li>[Jiraphon Yenphraphai](linkedin.com/in/jiraphon-yenphraphai-990ba6175)</li> <li>[Supasorn Suwajanakorn](https://www.supasorn.com/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2103.05606)</li><li>[data1](https://vistec-my.sharepoint.com/personal/pakkapon_p_s19_vistec_ac_th/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fpakkapon%5Fp%5Fs19%5Fvistec%5Fac%5Fth%2FDocuments%2Fpublic%2FVLL%2FNeX%2Fshiny%5Fdatasets&originalPath=aHR0cHM6Ly92aXN0ZWMtbXkuc2hhcmVwb2ludC5jb20vOmY6L2cvcGVyc29uYWwvcGFra2Fwb25fcF9zMTlfdmlzdGVjX2FjX3RoL0VuSVVoc1JWSk9kTnNaXzRzbWRoeWUwQjh6MFZseHFPUjM1SVIzYnAwdUd1cFE%5FcnRpbWU9WXRVQTQtQTcyVWc)</li><li>[data2](https://vistec-my.sharepoint.com/personal/pakkapon_p_s19_vistec_ac_th/_layouts/15/onedrive.aspx?originalPath=aHR0cHM6Ly92aXN0ZWMtbXkuc2hhcmVwb2ludC5jb20vOmY6L2cvcGVyc29uYWwvcGFra2Fwb25fcF9zMTlfdmlzdGVjX2FjX3RoL0VyalBSUkw5Sm5GSXA4TU42ZDFqRXVvQjNYVm94SmtmZlBqZm9QeWhIa2owZGc%5FcnRpbWU9bC0yYWctRTcyVWc&id=%2Fpersonal%2Fpakkapon%5Fp%5Fs19%5Fvistec%5Fac%5Fth%2FDocuments%2Fpublic%2FVLL%2FNeX%2Fmodified%5Fdataset)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/nex-mpi/nex-code), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/Fyusion/LLFF)</li><li>[project](https://nex-mpi.github.io/)</li><li>[vistec](https://vistec.ist/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=HyfkF7Z-ddA)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1hXVvYdAwLA0EFg2zrafJUE0bFgB_F7PU) | 25.03.2021 |
| encoder4editing | Designing an Encoder for StyleGAN Image Manipulation | <ul><li>[Omer Tov](https://github.com/omertov)</li> <li>[Yuval Alaluf](https://yuval-alaluf.github.io/)</li> <li>[Yotam Nitzan](https://yotamnitzan.github.io/)</li> <li>[Or Patashnik](https://orpatashnik.github.io/)</li> <li>[Daniel Cohen-Or](https://danielcohenor.com/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2102.02766)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/omertov/encoder4editing), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/eladrich/pixel2style2pixel)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/omertov/encoder4editing/blob/master/notebooks/inference_playground.ipynb) | 25.03.2021 |
| Big Sleep | Text to image generation, using OpenAI's CLIP and a BigGAN | [Phil Wang](https://github.com/lucidrains) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2103.00020), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1809.11096)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/lucidrains/big-sleep), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/openai/CLIP)</li><li>[<img src="images/reddit.svg" alt="reddit" height=20/>](https://www.reddit.com/r/bigsleep/comments/lxawb4/how_to_use_some_of_the_newer_features_of/), [<img src="images/reddit.svg" alt="reddit" height=20/>](https://www.reddit.com/r/bigsleep/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1MEWKbm-driRNF8PrU7ogS5o3se-ePyPb) | 17.03.2021 |
| Deep Daze | Text to image generation using OpenAI's CLIP and Siren | [Phil Wang](https://github.com/lucidrains) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2103.00020), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2006.09661)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/lucidrains/deep-daze), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/openai/CLIP)</li><li>[<img src="images/reddit.svg" alt="reddit" height=20/>](https://www.reddit.com/r/deepdaze/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1_YOHdORb0Fg1Q7vWZ_KlrtFe9Ur3pmVj) | 17.03.2021 |
| Taming Transformers for High-Resolution Image Synthesis | We combine the efficiancy of convolutional approaches with the expressivity of transformers by introducing a convolutional VQGAN, which learns a codebook of context-rich visual parts, whose composition is modeled with an autoregressive transformer | <ul><li>[Patrick Esser](https://github.com/pesser)</li> <li>[Robin Rombach](https://github.com/rromb)</li> <li>[Björn Ommer](https://hci.iwr.uni-heidelberg.de/people/bommer)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2012.09841)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/CompVis/taming-transformers)</li><li>[project](https://compvis.github.io/taming-transformers/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/CompVis/taming-transformers/blob/master/scripts/taming-transformers.ipynb) | 10.03.2021 |
| Talking Head Anime from a Single Image | The network takes as input an image of an anime character's face and a desired pose, and it outputs another image of the same character in the given pose | [Pramook Khungurn](https://pkhungurn.github.io/) | <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/pkhungurn/talking-head-anime-demo), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/lincolnhard/head-pose-estimation)</li><li>[project](https://pkhungurn.github.io/talking-head-anime/)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Virtual_YouTuber), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/MikuMikuDance)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/kMQCERkTdO0), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/T1Gp-RxFZwU), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/FioRJ6x_RbI)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/pkhungurn/talking-head-anime-demo/blob/master/tha_colab.ipynb) | 23.02.2021 |
| Multitrack MusicVAE | The models in this notebook are capable of encoding and decoding single measures of up to 8 tracks, optionally conditioned on an underlying chord | <ul><li>[Ian Simon](https://github.com/iansimon)</li> <li>[Adam Roberts](https://github.com/adarob)</li> <li>[Colin Raffel](https://colinraffel.com/)</li> <li>[Jesse Engel](https://github.com/jesseengel)</li> <li>[Curtis Hawthorne](https://github.com/cghawthorne)</li> <li>[Douglas Eck](https://github.com/douglaseck)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1806.00195)</li><li>[blog post](http://g.co/magenta/multitrack)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/Multitrack_MusicVAE.ipynb) | 17.02.2021 |
| NFNet | An adaptive gradient clipping technique, a significantly improved class of Normalizer-Free ResNets | <ul><li>[Andrew Brock](https://github.com/ajbrock)</li> <li>[Soham De](https://sohamde.github.io/)</li> <li>[Samuel L. Smith](https://scholar.google.co.uk/citations?user=fyEqU5oAAAAJ)</li> <li>[Karen Simonyan](https://scholar.google.com/citations?user=L7lMQkQAAAAJ)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2102.06171), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2101.08692)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/deepmind/deepmind-research/tree/master/nfnets), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/deepmind/jaxline)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/deepmind/deepmind-research/blob/master/nfnets/nfnet_demo_colab.ipynb) | 17.02.2021 |
| Open-Unmix | A deep neural network reference implementation for music source separation, applicable for researchers, audio engineers and artists | <ul><li>[Fabian-Robert Stöter](http://faroit.com/)</li> <li>[Antoine Liutkus](https://github.com/aliutkus)</li></ul> | <ul><li>[data](https://sigsep.github.io/datasets/musdb.html#musdb18-compressed-stems)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/sigsep/open-unmix-pytorch), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/sigsep/norbert)</li><li>[paper](https://www.theoj.org/joss-papers/joss.01667/10.21105.joss.01667.pdf)</li><li>[project](https://sigsep.github.io/open-unmix/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1mijF0zGWxN-KaxTnd0q6hayAlrID5fEQ) | 14.02.2021 |
| GPT-2 | Retrain an advanced text generating neural network on any text dataset using gpt-2-simple! | [Max Woolf](https://minimaxir.com/) | <ul><li>[blog post](https://minimaxir.com/2019/09/howto-gpt2/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/minimaxir/gpt-2-simple)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce) | 14.02.2021 |
| bsuite | A collection of carefully-designed experiments that investigate core capabilities of an RL agent with two main objectives | [Ian Osband](http://iosband.github.io/) | <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/deepmind/bsuite), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/openai/gym)</li><li>[paper](https://openreview.net/forum?id=rygf-kSYwH)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1rU20zJ281sZuMD1DHbsODFr1DbASL0RH) | 13.02.2021 |
| Wav2Lip | A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild | <ul><li>[Prajwal Renukanand](https://github.com/prajwalkr)</li> <li>[Rudrabha Mukhopadhyay](https://rudrabha.github.io/)</li> <li>[Vinay Namboodiri](https://vinaypn.github.io/)</li> <li>[C. V. Jawahar](https://faculty.iiit.ac.in/~jawahar/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2008.10010)</li><li>[data](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html)</li><li>[demo](http://bhaasha.iiit.ac.in/lipsync/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/Rudrabha/Wav2Lip)</li><li>[project](http://cvit.iiit.ac.in/research/projects/cvit-projects/a-lip-sync-expert-is-all-you-need-for-speech-to-lip-generation-in-the-wild/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=0fXaDCZNOJc)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/eyaler/avatars4all/blob/master/melaflefon.ipynb) | 12.02.2021 |
| CLIP | A neural network which efficiently learns visual concepts from natural language supervision | <ul><li>[Jong Wook](https://jongwook.kim/)</li> <li>[Alec Radford](http://newmu.github.io/)</li> <li>[Ilya Sutskever](http://www.cs.utoronto.ca/~ilya/)</li></ul> | <ul><li>[data](https://www.cs.toronto.edu/~kriz/cifar.html)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/openai/CLIP)</li><li>[paper](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf)</li><li>[project](https://openai.com/blog/clip/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/openai/clip/blob/master/Interacting_with_CLIP.ipynb) | 29.01.2021 |
| Adversarial Patch | A method to create universal, robust, targeted adversarial image patches in the real world | [Tom Brown](https://github.com/nottombrown) | [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1712.09665) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/cleverhans-lab/cleverhans/blob/master/examples/adversarial_patch/AdversarialPatch.ipynb) | 27.01.2021 |
| MSG-Net | Multi-style Generative Network with a novel Inspiration Layer, which retains the functionality of optimization-based approaches and has the fast speed of feed-forward networks | <ul><li>[Hang Zhang](https://hangzhang.org/)</li> <li>[Kristin Dana](https://www.ece.rutgers.edu/~kdana/dana.html)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1703.06953)</li><li>[project](http://computervisionrutgers.github.io/MSG-Net/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=oy6pWNWBt4Y)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/zhanghang1989/PyTorch-Multi-Style-Transfer/blob/master/msgnet.ipynb) | 25.01.2021 |
| Toon-Me | A fun project to toon portrait images | [Vijish Madhavan](https://github.com/vijishmadhavan) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1710.10196), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1707.02921), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1603.08155)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/vijishmadhavan/Toon-Me)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/vijishmadhavan/Light-Up/blob/master/Toon_Me_(Try_it_on_Colab).ipynb) | 22.01.2021 |
| Neural Style Transfer | Implementation of Neural Style Transfer in Keras 2.0+ | [Somshubra Majumdar](http://titu1994.github.io/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](http://arxiv.org/abs/1508.06576), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](http://arxiv.org/abs/1605.04603), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1606.05897)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/titu1994/Neural-Style-Transfer)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/titu1994/Neural-Style-Transfer/blob/master/NeuralStyleTransfer.ipynb) | 22.01.2021 |
| SkyAR | A vision-based method for video sky replacement and harmonization, which can automatically generate realistic and dramatic sky backgrounds in videos with controllable styles | [Zhengxia Zou](http://www-personal.umich.edu/~zzhengxi/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2010.11800)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/jiupinjia/SkyAR)</li><li>[project](https://jiupinjia.github.io/skyar/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=zal9Ues0aOQ)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/jiupinjia/SkyAR/blob/master/colab_demo.ipynb) | 18.01.2021 |
| Big GAN | Large Scale GAN Training for High Fidelity Natural Image Synthesis | [Google](https://www.tensorflow.org/hub) | [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1809.11096) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb) | 12.01.2021 |
| GrooVAE | Some applications of machine learning for generating and manipulating beats and drum performances | <ul><li>[Jon Gillick](https://www.jongillick.com/)</li> <li>[Adam Roberts](https://github.com/adarob)</li> <li>[Jesse Engel](https://github.com/jesseengel)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1905.06118)</li><li>[blog post](https://g.co/magenta/groovae)</li><li>[data](https://g.co/magenta/groove-datasets)</li><li>[<img src="images/git.svg" alt="git" height=20/>](http://goo.gl/magenta/musicvae-py)</li><li>[web app](https://groove-drums.glitch.me/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=x2YLmXzovDo)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/magenta-demos/blob/master/colab-notebooks/GrooVAE.ipynb) | 08.01.2021 |
| MusicXML Documentation | The goal of this notebook is to explore one of the magenta libraries for music | <ul><li>[Prakruti Joshi](https://github.com/prakruti-joshi)</li> <li>[Falak Shah](https://falaktheoptimist.github.io/)</li> <li>[Twisha Naik](https://github.com/twisha96)</li></ul> | <ul><li>[magenta](https://magenta.tensorflow.org/)</li><li>[music theory](http://musictheoryblog.blogspot.com/2008/02/learn-music-theory.html)</li><li>[musicXML](https://www.musicxml.com/for-developers/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/MusicXML_Document_Structure_Documentation.ipynb) | 08.01.2021 |
| SVG VAE | A colab demo for the SVG VAE model | [Raphael Gontijo Lopes](https://raphagl.com/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1904.02632)</li><li>[blog post](https://magenta.tensorflow.org/svg-vae)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/vae_svg_decoding.ipynb) | 08.01.2021 |
| PIFuHD | Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization | <ul><li>[Shunsuke Saito](https://github.com/shunsukesaito)</li> <li>[Tomas Simon](http://www.cs.cmu.edu/~tsimon/)</li> <li>[Jason Saragih](https://scholar.google.com/citations?user=ss-IvjMAAAAJ)</li> <li>[Hanbyul Joo](https://jhugestar.github.io/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2004.00452)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/pifuhd)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/uEDqCxvF5yc), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=8qnwbbDS8xk)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/11z58bl3meSzo6kFqkahMa35G5jmh2Wgt) | 05.01.2021 |
| Neural Magic Eye | Learning to See and Understand the Scene Behind an Autostereogram | <ul><li>[Zhengxia Zou](http://www-personal.umich.edu/~zzhengxi/)</li> <li>[Tianyang Shi](https://www.shitianyang.tech/)</li> <li>[Yi Yuan](https://yiyuan1991.github.io/)</li> <li>[Zhenwei Shi](http://levir.buaa.edu.cn/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2012.15692)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/jiupinjia/neural-magic-eye)</li><li>[project](https://jiupinjia.github.io/neuralmagiceye/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=Fkh7DEblqJ8)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1f59dFLJ748i2TleE54RkbUZSMo9Hyx7l) | 01.01.2021 |
| Flow-edge Guided Video Completion | Method first extracts and completes motion edges, and then uses them to guide piecewise-smooth flow completion with sharp edges | <ul><li>[Chen Gao](http://chengao.vision/)</li> <li>[Ayush Saraf](https://github.com/ayush29feb)</li> <li>[Johannes Kopf](https://johanneskopf.de/)</li> <li>[Jia-Bin Huang](https://filebox.ece.vt.edu/~jbhuang/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2009.01835)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/vt-vl-lab/FGVC)</li><li>[project](http://chengao.vision/FGVC/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=CHHVPxHT7rc)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1pb6FjWdwq_q445rG2NP0dubw7LKNUkqc) | 30.12.2020 |
| ArtLine | A Deep Learning based project for creating line art portraits | [Vijish Madhavan](https://github.com/vijishmadhavan) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1805.08318), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1710.10196), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1707.02921), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1603.08155)</li><li>[data](https://cg.cs.tsinghua.edu.cn/people/~Yongjin/APDrawingDB.zip)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/vijishmadhavan/ArtLine), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/yiranran/APDrawingGAN), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/jantic/DeOldify)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/vijishmadhavan/Light-Up/blob/master/ArtLine(AR).ipynb) | 24.12.2020 |
| WikiArt (stylegan2-ada) | Generation of paintings of different styles and genres | [Doron Adler](https://linktr.ee/Norod78) | [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2006.06676) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/Norod/my-colab-experiments/blob/master/WikiArt_ADA_Example_Generation.ipynb) | 08.12.2020 |
| GANSpace | A simple technique to analyze GANs and create interpretable controls for image synthesis, such as change of viewpoint, aging, lighting, and time of day | [Erik Härkönen](https://github.com/harskish) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2004.02546)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/harskish/ganspace), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/justinpinkney/awesome-pretrained-stylegan)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/jdTICDa_eAI)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/harskish/ganspace/blob/master/notebooks/Ganspace_colab.ipynb) | 06.12.2020 |
| Stylized Neural Painting | An image-to-painting translation method that generates vivid and realistic painting artworks with controllable styles | <ul><li>[Zhengxia Zou](http://www-personal.umich.edu/~zzhengxi/)</li> <li>[Tianyang Shi](https://www.shitianyang.tech/)</li> <li>[Yi Yuan](https://yiyuan1991.github.io/)</li> <li>[Zhenwei Shi](http://levir.buaa.edu.cn/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2011.08114)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/jiupinjia/stylized-neural-painting)</li><li>[project](https://jiupinjia.github.io/neuralpainter/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=oerb-nwrXhk)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1ch_41GtcQNQT1NLOA21vQJ_rQOjjv9D8) | 01.12.2020 |
| DeOldify (video) | Colorize your own videos! | [Jason Antic](https://github.com/jantic) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1805.08318), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1706.08500)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://medium.com/element-ai-research-lab/stabilizing-neural-style-transfer-for-video-62675e203e42)</li><li>[model](https://data.deepai.org/deoldify/ColorizeVideo_gen.pth)</li><li>[<img src="images/reddit.svg" alt="reddit" height=20/>](https://www.reddit.com/r/Nickelodeons/), [<img src="images/reddit.svg" alt="reddit" height=20/>](https://www.reddit.com/r/silentmoviegifs/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](http://www.youtube.com/watch?v=l3UXXid04Ys), [<img src="images/youtube.svg" alt="youtube" height=20/>](http://www.youtube.com/watch?v=EXn-n2iqEjI)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/jantic/DeOldify/blob/master/VideoColorizerColab.ipynb) | 13.11.2020 |
| DeOldify (photo) | Colorize your own photos! | <ul><li>[Jason Antic](https://github.com/jantic)</li> <li>[Matt Robinson](https://github.com/mc-robinson)</li> <li>[María Benavente](https://github.com/mariabg)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1805.08318), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1706.08500)</li><li>[model](https://data.deepai.org/deoldify/ColorizeArtistic_gen.pth)</li><li>[<img src="images/reddit.svg" alt="reddit" height=20/>](https://www.reddit.com/r/TheWayWeWere/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColab.ipynb) | 13.11.2020 |
| MakeItTalk | A method that generates expressive talking-head videos from a single facial image with audio as the only input | <ul><li>[Yang Zhou](https://people.umass.edu/~yangzhou/)</li> <li>[Xintong Han](http://users.umiacs.umd.edu/~xintong/)</li> <li>[Eli Shechtman](https://research.adobe.com/person/eli-shechtman/)</li> <li>[Jose Echevarria](http://www.jiechevarria.com/)</li> <li>[Evangelos Kalogerakis](https://people.cs.umass.edu/~kalo/)</li> <li>[Dingzeyu Li](https://dingzeyu.li/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2004.12992)</li><li>[data](https://drive.google.com/drive/folders/1EwuAy3j1b9Zc1MsidUfxG_pJGc_cV60O)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/yzhou359/MakeItTalk)</li><li>[project](https://people.umass.edu/~yangzhou/MakeItTalk/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=vUMGKASgbf8)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/yzhou359/MakeItTalk/blob/master/quick_demo.ipynb) | 10.11.2020 |
| LaSAFT | Latent Source Attentive Frequency Transformation for Conditioned Source Separation | [Woosung Choi](https://ws-choi.github.io/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2010.11631)</li><li>[data](https://sigsep.github.io/datasets/musdb.html)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/ws-choi/Conditioned-Source-Separation-LaSAFT)</li><li>[project](https://lasaft.github.io/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/ws-choi/Conditioned-Source-Separation-LaSAFT/blob/master/colab_demo/LaSAFT_with_GPoCM_Stella_Jang_Example.ipynb) | 01.11.2020 |
| Instance-aware Image Colorization | Novel deep learning framework to achieve instance-aware colorization | [Jheng-Wei Su](https://github.com/ericsujw) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2005.10825)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/ericsujw/InstColorization)</li><li>[project](https://ericsujw.github.io/InstColorization/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=Zj1N4uE1ehk)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/ericsujw/InstColorization/blob/master/InstColorization.ipynb) | 30.08.2020 |
| BERT score | An automatic evaluation metric for text generation | [Tianyi Zhang](https://tiiiger.github.io/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1904.09675)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/Tiiiger/bert_score)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1kpL8Y_AnUUiCxFjhxSrxCsc6-sDMNb_Q) | 17.07.2020 |
| HiDT | A generative image-to-image model and a new upsampling scheme that allows to apply image translation at high resolution | <ul><li>[Denis Korzhenkov](https://github.com/denkorzh)</li> <li>[Gleb Sterkin](https://github.com/belkakari)</li> <li>[Sergey Nikolenko](https://logic.pdmi.ras.ru/~sergey/)</li> <li>[Victor Lempitsky](http://sites.skoltech.ru/compvision/members/vilem/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2003.08791)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/saic-mdal/HiDT)</li><li>[project](https://saic-mdal.github.io/HiDT/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/playlist?list=PLuvGzlEQXT1KQuKrfBBEWh2f3PToxyeM5), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=EWKAgwgqXB4)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/saic-mdal/hidt/blob/master/notebooks/HighResolutionDaytimeTranslation.ipynb) | 17.07.2020 |
| Analyzing Tennis Serve | We'll use the Video Intelligence API to analyze a tennis serve, including the angle of the arms and legs during the serve | [Dale Markowitz](https://daleonai.com/) | <ul><li>[blog post](https://daleonai.com/machine-learning-for-sports)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/google/making_with_ml/tree/master/sports_ai)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://manivannan-ai.medium.com/find-the-angle-between-three-points-from-2d-using-python-348c513e2cd)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=yLrOy2Xedgk)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/google/making_with_ml/blob/master/sports_ai/Sports_AI_Analysis.ipynb) | 14.07.2020 |
| SIREN | Implicit Neural Representations with Periodic Activation Functions | <ul><li>[Vincent Sitzmann](https://vsitzmann.github.io/)</li> <li>[Julien Martel](http://web.stanford.edu/~jnmartel/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2006.09661)</li><li>[data](https://drive.google.com/drive/folders/1_iq__37-hw7FJOEUK1tX7mdp8SKB368K)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/vsitzmann/siren)</li><li>[project](https://vsitzmann.github.io/siren/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=Q2fLWGBeaiI)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/vsitzmann/siren/blob/master/explore_siren.ipynb) | 24.06.2020 |
| PIFu | Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization | <ul><li>[Ryota Natsume](https://github.com/nanopoteto)</li> <li>[Shunsuke Saito](https://github.com/shunsukesaito)</li> <li>[Zeng Huang](https://zeng.science/)</li> <li>[Angjoo Kanazawa](https://people.eecs.berkeley.edu/~kanazawa/)</li> <li>[Hao Li](http://hao.li)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1905.05172)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/shunsukesaito/PIFu)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=S1FpjwKqtPs)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1GFSsqP2BWz4gtq0e-nki00ZHSirXwFyY) | 18.06.2020 |
| 3D Ken Burns | A reference implementation of 3D Ken Burns Effect from a Single Image using PyTorch - given a single input image, it animates this still image with a virtual camera scan and zoom subject to motion parallax | [Manuel Romero](https://mrm8488.github.io/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1909.05483)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/sniklaus/3d-ken-burns)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=WrajxHHfRBA)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/mrm8488/shared_colab_notebooks/blob/master/3D_Ken_Burns.ipynb) | 13.06.2020 |
| MusicVAE | A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music | <ul><li>[Adam Roberts](https://github.com/adarob)</li> <li>[Jesse Engel](https://github.com/jesseengel)</li> <li>[Colin Raffel](https://colinraffel.com/)</li> <li>[Curtis Hawthorne](https://github.com/cghawthorne)</li> <li>[Douglas Eck](https://github.com/douglaseck)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1803.05428)</li><li>[blog post](https://g.co/magenta/music-vae)</li><li>[project](https://magenta.tensorflow.org/music-vae)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/playlist?list=PLBUMAYA6kvGU8Cgqh709o5SUvo-zHGTxr)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/MusicVAE.ipynb) | 02.06.2020 |
| Background Matting | The notebook is split into three parts: required setup, running the algorithm on photos, and running it on videos | [Andrey Ryabtsev](https://github.com/andreyryabtsev) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2004.00626)</li><li>[blog post](https://towardsdatascience.com/background-matting-the-world-is-your-green-screen-83a3c4f0f635)</li><li>[data](https://drive.google.com/open?id=1j3BMrRFhFpfzJAe6P2WDtfanoeSCLPiq)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/senguptaumd/Background-Matting)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/gist/andreyryabtsev/243aa3eefa6e06891dda7b1583d1d08f/backmatting.ipynb) | 18.05.2020 |
| Jukebox | A neural net that generates music, including rudimentary singing, as raw audio in a variety of genres and artist styles | [Christine Payne](http://christinemcleavey.com/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2005.00341)</li><li>[blog post](https://openai.com/blog/jukebox)</li><li>[explorer](http://jukebox.openai.com/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/openai/jukebox)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/openai/jukebox/blob/master/jukebox/Interacting_with_Jukebox.ipynb) | 04.05.2020 |
| 3D Photo Inpainting | Method for converting a single RGB-D input image into a 3D photo, i.e., a multi-layer representation for novel view synthesis that contains hallucinated color and depth structures in regions occluded in the original view | <ul><li>[Meng-Li Shih](https://shihmengli.github.io/)</li> <li>[Shih-Yang Su](https://lemonatsu.github.io/)</li> <li>[Johannes Kopf](https://johanneskopf.de/)</li> <li>[Jia-Bin Huang](https://filebox.ece.vt.edu/~jbhuang/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2004.04727)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/vt-vl-lab/3d-photo-inpainting)</li><li>[project](https://shihmengli.github.io/3D-Photo-Inpainting/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1706ToQrkIZshRSJSHvZ1RuCiM__YX3Bz) | 04.05.2020 |
| textgenrnn | Generate text using a pretrained neural network with a few lines of code, or easily train your own text-generating neural network of any size and complexity | [Max Woolf](https://minimaxir.com/) | <ul><li>[blog post](http://minimaxir.com/2018/05/text-neural-networks/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/minimaxir/textgenrnn)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=RW7mP6BfZuY)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1mMKGnVxirJnqDViH7BDJxFqWrsXlPSoK) | 28.04.2020 |
| CartoonGAN | The implementation of the cartoon GAN model with PyTorch | [Tobias Sunderdiek](https://github.com/TobiasSunderdiek) | <ul><li>[cvpr](http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper.pdf)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/alamson/safebooru)</li><li>[project](https://tobiassunderdiek.github.io/cartoon-gan/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/TobiasSunderdiek/cartoon-gan/blob/master/CartoonGAN.ipynb) | 13.04.2020 |
| Motion Supervised co-part Segmentation | A self-supervised deep learning method for co-part segmentation | <ul><li>[Aliaksandr Siarohin](https://aliaksandrsiarohin.github.io/aliaksandr-siarohin-website/)</li> <li>[Subhankar Roy](https://github.com/roysubhankar)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](http://arxiv.org/abs/2004.03234)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/AliaksandrSiarohin/motion-cosegmentation), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/AliaksandrSiarohin/video-preprocessing)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=RJ4Nj1wV5iA)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/AliaksandrSiarohin/motion-cosegmentation/blob/master/part_swap.ipynb) | 07.04.2020 |
| RNN for Predictive Maintenance | LSTM network in order to predict remaining useful life of aircraft engines | [Umberto Griffo](https://github.com/umbertogriffo) | <ul><li>[data](https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/#turbofan)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/umbertogriffo/Predictive-Maintenance-using-LSTM), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/Azure/lstms_for_predictive_maintenance/blob/master/Deep%20Learning%20Basics%20for%20Predictive%20Maintenance.ipynb)</li><li>[link](https://gallery.azure.ai/Experiment/Predictive-Maintenance-Step-2A-of-3-train-and-evaluate-regression-models-2)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1tjIOud2Cc6smmvZsbl-QDBA6TLA2iEtd) | 03.04.2020 |
| Onsets and Frames | Onsets and Frames is an automatic music transcription framework with piano and drums models | <ul><li>[Curtis Hawthorne](https://github.com/cghawthorne)</li> <li>[Erich Elsen](https://github.com/ekelsen)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1710.11153), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1810.12247), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2004.00188)</li><li>[blog post](http://g.co/magenta/onsets-frames)</li><li>[data1](https://g.co/magenta/maestro-wave2midi2wave)</li><li>[data2](https://magenta.tensorflow.org/datasets/e-gmd)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://goo.gl/magenta/onsets-frames-code)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/notebooks/magenta/onsets_frames_transcription/onsets_frames_transcription.ipynb) | 02.04.2020 |
| Classification of chest vs. adominal X-rays | The goal of this tutorial is to build a deep learning classifier to accurately differentiate between chest and abdominal X-rays | [tmoneyx01](https://github.com/tmoneyx01) | <ul><li>[annotator](https://public.md.ai/annotator/project/PVq9raBJ)</li><li>[docs](https://docs.md.ai/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/mdai/mdai-client-py)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/mdai/ml-lessons/blob/master/lesson1-xray-images-classification.ipynb) | 07.03.2020 |
| Lung X-Rays Semantic Segmentation | This lesson applies a U-Net for Semantic Segmentation of the lung fields on chest x-rays | [tmoneyx01](https://github.com/tmoneyx01) | <ul><li>[annotator](https://public.md.ai/annotator/project/aGq4k6NW)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1505.04597)</li><li>[data](https://ceb.nlm.nih.gov/repositories/tuberculosis-chest-x-ray-image-data-sets/)</li><li>[docs](https://docs.md.ai/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/mdai/mdai-client-py)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/mdai/ml-lessons/blob/master/lesson2-lung-xrays-segmentation.ipynb) | 07.03.2020 |
| WikiArt (stylegan2) | Generation of paintings of different styles and genres | [Doron Adler](https://linktr.ee/Norod78) | [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1912.04958) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/Norod/my-colab-experiments/blob/master/WikiArt_Example_Generation_By_Peter_Baylies.ipynb) | 27.01.2020 |
| Earth Engine Python API and Folium Interactive Mapping | This notebook demonstrates how to setup the Earth Engine and provides several examples for visualizing Earth Engine processed data interactively using the folium library | [Qiusheng Wu](https://wetlands.io/) | <ul><li>[api](https://developers.google.com/earth-engine/python_install)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/python-visualization/folium)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/giswqs/qgis-earthengine-examples/blob/master/Folium/ee-api-folium-setup.ipynb) | 20.01.2020 |
| Train a GPT-2 Model on Tweets | Train the model on your downloaded tweets, and generate massive amounts of Tweets from it | [Max Woolf](https://minimaxir.com/) | <ul><li>[GPT-2](https://openai.com/blog/better-language-models/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/minimaxir/download-tweets-ai-text-gen)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1qxcQ2A1nNjFudAGN_mcMOnvV9sF_PkEb) | 16.01.2020 |
| Traffic counting | Making Road Traffic Counting App based on Computer Vision and OpenCV | [Andrey Nikishaev](https://github.com/creotiv) | <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/creotiv/object_detection_projects/tree/master/opencv_traffic_counting)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://medium.com/machine-learning-world/tutorial-making-road-traffic-counting-app-based-on-computer-vision-and-opencv-166937911660)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=_o5iLbRHKao)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/12N4m_RYKqrpozRzh9qe7nQE_sIqQH9U8) | 10.01.2020 |
| Siamese NN | Implementation of Siamese Neural Networks built upon multihead attention mechanism for text semantic similarity task | [Tomasz Latkowski](https://github.com/tlatkowski) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1910.14599)</li><li>[data](https://nlp.stanford.edu/projects/snli/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/tlatkowski/multihead-siamese-nets), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/anli/)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/c/quora-question-pairs)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tlatkowski/multihead-siamese-nets/blob/master/colab/multihead_siamese_nets.ipynb) | 19.12.2019 |
| Learning to Paint | Learning to Paint With Model-based Deep Reinforcement Learning | [Manuel Romero](https://mrm8488.github.io/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1903.04411)</li><li>[<img src="images/reddit.svg" alt="reddit" height=20/>](https://www.reddit.com/r/reinforcementlearning/comments/b5lpfl/learning_to_paint_with_modelbased_deep/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=YmOgKZ5oipk)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/mrm8488/shared_colab_notebooks/blob/master/custom_learningtopaint.ipynb) | 17.12.2019 |
| StyleGAN 2 | Generation of faces, cars, etc. | [Mikael Christensen](https://github.com/Syntopia) | [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1912.04958) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1ShgW6wohEFQtqs_znMna3dzrcVoABKIH) | 15.12.2019 |
| Imaging-AMARETTO | An imaging genomics software tool to systematically interrogate multi-omics networks for relevance to radiography and histopathology imaging biomarkers of clinical outcomes with application to studies of brain tumors | <ul><li>[Nathalie Pochet](http://portals.broadinstitute.org/pochetlab/)</li> <li>[Olivier Gevaert](https://profiles.stanford.edu/olivier-gevaert)</li> <li>[Mohsen Nabian](https://github.com/monabiyan)</li> <li>[Celine Everaert](http://www.crig.ugent.be/en/node/510)</li> <li>[Jayendra Shinde](https://jayendrashinde91.github.io/)</li> <li>[Artur Manukyan](https://artur-man.github.io/)</li> <li>[Thorin Tabor](http://thorin.tabcreations.com/)</li> <li>[Mikel Hernaez](http://mikelhernaez.github.io/)</li> <li>[Jill Mesirov](https://en.wikipedia.org/wiki/Jill_P._Mesirov)</li></ul> | <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/broadinstitute/ImagingAMARETTO)</li><li>[project](http://portals.broadinstitute.org/pochetlab/amaretto.html)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/14u1KZJ3Gf-9qjDycyBKzBiN5VzzOa2xU) | 29.11.2019 |
| Rethinking Style Transfer: From Pixels to Parameterized Brushstrokes | A method to stylize images by optimizing parameterized brushstrokes instead of pixels | <ul><li>[Dmytro Kotovenko](https://scholar.google.de/citations?user=T_U8yxwAAAAJ)</li> <li>[Matthias Wright](https://matthias-wright.github.io/)</li> <li>[Arthur Heimbrecht](https://github.com/arwehei)</li> <li>[Björn Ommer](https://hci.iwr.uni-heidelberg.de/people/bommer)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2103.17185)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/CompVis/brushstroke-parameterized-style-transfer)</li><li>[project](https://compvis.github.io/brushstroke-parameterized-style-transfer/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/clovaai/deep-text-recognition-benchmark/blob/master/demo.ipynb) | 27.10.2019 |
| Face toolbox | A collection of deep learning frameworks ported to Keras for face detection, face segmentation, face parsing, iris detection, and face verification | [shaoanlu](https://shaoanlu.github.io/) | [<img src="images/git.svg" alt="git" height=20/>](https://github.com/shaoanlu/face_toolbox_keras) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/shaoanlu/face-toolbox-keras/blob/master/demo.ipynb) | 03.10.2019 |
| Generating Piano Music with Transformer | This Colab notebook lets you play with pretrained Transformer models for piano music generation, based on the Music Transformer | <ul><li>[Ian Simon](https://github.com/iansimon)</li> <li>[Anna Huang](https://github.com/czhuang)</li> <li>[Jesse Engel](https://github.com/jesseengel)</li> <li>[Curtis Hawthorne](https://github.com/cghawthorne)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1706.03762), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1809.04281)</li><li>[blog post](http://g.co/magenta/music-transformer)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/notebooks/magenta/piano_transformer/piano_transformer.ipynb) | 16.09.2019 |
| Few-shot face translation | A GAN based approach for one model to swap them all: model is capable of producing faces that has its gaze direction, glasses, and hiar occlusions being consistent with given source face | [shaoanlu](https://shaoanlu.github.io/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1903.07291), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1905.01723)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/shaoanlu/fewshot-face-translation-GAN)</li><li>[link](https://drum.lib.umd.edu/handle/1903/21337)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/shaoanlu/fewshot-face-translation-GAN/blob/master/colab_demo.ipynb) | 02.09.2019 |
| Waifu2x | This is the Google Colab implementation of tsurumeso's chainer implementation of waifu2x | [Margesh Phirke](https://github.com/mphirke) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](http://arxiv.org/abs/1501.00092)</li><li>[demo](http://waifu2x.udp.jp/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/mphirke/Google-Colab-waifu2x-chainer)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/mphirke/waifu2x-chainer/blob/master/Waifu2x_Colab_Implementation.ipynb) | 23.08.2019 |
| GMCNN | Generative Multi-column Convolutional Neural Networks inpainting model in Keras | [Tomasz Latkowski](https://github.com/tlatkowski) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1810.08771)</li><li>[data1](http://places2.csail.mit.edu/download.html)</li><li>[data2](https://nv-adlr.github.io/publication/partialconv-inpainting)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/tlatkowski/inpainting-gmcnn-keras), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/keras-team/keras-contrib/blob/master/examples/improved_wgan.py)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tlatkowski/inpainting-gmcnn-keras/blob/master/colab/Image_Inpainting_with_GMCNN_model.ipynb) | 09.08.2019 |
| AMARETTO | Multiscale and multimodal inference of regulatory networks to identify cell circuits and their drivers shared and distinct within and across biological systems of human disease | <ul><li>[Nathalie Pochet](http://portals.broadinstitute.org/pochetlab/)</li> <li>[Olivier Gevaert](https://profiles.stanford.edu/olivier-gevaert)</li> <li>[Mohsen Nabian](https://github.com/monabiyan)</li> <li>[Jayendra Shinde](https://jayendrashinde91.github.io/)</li> <li>[Celine Everaert](http://www.crig.ugent.be/en/node/510)</li> <li>[Thorin Tabor](http://thorin.tabcreations.com/)</li></ul> | <ul><li>[bioconductor](https://bioconductor.org/packages/release/bioc/html/AMARETTO.html)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/gevaertlab/AMARETTO)</li><li>[project](http://portals.broadinstitute.org/pochetlab/amaretto.html)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1JfnRoNgTVX_7VEGAAmjGjwP_yX2tdDxs) | 25.07.2019 |
| XLNet | Generalized Autoregressive Pretraining for Language Understanding | [Zhilin Yang](http://kimiyoung.github.io/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1906.08237)</li><li>[data1](https://ai.stanford.edu/~amaas/data/sentiment/)</li><li>[data2](https://rajpurkar.github.io/SQuAD-explorer/)</li><li>[data3](https://www.cs.cmu.edu/~glai1/data/race/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/zihangdai/xlnet)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/zihangdai/xlnet/blob/master/notebooks/colab_imdb_gpu.ipynb) | 28.06.2019 |
| Breast Cancer detection | A Neural Network for detecting breast cancer in cell scans! | [Peter Teoh](https://github.com/tthtlc) | [blog post](http://www.laurencemoroney.com/easily-build-a-neural-net-for-breast-cancer-detection/) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/12DKmhi5z5Qx84iJQ8FTq5hHsG5UUHUcG) | 28.04.2019 |
| GazeML | Eye region landmarks detection | [shaoanlu](https://shaoanlu.github.io/) | <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/shaoanlu/GazeML-keras)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=cLUHKYfZN5s)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/shaoanlu/GazeML-keras/blob/master/demo_colab.ipynb) | 03.04.2019 |
| BERT with TPU | Using a free Colab Cloud TPU to fine-tune sentence and sentence-pair classification tasks built on top of pretrained BERT models and run predictions on tuned model | [Sourabh Bajaj](https://sourabhbajaj.com/) | <ul><li>[TPU quickstart](https://cloud.google.com/tpu/docs/quickstart)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1810.04805)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/hub)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb) | 29.03.2019 |
| automl-gs on a TPU | Give an input CSV file and a target field you want to predict to automl-gs, and get a trained high-performing machine learning or deep learning model plus native Python code pipelines allowing you to integrate that model into any prediction workflow | [Max Woolf](https://minimaxir.com/) | [<img src="images/git.svg" alt="git" height=20/>](https://github.com/minimaxir/automl-gs) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1sbF8cqnOsdzN9Bdt74eER5s_xXcdvatV) | 26.03.2019 |
| GANSynth | This notebook is a demo GANSynth, which generates audio with Generative Adversarial Networks | [Jesse Engel](https://github.com/jesseengel) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1809.11096)</li><li>[<img src="images/git.svg" alt="git" height=20/>](http://goo.gl/magenta/gansynth-code)</li><li>[project](https://storage.googleapis.com/magentadata/papers/gansynth/index.html)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/notebooks/magenta/gansynth/gansynth_demo.ipynb) | 25.02.2019 |
| Edge Detection | Edge detection in OpenCV and skimage | [Yuhuang Hu](https://dgyblog.com/) | [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Edge_detection) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/10ZIvyVgDjGlWd09LJZzwboQs-4RPlCut) | 15.02.2019 |
| BERT on TF Hub | Predicting Movie Review Sentiment with BERT on TF Hub | [Dale Markowitz](https://daleonai.com/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1810.04805)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/google-research/bert)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb) | 12.02.2019 |
| Mask R-CNN | Code and visualizations to test, debug, and evaluate the Mask R-CNN model | <ul><li>[Jirka Borovec](https://github.com/Borda)</li> <li>[Waleed Abdulla](https://waleedka.github.io/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1703.06870)</li><li>[blog post](https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46)</li><li>[data](http://cocodataset.org/#home)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/matterport/Mask_RCNN), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/jremillard/images-to-osm), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/huuuuusy/Mask-RCNN-Shiny)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/c/data-science-bowl-2018)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://medium.com/geoai/reconstructing-3d-buildings-from-aerial-lidar-with-ai-details-6a81cb3079c0)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=OOT3UIXZztE)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/matterport/Mask_RCNN/blob/master/samples/coco/inspect_model.ipynb) | 22.01.2019 |
| Faceswap-GAN | A minimum demo for faceswap-GAN v2.2 | [shaoanlu](https://shaoanlu.github.io/) | [<img src="images/git.svg" alt="git" height=20/>](https://github.com/shaoanlu/faceswap-GAN) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/shaoanlu/faceswap-GAN/blob/master/colab_demo/faceswap-GAN_colab_demo.ipynb) | 08.11.2018 |
| RSNA Pneumonia Detection Challenge (Kaggel API) | The basics of parsing the competition dataset, training using a detector basd on the Mask-RCNN algorithm for object detection and instance segmentation | [tmoneyx01](https://github.com/tmoneyx01) | <ul><li>[annotator](https://public.md.ai/annotator/project/LxR6zdR2/workspace)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1703.06870)</li><li>[docs](https://docs.md.ai/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/mdai/mdai-client-py)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/c/rsna-pneumonia-detection-challenge)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/mdai/ml-lessons/blob/master/lesson3-rsna-pneumonia-detection-kaggle.ipynb) | 03.09.2018 |
| HoF | This notebook will walk you step by step through the process of using a pre-trained model to detect faces in an image | [Lucas Persona](http://www.lucaspersona.com/) | <ul><li>[data](http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/the-house-of-black-and-white/hall-of-faces)</li><li>[yolo](https://pjreddie.com/darknet/yolo/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1lJWquGmKoMm68qNuwjSnfMjjIi-UTzI1) | 23.04.2018 |
| Group Normalization | A simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. | [shaoanlu](https://shaoanlu.github.io/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1803.08494)</li><li>[blog post](https://shaoanlu.wordpress.com/2018/03/26/experiment-with-group-normalization/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/amrzv/GroupNormalization-keras), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/zalandoresearch/fashion-mnist)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/shaoanlu/GroupNormalization-keras/blob/master/group_norm_experiments.ipynb) | 26.03.2018 |
| Latent Constraints | Conditional Generation from Unconditional Generative Models | <ul><li>[Jesse Engel](https://github.com/jesseengel)</li> <li>[Matthew Hoffman](http://matthewdhoffman.com/)</li> <li>[Adam Roberts](https://github.com/adarob)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1711.05772)</li><li>[data](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/notebooks/latent_constraints/latentconstraints.ipynb) | 27.11.2017 |
| Performance RNN | This notebook shows you how to generate new performed compositions from a trained model | <ul><li>[Ian Simon](https://github.com/iansimon)</li> <li>[Sageev Oore](https://github.com/osageev)</li> <li>[Curtis Hawthorne](https://github.com/cghawthorne)</li></ul> | <ul><li>[blog post](https://magenta.tensorflow.org/performance-rnn)</li><li>[data](http://www.piano-e-competition.com/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/magenta/magenta/tree/master/magenta/models/performance_rnn)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/notebooks/magenta/performance_rnn/performance_rnn.ipynb) | 11.07.2017 |
| NSynth | This colab notebook has everything you need to upload your own sounds and use NSynth models to reconstruct and interpolate between them | <ul><li>[Jesse Engel](https://github.com/jesseengel)</li> <li>[Cinjon Resnick](https://github.com/cinjon)</li> <li>[Adam Roberts](https://github.com/adarob)</li> <li>[Sander Dieleman](https://benanne.github.io/)</li> <li>[Karen Simonyan](https://scholar.google.co.uk/citations?user=L7lMQkQAAAAJ)</li> <li>[Mohammad Norouzi](https://norouzi.github.io/)</li> <li>[Douglas Eck](https://github.com/douglaseck)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1704.01279)</li><li>[blog post](https://magenta.tensorflow.org/nsynth)</li><li>[data](https://magenta.tensorflow.org/datasets/nsynth)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/tensorflow/magenta/tree/master/magenta/models/nsynth)</li><li>[tutorial](https://magenta.tensorflow.org/nsynth-fastgen)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=AaALLWQmCdI), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=BOoSy-Pg8is)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/notebooks/magenta/nsynth/nsynth.ipynb) | 06.04.2017 |
## Tutorials
| name | description | authors | links | colaboratory | update |
|------|-------------|:--------|:------|:------------:|:------:|
| Detectron2 | FAIR's next-generation platform for object detection and segmentation | [Yuxin Wu](http://ppwwyyxx.com/) | <ul><li>[blog post](https://ai.facebook.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-/)</li><li>[docs](https://detectron2.readthedocs.io/en/latest/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/facebookresearch/detectron2), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/matterport/Mask_RCNN/tree/master/samples/balloon)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5) | 24.06.2021 |
| Pix2Pix | This notebook demonstrates image to image translation using conditional GAN's | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1611.07004)</li><li>[data](https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/pix2pix.ipynb) | 18.06.2021 |
| The Autodiff Cookbook | You'll go through a whole bunch of neat autodiff ideas that you can cherry pick for your own work, starting with the basics | <ul><li>[Alex Wiltschko](https://github.com/alexbw)</li> <li>[Matthew Johnson](http://people.csail.mit.edu/mattjj/)</li></ul> | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1406.2572), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1706.04454), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1802.03451), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1811.07062)</li><li>[book1](https://mitpress.mit.edu/sites/default/files/titles/content/sicm_edition_2/book.html)</li><li>[book2](https://mitpress.mit.edu/books/functional-differential-geometry)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/google/jax/issues/446#issuecomment-467105048), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/google/jax#auto-vectorization-with-vmap), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/hips/autograd)</li><li>[tutorial](http://videolectures.net/deeplearning2017_johnson_automatic_differentiation/)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Truncated_Newton_method), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Pullback_(differential_geometry)), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Holomorphic_function), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Riemann_equations)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/google/jax/blob/master/docs/notebooks/autodiff_cookbook.ipynb) | 18.06.2021 |
| Neural style transfer | This tutorial uses deep learning to compose one image in the style of another image | [Billy Lamberta](https://github.com/lamberta) | [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1508.06576) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/style_transfer.ipynb) | 15.06.2021 |
| Transformer | This tutorial trains a Transformer model to translate Portuguese to English | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1706.03762), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1903.03878)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/neulab/word-embeddings-for-nmt)</li><li>[link](https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb) | 15.06.2021 |
| GPT-J-6B | A 6 billion parameter, autoregressive text generation model trained on The Pile | <ul><li>[Ben Wang](https://benwang.dev/)</li> <li>[Aran Komatsuzaki](https://arankomatsuzaki.wordpress.com/about-me/)</li> <li>[Janko Prester](https://www.jankoprester.com/)</li></ul> | <ul><li>[The Pile](https://pile.eleuther.ai/)</li><li>[blog post](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/kingoflolz/mesh-transformer-jax), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/EleutherAI/gpt-neox), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/microsoft/DeepSpeed)</li><li>[web demo](https://6b.eleuther.ai/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb) | 14.06.2021 |
| NMT with attention | This notebook trains a seq2seq model for Spanish to English translation | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1409.0473), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1508.04025v5)</li><li>[data](http://www.manythings.org/anki/)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Neural_machine_translation)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/nmt_with_attention.ipynb) | 10.06.2021 |
| Lucid Sonic Dreams | Syncs GAN-generated visuals to music | [Mikael Alafriz](https://github.com/mikaelalafriz) | <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/mikaelalafriz/lucid-sonic-dreams), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/NVlabs/stylegan2), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/justinpinkney/awesome-pretrained-stylegan2)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://towardsdatascience.com/introducing-lucid-sonic-dreams-sync-gan-art-to-music-with-a-few-lines-of-python-code-b04f88722de1)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/l-nGC-ve7sI)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1Y5i50xSFIuN3V4Md8TB30_GOAtts7RQD) | 08.06.2021 |
| Autoencoders | This tutorial introduces autoencoders with three examples: the basics, image denoising, and anomaly detection | [Google](https://www.tensorflow.org/) | <ul><li>[blog post](https://blog.keras.io/building-autoencoders-in-keras.html)</li><li>[book](https://www.deeplearningbook.org/contents/autoencoders.html)</li><li>[data](http://www.timeseriesclassification.com/description.php?Dataset=ECG5000)</li><li>[examples](https://anomagram.fastforwardlabs.com/#/)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/method/autoencoder)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/autoencoder.ipynb) | 08.06.2021 |
| Introduction to the TensorFlow Models NLP library | You will learn how to build transformer-based models for common NLP tasks including pretraining, span labelling and classification using the building blocks from NLP modeling library | [Chen Chen](https://github.com/chenGitHuber) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1810.04805)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/tensorflow/models/tree/master/official/nlp/modeling)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/models/blob/master/official/colab/nlp/nlp_modeling_library_intro.ipynb) | 06.06.2021 |
| CycleGAN | This notebook demonstrates unpaired image to image translation using conditional GAN's | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1703.10593)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/datasets/catalog/cycle_gan)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb) | 02.06.2021 |
| Transfer learning and fine-tuning | You will learn how to classify images of cats and dogs by using transfer learning from a pre-trained network | [François Chollet](https://fchollet.com/) | <ul><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/transfer-learning)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Transfer_learning)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb) | 02.06.2021 |
| Classify text with BERT | This tutorial contains complete code to fine-tune BERT to perform sentiment analysis on a dataset of plain-text IMDB movie reviews | [Google](https://www.tensorflow.org/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1810.04805), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1711.05101)</li><li>[data](https://ai.stanford.edu/~amaas/data/sentiment/)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/text-classification)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://tfhub.dev/google/collections/bert/1)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb) | 27.05.2021 |
| GLUE using BERT on TPU | This tutorial contains complete end-to-end code to train models on a TPU | [Google](https://www.tensorflow.org/) | <ul><li>[GLUE](https://gluebenchmark.com/)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1810.04805)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/guide/tpu)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/bert_glue.ipynb) | 25.05.2021 |
| Image captioning | Given an image our goal is to generate a caption | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1502.03044)</li><li>[data](https://cocodataset.org/#home)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/image_captioning.ipynb) | 25.05.2021 |
| Text classification with RNN | This text classification tutorial trains a recurrent neural network on the IMDB large movie review dataset for sentiment analysis | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[data](http://ai.stanford.edu/~amaas/data/sentiment/)</li><li>[link](https://developers.google.com/machine-learning/glossary/#recurrent_neural_network)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/text-classification)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/text_classification_rnn.ipynb) | 22.05.2021 |
| Word embeddings | This tutorial contains an introduction to word embeddings | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[data](http://ai.stanford.edu/~amaas/data/sentiment/)</li><li>[projector](http://projector.tensorflow.org/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/guide/word_embeddings.ipynb) | 22.05.2021 |
| CNN | This tutorial demonstrates training a simple Convolutional Neural Network to classify CIFAR images | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[cifar](https://www.cs.toronto.edu/~kriz/cifar.html)</li><li>[link](https://developers.google.com/machine-learning/glossary/#convolutional_neural_network)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/cnn.ipynb) | 21.05.2021 |
| Custom GPT-2 + Tokenizer | Train a custom GPT-2 model for free on a GPU using aitextgen! | [Max Woolf](https://minimaxir.com/) | <ul><li>[data](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)</li><li>[docs](https://docs.aitextgen.io/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/minimaxir/aitextgen)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/144MdX5aLqrQ3-YW-po81CQMrD6kpgpYh) | 17.05.2021 |
| Train a GPT-2 Text-Generating Model | Retrain an advanced text generating neural network on any text dataset for free on a GPU using Colaboratory using aitextgen! | [Max Woolf](https://minimaxir.com/) | <ul><li>[data](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)</li><li>[docs](https://docs.aitextgen.io/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/minimaxir/aitextgen)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/text-generation)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/15qBZx5y9rdaQSyWpsreMDnTiZ5IlN0zD) | 17.05.2021 |
| Text generation with RNN | This tutorial demonstrates how to generate text using a character-based RNN | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[link](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/text-generation)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/text_generation.ipynb) | 13.05.2021 |
| Image segmentation | This tutorial focuses on the task of image segmentation, using a modified U-Net | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[data](https://www.robots.ox.ac.uk/~vgg/data/pets/)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/c/carvana-image-masking-challenge/overview)</li><li>[u-net](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/segmentation.ipynb) | 12.05.2021 |
| Data augmentation | This tutorial demonstrates data augmentation: a technique to increase the diversity of your training set by applying random transformations such as image rotation | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/data-augmentation)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/datasets/catalog/tf_flowers)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Data_augmentation)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/data_augmentation.ipynb) | 06.05.2021 |
| LightAutoML | Framework for automatic classification and regression model creation | <ul><li>[Alexander Ryzhkov](https://www.kaggle.com/alexryzhkov)</li> <li>[Anton Vakhrushev](https://www.kaggle.com/btbpanda)</li> <li>[Dmitry Simakov](https://www.kaggle.com/simakov)</li> <li>[Vasiliy Bunakov](https://github.com/VaBun)</li> <li>[Alexander Kirilin](https://github.com/Cybsloth)</li></ul> | <ul><li>[blog post](https://analyticsindiamag.com/hands-on-python-guide-to-lama-an-automatic-ml-model-creation-framework/)</li><li>[docs](https://lightautoml.readthedocs.io/en/latest/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/sberbank-ai-lab/LightAutoML)</li><li>[tds](https://towardsdatascience.com/lightautoml-preset-usage-tutorial-2cce7da6f936)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=ci8uqgWFJGg), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=TYu1UG-E9e8), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=4pbO673B9Oo)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/sberbank-ai-lab/LightAutoML/blob/master/Tutorial_1.%20Create%20your%20own%20pipeline.ipynb) | 27.04.2021 |
| Building Your Own Federated Learning Algorithm | In this tutorial, we discuss how to implement federated learning algorithms without deferring to the tff.learning API | [Zachary Charles](https://zachcharles.com/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1907.08610)</li><li>[blog post](https://ai.googleblog.com/2020/05/federated-analytics-collaborative-data.html)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/federated-learning)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/federated/api_docs/python/tff/learning/Model)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/building_your_own_federated_learning_algorithm.ipynb) | 21.04.2021 |
| TFF for Federated Learning Research: Model and Update Compression | In this tutorial, we use the EMNIST dataset to demonstrate how to enable lossy compression algorithms to reduce communication cost in the Federated Averaging algorithm | [Weikang Song](https://github.com/swkpku) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1602.05629)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/federated-learning)</li><li>[tensor encoding](http://jakubkonecny.com/files/tensor_encoding.pdf)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/emnist), [<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/federated/api_docs/python/tff/learning/build_federated_averaging_process)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/tff_for_federated_learning_research_compression.ipynb) | 21.04.2021 |
| Federated Learning for Image Classification | In this tutorial, we use the classic MNIST training example to introduce the Federated Learning API layer of TFF, tff.learning - a set of higher-level interfaces that can be used to perform common types of federated learning tasks, such as federated training, against user-supplied models implemented in TensorFlow | [Krzysztof Ostrowski](https://github.com/krzys-ostrowski) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1602.05629)</li><li>[data](https://www.nist.gov/srd/nist-special-database-19)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://medium.com/tensorflow/standardizing-on-keras-guidance-on-high-level-apis-in-tensorflow-2-0-bad2b04c819a)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/federated-learning), [<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/image-classification)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_image_classification.ipynb) | 19.04.2021 |
| High-performance simulations with TFF | This tutorial will describe how to setup high-performance simulations with TFF in a variety of common scenarios | [Krzysztof Ostrowski](https://github.com/krzys-ostrowski) | [<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/federated-learning) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/simulations.ipynb) | 19.04.2021 |
| High-performance Simulation with Kubernetes | This tutorial will describe how to set up high-performance simulation using a TFF runtime running on Kubernetes | [Jason Roselander](https://github.com/roselander) | <ul><li>[GKE](https://cloud.google.com/kubernetes-engine/)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/federated-learning)</li><li>[shell](https://cloud.google.com/shell/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/high_performance_simulation_with_kubernetes.ipynb) | 19.04.2021 |
| Word2Vec | Word2Vec is not a singular algorithm, rather, it is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets | [Google](https://www.tensorflow.org/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1301.3781)</li><li>[link](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf)</li><li>[<img src="images/neurips.svg" alt="neurips" height=20/>](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/method/cbow-word2vec), [<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/method/skip-gram-word2vec)</li><li>[projector](http://projector.tensorflow.org/)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Zipf%27s_law)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/word2vec.ipynb) | 13.04.2021 |
| Simple audio recognition | This tutorial will show you how to build a basic speech recognition network that recognizes ten different words | [Google](https://www.tensorflow.org/) | <ul><li>[coursera](https://www.coursera.org/lecture/audio-signal-processing/stft-2-tjEQe)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/speech-recognition)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/datasets/catalog/speech_commands)</li><li>[tf.js](https://codelabs.developers.google.com/codelabs/tensorflowjs-audio-codelab/index.html#0)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/audio/simple_audio.ipynb) | 06.04.2021 |
| Actor-Critic | This tutorial demonstrates how to implement the Actor-Critic method using TensorFlow to train an agent on the Open AI Gym CartPole-V0 environment | [Mark Daoust](https://github.com/MarkDaoust) | <ul><li>[gym](https://gym.openai.com/)</li><li>[<img src="images/neurips.svg" alt="neurips" height=20/>](https://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf), [<img src="images/neurips.svg" alt="neurips" height=20/>](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Temporal_difference_learning)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/reinforcement_learning/actor_critic.ipynb) | 05.04.2021 |
| GPT Neo | An implementation of model & data parallel GPT2 & GPT3 -like models, with the ability to scale up to full GPT3 sizes (and possibly more!), using the mesh-tensorflow library | [EleutherAI](https://www.eleuther.ai/) | <ul><li>[GPT-2](https://openai.com/blog/better-language-models/)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2005.14165), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2004.05150), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1701.06538)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/EleutherAI/gpt-neo), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/tensorflow/mesh), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/EleutherAI/gpt-neox/)</li><li>[pretrained](https://the-eye.eu/public/AI/gptneo-release/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/EleutherAI/GPTNeo/blob/master/GPTNeo_example_notebook.ipynb) | 28.03.2021 |
| CVAE | This notebook demonstrates how train a Variational Autoencoder on the MNIST dataset | [Billy Lamberta](https://github.com/lamberta) | [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1312.6114), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1401.4082) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/cvae.ipynb) | 22.03.2021 |
| DCGAN | This tutorial demonstrates how to generate images of handwritten digits using a Deep Convolutional Generative Adversarial Network | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1511.06434), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1701.00160)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/jessicali9530/celeba-dataset)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/dcgan.ipynb) | 12.03.2021 |
| DeepDream | This tutorial contains a minimal implementation of DeepDream: an experiment that visualizes the patterns learned by a neural network | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/pdf/1409.4842.pdf)</li><li>[blog post](https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Inception)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/deepdream.ipynb) | 12.03.2021 |
| Adversarial FGSM | This tutorial creates an adversarial example using the Fast Gradient Signed Method attack. This was one of the first and most popular attacks to fool a neural network. | [Billy Lamberta](https://github.com/lamberta) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1412.6572)</li><li>[imagenet](http://www.image-net.org/)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/applications/MobileNetV2)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/adversarial_fgsm.ipynb) | 12.03.2021 |
| highway-env | A collection of environments for autonomous driving and tactical decision-making tasks | [Edouard Leurent](https://edouardleurent.com/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2102.03483), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2105.05701)</li><li>[docs](https://highway-env.readthedocs.io/en/latest/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/eleurent/highway-env), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/eleurent/rl-agents), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/eleurent/finite-mdp), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/openai/baselines/tree/master/baselines/her)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/eleurent/highway-env/blob/master/scripts/parking_model_based.ipynb) | 12.03.2021 |
| YOLOv5 on Custom Objects | This notebook shows training on your own custom objects | [Jacob Solawetz](https://blog.roboflow.com/author/jacob/) | <ul><li>[blog post](https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset/)</li><li>[data](https://public.roboflow.ai/object-detection/bccd)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/ultralytics/yolov5)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1gDZ2xcTOgR39tGGs-EZ6i3RTs16wmzZQ) | 10.03.2021 |
| Federated Learning for Text Generation | For this tutorial, we start with a RNN that generates ASCII characters, and refine it via federated learning | [Krzysztof Ostrowski](https://github.com/krzys-ostrowski) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1812.01097), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1602.05629)</li><li>[data1](http://www.ibiblio.org/pub/docs/books/gutenberg/9/98/98.txt)</li><li>[data2](http://www.ibiblio.org/pub/docs/books/gutenberg/4/46/46.txt)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/hub)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_text_generation.ipynb) | 25.02.2021 |
| Custom Federated Algorithms, Part 1: Introduction to the Federated Core | This tutorial is the first part of a two-part series that demonstrates how to implement custom types of federated algorithms in TensorFlow Federated using the Federated Core - a set of lower-level interfaces that serve as a foundation upon which we have implemented the Federated Learning layer | [Krzysztof Ostrowski](https://github.com/krzys-ostrowski) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1602.05629)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/federated-learning)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/federated/federated_core), [<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/federated/federated_learning)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/custom_federated_algorithms_1.ipynb) | 25.02.2021 |
| Custom Federated Algorithms, Part 2: Implementing Federated Averaging | This tutorial is the second part of a two-part series that demonstrates how to implement custom types of federated algorithms in TFF using the Federated Core, which serves as a foundation for the Federated Learning layer | [Krzysztof Ostrowski](https://github.com/krzys-ostrowski) | <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/tensorflow/federated/blob/master/tensorflow_federated/python/learning/federated_averaging.py)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/federated-learning)</li><li>[<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/federated/federated_core), [<img src="images/tf.svg" alt="tf" height=20/>](https://www.tensorflow.org/federated/federated_learning)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/custom_federated_algorithms_2.ipynb) | 25.02.2021 |
| ruGPT3 | Example of inference of RuGPT3XL | [Anton Emelyanov](https://github.com/king-menin) | <ul><li>[cristofari](https://sbercloud.ru/ru/christofari)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/sberbank-ai/ru-gpts), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/microsoft/DeepSpeedExamples/tree/master/Megatron-LM)</li><li>[huggingface](https://huggingface.co/transformers/main_classes/model.html#transformers.generation_utils.GenerationMixin.generate)</li><li>[sparse attention](deepspeed.ai/tutorials/sparse-attention/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/sberbank-ai/ru-gpts/blob/master/examples/ruGPT3XL_generation.ipynb) | 17.02.2021 |
| TF-Ranking | This tutorial is an end-to-end walkthrough of training a TensorFlow Ranking neural network model which incorporates sparse textual features | [Rama Kumar](https://github.com/ramakumar1729) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1910.09676), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1812.00073), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1905.08957), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1811.04415)</li><li>[data](http://hamedz.ir/resources/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/tensorflow/ranking), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/input.proto#L72)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Mean_reciprocal_rank), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/ranking/blob/master/tensorflow_ranking/examples/handling_sparse_features.ipynb) | 04.02.2021 |
| TensorNetwork | A library for easy and efficient manipulation of tensor networks | [Chase Roberts](http://thenerdstation.github.io/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1708.00006), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1306.2164)</li><li>[docs](https://tensornetwork.readthedocs.io/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/google/TensorNetwork)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://www.youtube.com/watch?v=YN2YBB0viKo)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/google/TensorNetwork/blob/master/colabs/Tensor_Networks_in_Neural_Networks.ipynb) | 21.01.2021 |
| Spleeter | Deezer source separation library including pretrained models | <ul><li>[Romain Hennequin](http://romain-hennequin.fr/)</li> <li>[Anis Khlif](https://github.com/alreadytaikeune)</li> <li>[Félix Voituret](https://github.com/Faylixe)</li> <li>[Manuel Moussallam](https://mmoussallam.github.io/)</li></ul> | <ul><li>[blog post](https://deezer.io/releasing-spleeter-deezer-r-d-source-separation-engine-2b88985e797e)</li><li>[data](https://sigsep.github.io/datasets/musdb.html)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/deezer/spleeter)</li><li>[project](https://research.deezer.com/projects/spleeter.html)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/deezer/spleeter/blob/master/spleeter.ipynb) | 10.01.2021 |
| Image classification | This tutorial shows how to classify images of flowers | [Billy Lamberta](https://github.com/lamberta) | [<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/image-classification) | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/classification.ipynb) | 07.01.2021 |
| Integrated gradients | This tutorial demonstrates how to implement Integrated Gradients, an Explainable AI technique | [Google](https://www.tensorflow.org/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1703.01365)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/blogs/integrated_gradients)</li><li>[visualizing](https://distill.pub/2020/attribution-baselines/)</li><li>[<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Explainable_artificial_intelligence), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Linear_interpolation), [<img src="images/wiki.svg" alt="wiki" height=20/>](https://en.wikipedia.org/wiki/Riemann_sum)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/interpretability/integrated_gradients.ipynb) | 17.12.2020 |
| YOLOv4 | This tutorial will help you build YOLOv4 easily in the cloud with GPU enabled so that you can run object detections in milliseconds! | [Aleksey Bochkovskiy](http://www.alexeyab.com/) | <ul><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2004.10934), [<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/2011.08036)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/AlexeyAB/darknet/wiki)</li><li>[<img src="images/medium.svg" alt="medium" height=20/>](https://alexeyab84.medium.com/yolov4-the-most-accurate-real-time-neural-network-on-ms-coco-dataset-73adfd3602fe), [<img src="images/medium.svg" alt="medium" height=20/>](https://alexeyab84.medium.com/scaled-yolo-v4-is-the-best-neural-network-for-object-detection-on-ms-coco-dataset-39dfa22fa982)</li><li>[project]()</li><li>[<img src="images/reddit.svg" alt="reddit" height=20/>](https://www.reddit.com/r/MachineLearning/comments/gydxzd/p_yolov4_the_most_accurate_realtime_neural/)</li><li>[<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/1_SiUOYUoOI), [<img src="images/youtube.svg" alt="youtube" height=20/>](https://youtu.be/YDFf-TqJOFE)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1_GdoqCJWXsChrOiY8sZMr_zbr_fH-0Fg) | 25.06.2020 |
| Transfer Learning in NLP | This notebook accompanies the tutorial given at NAACL 2019 on Transfer Learning in Natural Language Processing | <ul><li>[Sebastian Ruder](https://ruder.io/)</li> <li>[Swabha Swayamdipta](https://swabhs.com/)</li> <li>[Thomas Wolf](https://thomwolf.io/)</li></ul> | <ul><li>[data](https://cogcomp.seas.upenn.edu/Data/QA/QC/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/huggingface/naacl_transfer_learning_tutorial), [<img src="images/git.svg" alt="git" height=20/>](https://github.com/HazyResearch/metal)</li><li>[<img src="images/paperswithcode.svg" alt="paperswithcode" height=20/>](https://paperswithcode.com/task/transfer-learning)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/drive/1iDHCYIrWswIKp-n-pOg69xLoZO09MEgf) | 03.06.2019 |
| RSNA Pneumonia Detection Challenge (MD.ai API) | The basics of parsing the competition dataset, training using a detector basd on the Mask-RCNN algorithm for object detection and instance segmentation | [tmoneyx01](https://github.com/tmoneyx01) | <ul><li>[annotator](https://public.md.ai/annotator/project/LxR6zdR2/workspace)</li><li>[<img src="images/arxiv.svg" alt="arxiv" height=20/>](https://arxiv.org/abs/1703.06870)</li><li>[docs](https://docs.md.ai/)</li><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/mdai/mdai-client-py)</li><li>[<img src="images/kaggle.svg" alt="kaggle" height=20/>](https://www.kaggle.com/c/rsna-pneumonia-detection-challenge)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/mdai/ml-lessons/blob/master/lesson3-rsna-pneumonia-detection-mdai-client-lib.ipynb) | 29.08.2018 |
| Python Data Science Handbook | Jupyter notebook version of the Python Data Science Handbook by Jake VanderPlas | [Jake Vanderplas](http://vanderplas.com/) | <ul><li>[<img src="images/git.svg" alt="git" height=20/>](https://github.com/jakevdp/PythonDataScienceHandbook)</li><li>[project](https://jakevdp.github.io/PythonDataScienceHandbook/)</li></ul> | [![Open In Colab](images/colab.svg)](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb) | 14.08.2017 |

(generated by [generate_markdown.py](generate_markdown.py) based on [research.json](data/research.json) and [tutorials.json](data/tutorials.json))
