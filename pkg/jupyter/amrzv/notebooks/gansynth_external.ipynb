{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "gansynth_external.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hMqWDc_m6rUC"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMqWDc_m6rUC"
      },
      "source": [
        "\n",
        "##### Copyright 2019 Google LLC.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNhgka4UKNjf"
      },
      "source": [
        "# Copyright 2019 Google LLC. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JndnmDMp66FL"
      },
      "source": [
        "# GANSynth Demo\n",
        "\n",
        "This notebook is a demo *GANSynth*, which generates audio with Generative Adversarial Networks. \n",
        "GANSynth learns to produce individual instrument notes like the [NSynth Dataset](https://magenta.tensorflow.org/datasets/nsynth). With pitch provided as a conditional attribute, the generator learns to use its latent space to represent different instrument timbres. This allows us to synthesize performances from MIDI files, either keeping the timbre constant, or interpolating between instruments over time.\n",
        "\n",
        "* [GANSynth ICLR paper](https://arxiv.org/abs/1809.11096)\n",
        "* [Audio Examples](http://goo.gl/magenta/gansynth-examples) \n",
        "\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/magentadata/papers/gansynth/figures/models.jpeg\" alt=\"GANSynth figure\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J2z5Sh6wZrP"
      },
      "source": [
        "## 1: Environment Setup\n",
        "\n",
        "\n",
        "This notebook synthesizes audio from uploaded MIDI files. There are two different flavors:\n",
        "* Interpolate between random instruments\n",
        "* Interpolate between two chosen instruments\n",
        "\n",
        "\n",
        "Have fun! And please feel free to hack this notebook to make your own creative interactions.\n",
        "\n",
        "### Instructions for running:\n",
        "\n",
        "* Make sure to use a GPU runtime, click:  __Runtime >> Change Runtime Type >> GPU__\n",
        "* Then press the **Play** button on the left of each of the cells\n",
        "* Double-click any of the cells to view the code\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "dwQleTRmCoE5"
      },
      "source": [
        "#@title Install\n",
        "\n",
        "#@markdown Install magenta, define some helper functions, and download the model. This transfers a lot of data and _should take a minute or two_.\n",
        "\n",
        "# Install Magenta\n",
        "print('Copying data from GCS...')\n",
        "!rm -r /content/gansynth &>/dev/null\n",
        "!mkdir /content/gansynth\n",
        "!mkdir /content/gansynth/midi\n",
        "!mkdir /content/gansynth/samples\n",
        "\n",
        "# Get default MIDI (Bach Prelude)\n",
        "!curl -o /content/gansynth/midi/bach.mid http://www.jsbach.net/midi/cs1-1pre.mid\n",
        "MIDI_SONG_DEFAULT = '/content/gansynth/midi/bach.mid'\n",
        "!curl -o /content/gansynth/midi/riff-default.mid http://storage.googleapis.com/magentadata/papers/gansynth/midi/arp.mid\n",
        "MIDI_RIFF_DEFAULT = '/content/gansynth/midi/riff-default.mid'\n",
        "\n",
        "!pip install -q -U magenta\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "import librosa\n",
        "from magenta.models.nsynth.utils import load_audio\n",
        "from magenta.models.gansynth.lib import flags as lib_flags\n",
        "from magenta.models.gansynth.lib import generate_util as gu\n",
        "from magenta.models.gansynth.lib import model as lib_model\n",
        "from magenta.models.gansynth.lib import util\n",
        "import matplotlib.pyplot as plt\n",
        "import note_seq\n",
        "from note_seq.notebook_utils import colab_play as play\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# File IO\n",
        "download = files.download\n",
        "\n",
        "def upload():\n",
        "  '''Upload a .wav file.'''\n",
        "  filemap = files.upload()\n",
        "  file_list = []\n",
        "  for key, value in filemap.items():\n",
        "    fname = os.path.join('/content/gansynth/midi', key)\n",
        "    with open(fname, 'wb') as f:\n",
        "      f.write(value)\n",
        "      print('Writing {}'.format(fname))\n",
        "    file_list.append(fname)\n",
        "  return file_list\n",
        "\n",
        "# GLOBALS\n",
        "CKPT_DIR = 'gs://magentadata/models/gansynth/acoustic_only'\n",
        "output_dir = '/content/gansynth/samples'\n",
        "BATCH_SIZE = 16\n",
        "SR = 16000\n",
        "\n",
        "# Make an output directory if it doesn't exist\n",
        "OUTPUT_DIR = util.expand_path(output_dir)\n",
        "if not tf.gfile.Exists(OUTPUT_DIR):\n",
        "  tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "\n",
        "# Load the model\n",
        "tf.reset_default_graph()\n",
        "flags = lib_flags.Flags({\n",
        "    'batch_size_schedule': [BATCH_SIZE],\n",
        "    'tfds_data_dir': \"gs://tfds-data/datasets\",\n",
        "})\n",
        "model = lib_model.Model.load_from_path(CKPT_DIR, flags)\n",
        "\n",
        "# Helper functions\n",
        "def load_midi(midi_path, min_pitch=36, max_pitch=84):\n",
        "  \"\"\"Load midi as a notesequence.\"\"\"\n",
        "  midi_path = util.expand_path(midi_path)\n",
        "  ns = note_seq.midi_file_to_sequence_proto(midi_path)\n",
        "  pitches = np.array([n.pitch for n in ns.notes])\n",
        "  velocities = np.array([n.velocity for n in ns.notes])\n",
        "  start_times = np.array([n.start_time for n in ns.notes])\n",
        "  end_times = np.array([n.end_time for n in ns.notes])\n",
        "  valid = np.logical_and(pitches >= min_pitch, pitches <= max_pitch)\n",
        "  notes = {'pitches': pitches[valid],\n",
        "           'velocities': velocities[valid],\n",
        "           'start_times': start_times[valid],\n",
        "           'end_times': end_times[valid]}\n",
        "  return ns, notes\n",
        "\n",
        "def get_envelope(t_note_length, t_attack=0.010, t_release=0.3, sr=16000):\n",
        "  \"\"\"Create an attack sustain release amplitude envelope.\"\"\"\n",
        "  t_note_length = min(t_note_length, 3.0)\n",
        "  i_attack = int(sr * t_attack)\n",
        "  i_sustain = int(sr * t_note_length)\n",
        "  i_release = int(sr * t_release)\n",
        "  i_tot = i_sustain + i_release  # attack envelope doesn't add to sound length\n",
        "  envelope = np.ones(i_tot)\n",
        "  # Linear attack\n",
        "  envelope[:i_attack] = np.linspace(0.0, 1.0, i_attack)\n",
        "  # Linear release\n",
        "  envelope[i_sustain:i_tot] = np.linspace(1.0, 0.0, i_release)\n",
        "  return envelope\n",
        "\n",
        "def combine_notes(audio_notes, start_times, end_times, velocities, sr=16000):\n",
        "  \"\"\"Combine audio from multiple notes into a single audio clip.\n",
        "\n",
        "  Args:\n",
        "    audio_notes: Array of audio [n_notes, audio_samples].\n",
        "    start_times: Array of note starts in seconds [n_notes].\n",
        "    end_times: Array of note ends in seconds [n_notes].\n",
        "    sr: Integer, sample rate.\n",
        "\n",
        "  Returns:\n",
        "    audio_clip: Array of combined audio clip [audio_samples]\n",
        "  \"\"\"\n",
        "  n_notes = len(audio_notes)\n",
        "  clip_length = end_times.max() + 3.0\n",
        "  audio_clip = np.zeros(int(clip_length) * sr)\n",
        "\n",
        "  for t_start, t_end, vel, i in zip(start_times, end_times, velocities, range(n_notes)):\n",
        "    # Generate an amplitude envelope\n",
        "    t_note_length = t_end - t_start\n",
        "    envelope = get_envelope(t_note_length)\n",
        "    length = len(envelope)\n",
        "    audio_note = audio_notes[i, :length] * envelope\n",
        "    # Normalize\n",
        "    audio_note /= audio_note.max()\n",
        "    audio_note *= (vel / 127.0)\n",
        "    # Add to clip buffer\n",
        "    clip_start = int(t_start * sr)\n",
        "    clip_end = clip_start + length\n",
        "    audio_clip[clip_start:clip_end] += audio_note\n",
        "\n",
        "  # Normalize\n",
        "  audio_clip /= audio_clip.max()\n",
        "  audio_clip /= 2.0\n",
        "  return audio_clip\n",
        "\n",
        "# Plotting tools\n",
        "def specplot(audio_clip):\n",
        "  p_min = np.min(36)\n",
        "  p_max = np.max(84)\n",
        "  f_min = librosa.midi_to_hz(p_min)\n",
        "  f_max = 2 * librosa.midi_to_hz(p_max)\n",
        "  octaves = int(np.ceil(np.log2(f_max) - np.log2(f_min)))\n",
        "  bins_per_octave = 36\n",
        "  n_bins = int(bins_per_octave * octaves)\n",
        "  C = librosa.cqt(audio_clip, sr=SR, hop_length=2048, fmin=f_min, n_bins=n_bins, bins_per_octave=bins_per_octave)\n",
        "  power = 10 * np.log10(np.abs(C)**2 + 1e-6)\n",
        "  plt.matshow(power[::-1, 2:-2], aspect='auto', cmap=plt.cm.magma)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "\n",
        "print('And...... Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFlCLrPyxI2Q"
      },
      "source": [
        "## 2(a): Random Interpolation\n",
        "\n",
        "These cells take the MIDI for a full song and interpolate between several random latent vectors (equally spaced in time) over the whole song. The result sounds like instruments that slowly and smoothly morph between each other.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "yDq-YKHoYhkz"
      },
      "source": [
        "#@title Choose a MIDI file\n",
        "\n",
        "#@markdown Upload a MIDI file _(.mid, single instrument)_ for audio synthesis, or use the provided default. You can find lots of free MIDI files [online](http://www.midiworld.com/files/).\n",
        "\n",
        "midi_file = \"Bach Prelude (Default)\" #@param [\"Bach Prelude (Default)\", \"Upload your own\"]\n",
        "\n",
        "midi_path = MIDI_SONG_DEFAULT\n",
        "if midi_file == \"Upload your own\":\n",
        "  try:\n",
        "    file_list = upload()\n",
        "    midi_path = file_list[0]\n",
        "  except Exception as e:\n",
        "    print('Upload Cancelled')\n",
        "\n",
        "ns, notes = load_midi(midi_path)\n",
        "print('Loaded {}'.format(midi_path))\n",
        "note_seq.plot_sequence(ns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "EwFvRyg6xSoz"
      },
      "source": [
        "#@title Generate random interpolation 🎵\n",
        "#@markdown Select the number of seconds to take in interpolating between each random instrument. Larger numbers will have slower and smoother interpolations.\n",
        "\n",
        "seconds_per_instrument = 5 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "\n",
        "#@markdown This cell plays and displays a [Constant-Q spectrogram](https://en.wikipedia.org/wiki/Constant-Q_transform) of the synthesized audio.\n",
        "\n",
        "# Distribute latent vectors linearly in time\n",
        "z_instruments, t_instruments = gu.get_random_instruments(\n",
        "    model, notes['end_times'][-1], secs_per_instrument=seconds_per_instrument)\n",
        "\n",
        "# Get latent vectors for each note\n",
        "z_notes = gu.get_z_notes(notes['start_times'], z_instruments, t_instruments)\n",
        "\n",
        "# Generate audio for each note\n",
        "print('Generating {} samples...'.format(len(z_notes)))\n",
        "audio_notes = model.generate_samples_from_z(z_notes, notes['pitches'])\n",
        "\n",
        "# Make a single audio clip\n",
        "audio_clip = combine_notes(audio_notes,\n",
        "                           notes['start_times'],\n",
        "                           notes['end_times'],\n",
        "                           notes['velocities'])\n",
        "\n",
        "# Play the audio\n",
        "print('\\nAudio:')\n",
        "play(audio_clip, sample_rate=SR)\n",
        "print('CQT Spectrogram:')\n",
        "specplot(audio_clip)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "EX-tM_Gp3X6W"
      },
      "source": [
        "#@title Download \n",
        "#@markdown Get the .wav file (optional)\n",
        "\n",
        "# Write the file\n",
        "fname = os.path.join(output_dir, 'generated_clip.wav')\n",
        "gu.save_wav(audio_clip, fname)\n",
        "download(fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scqmGhen5VVv"
      },
      "source": [
        "## 2(b): You Choose the Interpolation\n",
        "\n",
        "These cells allow you to choose two latent vectors and interpolate between them over a MIDI clip."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Jg7AlosuYm3M"
      },
      "source": [
        "#@title Choose a MIDI file\n",
        "\n",
        "#@markdown Upload a MIDI file _(.mid, single instrument)_ for audio synthesis, or use the provided default.\n",
        "\n",
        "midi_file = \"Arpeggio (Default)\" #@param [\"Arpeggio (Default)\", \"Upload your own\"]\n",
        "\n",
        "midi_path = MIDI_RIFF_DEFAULT\n",
        "if midi_file == \"Upload your own\":\n",
        "  try:\n",
        "    file_list = upload()\n",
        "    midi_path = file_list[0]\n",
        "    ns, notes_2 = load_midi(midi_path)\n",
        "  except Exception as e:\n",
        "    print('Upload Cancelled')\n",
        "else:\n",
        "  # Load Default, but slow it down 30%\n",
        "  ns, notes_2 = load_midi(midi_path)\n",
        "  notes_2['start_times'] *= 1.3\n",
        "  notes_2['end_times'] *= 1.3\n",
        "\n",
        "\n",
        "print('Loaded {}'.format(midi_path))\n",
        "note_seq.plot_sequence(ns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "lTWOFFom6tMr"
      },
      "source": [
        "#@title Sample some random instruments\n",
        "\n",
        "number_of_random_instruments = 10 #@param {type:\"slider\", min:4, max:16, step:1}\n",
        "pitch_preview = 60\n",
        "n_preview = number_of_random_instruments\n",
        "\n",
        "pitches_preview = [pitch_preview] * n_preview\n",
        "z_preview = model.generate_z(n_preview)\n",
        "\n",
        "audio_notes = model.generate_samples_from_z(z_preview, pitches_preview)\n",
        "for i, audio_note in enumerate(audio_notes):\n",
        "  print(\"Instrument: {}\".format(i))\n",
        "  play(audio_note, sample_rate=16000)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Vw9-tp6J5VV1"
      },
      "source": [
        "#@title Generate custom interpolation 🎵\n",
        "##@markdown Using the instrument numbers from the cell above, pick two instruments to interpolate between over the clip.\n",
        "\n",
        "#@markdown Using the instrument numbers from the cell above, create a list of instruments to interpolate between. You can repeat instruments.\n",
        "\n",
        "instruments = [0, 2, 4, 0] #@param\n",
        "\n",
        "#@markdown For each instrument in the list above, place it at a relative time [0-1.0] in the clip. __The list of times must _always increase_ and start at 0 and end at 1.0__.\n",
        "\n",
        "times = [0, 0.3, 0.6, 1.0] #@param\n",
        "\n",
        "# Force endpoints\n",
        "times[0] = -0.001\n",
        "times[-1] = 1.0\n",
        "\n",
        "#@markdown This cell plays and displays a [Constant-Q spectrogram](https://en.wikipedia.org/wiki/Constant-Q_transform) of the synthesized audio.\n",
        "\n",
        "z_instruments = np.array([z_preview[i] for i in instruments])\n",
        "t_instruments = np.array([notes_2['end_times'][-1] * t for t in times])\n",
        "\n",
        "# Get latent vectors for each note\n",
        "z_notes = gu.get_z_notes(notes_2['start_times'], z_instruments, t_instruments)\n",
        "\n",
        "# Generate audio for each note\n",
        "print('Generating {} samples...'.format(len(z_notes)))\n",
        "audio_notes = model.generate_samples_from_z(z_notes, notes_2['pitches'])\n",
        "\n",
        "# Make a single audio clip\n",
        "audio_clip = combine_notes(audio_notes,\n",
        "                           notes_2['start_times'],\n",
        "                           notes_2['end_times'],\n",
        "                           notes_2['velocities'])\n",
        "\n",
        "# Play the audio\n",
        "print('\\nAudio:')\n",
        "play(audio_clip, sample_rate=SR)\n",
        "print('CQT Spectrogram:')\n",
        "specplot(audio_clip)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "DCA9o-PI5VV_"
      },
      "source": [
        "#@title Download\n",
        "#@markdown Get the .wav file (optional)\n",
        "\n",
        "# Write the file\n",
        "fname = os.path.join(output_dir, 'generated_clip.wav')\n",
        "gu.save_wav(audio_clip, fname)\n",
        "download(fname)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}