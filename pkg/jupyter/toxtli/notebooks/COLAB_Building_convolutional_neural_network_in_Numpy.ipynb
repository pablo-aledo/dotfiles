{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "COLAB - Building convolutional neural network in Numpy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePqZiPBcq8HU"
      },
      "source": [
        "Adapted by Carlos Toxtli https://www.carlostoxtli.com/#colab-cnnscratch-1\n",
        "\n",
        "Source: https://github.com/SkalskiP/ILearnDeepLearning.py/blob/master/01_mysteries_of_neural_networks/06_numpy_convolutional_neural_net_IN_PROGRESS/Building%20convolutional%20neural%20network%20in%20Numpy.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cH6U8tKq2dc"
      },
      "source": [
        "# Building convolutional neural network in Numpy\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPrDAN8Fq2de"
      },
      "source": [
        "***Author: Piotr Skalski***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFItqZBZq2dg"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk6rxr2Wq2dh"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from mlxtend.data import loadlocal_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nxVIQ77q2dk"
      },
      "source": [
        "## Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvuUBq66q2dl"
      },
      "source": [
        "# number of samples in the data set\n",
        "N_SAMPLES = 1000\n",
        "# ratio between training and test sets\n",
        "TEST_SIZE = 0.1\n",
        "# size of the photo\n",
        "PHOTO_SIZE = 28\n",
        "# number of pixels in the photo\n",
        "PIXEL_NUMBER = PHOTO_SIZE * PHOTO_SIZE\n",
        "# neural network architecture\n",
        "NN_ARCHITECTURE = [\n",
        "    {\"input_dim\": PIXEL_NUMBER, \"output_dim\": 1000, \"activation\": \"relu\"},\n",
        "    {\"input_dim\": 1000, \"output_dim\": 1000, \"activation\": \"relu\"},\n",
        "    {\"input_dim\": 1000, \"output_dim\": 500, \"activation\": \"relu\"},\n",
        "    {\"input_dim\": 500, \"output_dim\": 500, \"activation\": \"relu\"},\n",
        "    {\"input_dim\": 500, \"output_dim\": 10, \"activation\": \"softmax\"},\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUx497XAq2do"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJPWdrVTq2dp"
      },
      "source": [
        "### Auxiliary function downloading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlY3s2wwq2dq"
      },
      "source": [
        "def download_mnist_dataset():\n",
        "    # The MNIST data set is available at http://yann.lecun.com, let's use curl to download it\n",
        "    if not os.path.exists(\"train-images-idx3-ubyte\"):\n",
        "        !curl -O http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
        "        !curl -O http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
        "        !curl -O http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
        "        !curl -O http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
        "        !gunzip t*-ubyte.gz\n",
        "        \n",
        "    # Let's use loadlocal_mnist available in mlxtend.data to get data in numpy array form.\n",
        "    X1, y1 = loadlocal_mnist(\n",
        "        images_path=\"train-images-idx3-ubyte\", \n",
        "        labels_path=\"train-labels-idx1-ubyte\")\n",
        "\n",
        "    X2, y2 = loadlocal_mnist(\n",
        "        images_path=\"t10k-images-idx3-ubyte\", \n",
        "        labels_path=\"t10k-labels-idx1-ubyte\")\n",
        "    \n",
        "    # We normalize the brightness values for pixels\n",
        "    X1 = X1.reshape(X1.shape[0], -1) / 255\n",
        "    X2 = X2.reshape(X2.shape[0], -1) /255\n",
        "\n",
        "    # Combine downloaded data bundles\n",
        "    X = np.concatenate([X1, X2])\n",
        "    y = np.concatenate([y1, y2])\n",
        "    \n",
        "    return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNgVyPh1q2dt"
      },
      "source": [
        "### Auxiliary function for labels one hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vEgrD4_q2dt"
      },
      "source": [
        "def one_hot_encoding(y):\n",
        "    n_values = np.max(y) + 1\n",
        "    return np.eye(n_values)[y]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQRvE21Tq2dv"
      },
      "source": [
        "### Auxiliary function for data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwSkYJw8q2dw"
      },
      "source": [
        "def prepare_data(split_percentage):\n",
        "    # Download data\n",
        "    X, y = download_mnist_dataset()\n",
        "    # One hot encode labels\n",
        "    y = one_hot_encoding(y)\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_percentage, random_state=42)\n",
        "    return np.transpose(X_train), np.transpose(X_test), np.transpose(y_train), np.transpose(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP4ZNpKGq2d0"
      },
      "source": [
        "X_train, X_test, y_train, y_test = prepare_data(TEST_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VavSwF_q2d2"
      },
      "source": [
        "### Auxiliary function to display the selected data set element"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTd2oRDuq2d3"
      },
      "source": [
        "def plot_digit(X, y, idx):\n",
        "    img = X[:, idx].reshape(28,28)\n",
        "    plt.imshow(img, cmap='Greys',  interpolation='nearest')\n",
        "    plt.title('true label: %d' % np.where(y[:, idx] != 0)[0][0])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4JDxSuwq2d5",
        "outputId": "64c1c49d-623c-422c-abb7-f841c46eb154"
      },
      "source": [
        "plot_digit(X_train, y_train, 3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5c5a65d0ddd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_digit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB1UNdmdq2d8"
      },
      "source": [
        "### Adaptation of the existing implementation to support multiple classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-u79eo8q2d9"
      },
      "source": [
        "def sigmoid(Z):\n",
        "    return 1/(1+np.exp(-Z))\n",
        "\n",
        "def relu(Z):\n",
        "    return np.maximum(0,Z)\n",
        "\n",
        "def softmax(Z):\n",
        "    # Column wise softmax\n",
        "    e_Z = np.exp(Z - np.max(Z))\n",
        "    return e_Z / e_Z.sum(axis = 0)\n",
        "\n",
        "def sigmoid_backward(dA, Z):\n",
        "    sig = sigmoid(Z)\n",
        "    return dA * sig * (1 - sig)\n",
        "\n",
        "def relu_backward(dA, Z):\n",
        "    dZ = np.array(dA, copy = True)\n",
        "    dZ[Z <= 0] = 0\n",
        "    return dZ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5jUcNG-q2eA"
      },
      "source": [
        "scores2D = np.array([[1, 2, 3, 6],\n",
        "                     [2, 4, 5, 6],\n",
        "                     [3, 8, 7, 6]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqH5cSwPq2eC",
        "outputId": "dfeb1075-9632-4ceb-81a0-ee40fcf0e5f5"
      },
      "source": [
        "softmax(scores2D)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.09003057, 0.00242826, 0.01587624, 0.33333333],\n",
              "       [0.24472847, 0.01794253, 0.11731043, 0.33333333],\n",
              "       [0.66524096, 0.97962921, 0.86681333, 0.33333333]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fgmrox8dq2eF"
      },
      "source": [
        "def init_layers(nn_architecture, seed = 99):\n",
        "    np.random.seed(seed)\n",
        "    number_of_layers = len(nn_architecture)\n",
        "    params_values = {}\n",
        "    \n",
        "    for idx, layer in enumerate(nn_architecture):\n",
        "        layer_idx = idx + 1\n",
        "        \n",
        "        layer_input_size = layer[\"input_dim\"]\n",
        "        layer_output_size = layer[\"output_dim\"]\n",
        "        \n",
        "        params_values['W' + str(layer_idx)] = np.random.randn(\n",
        "            layer_output_size, layer_input_size) * 0.1\n",
        "        params_values['b' + str(layer_idx)] = np.random.randn(\n",
        "            layer_output_size, 1) * 0.1\n",
        "        \n",
        "    return params_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PQfBMyoq2eN"
      },
      "source": [
        "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
        "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
        "    \n",
        "    if activation is \"relu\":\n",
        "        activation_func = relu\n",
        "    elif activation is \"sigmoid\":\n",
        "        activation_func = sigmoid\n",
        "    elif activation is \"softmax\":\n",
        "        activation_func = softmax\n",
        "    else:\n",
        "        raise Exception('Non-supported activation function')\n",
        "        \n",
        "    return activation_func(Z_curr), Z_curr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TctHFofkq2eP"
      },
      "source": [
        "def full_forward_propagation(X, params_values, nn_architecture):\n",
        "    memory = {}\n",
        "    A_curr = X\n",
        "    \n",
        "    for idx, layer in enumerate(nn_architecture):\n",
        "        layer_idx = idx + 1\n",
        "        A_prev = A_curr\n",
        "        \n",
        "        activ_function_curr = layer[\"activation\"]\n",
        "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
        "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
        "        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n",
        "        \n",
        "        memory[\"A\" + str(idx)] = A_prev\n",
        "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
        "       \n",
        "    return A_curr, memory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alV9jkPNq2eR"
      },
      "source": [
        "# def get_cost_value(Y_hat, Y, eps = 0.001):\n",
        "#     m = Y_hat.shape[1]\n",
        "#     cost = -1 / m * (np.dot(Y, np.log(Y_hat + eps).T) + np.dot(1 - Y, np.log(1 - Y_hat  + eps).T))\n",
        "#     return np.squeeze(cost)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYYJDfRfq2eT"
      },
      "source": [
        "def multi_class_cross_entropy_loss(Y_hat, Y):\n",
        "    m = Y_hat.shape[1]\n",
        "    loss = - np.sum(Y * np.log(Y_hat)) / m\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xepSreLZq2eV"
      },
      "source": [
        "# def convert_prob_into_class(probs):\n",
        "#     probs_ = np.copy(probs)\n",
        "#     probs_[probs_ > 0.5] = 1\n",
        "#     probs_[probs_ <= 0.5] = 0\n",
        "#     return probs_\n",
        "\n",
        "\n",
        "# def get_accuracy_value(Y_hat, Y):\n",
        "#     Y_hat_ = convert_prob_into_class(Y_hat)\n",
        "#     return (Y_hat_ == Y).all(axis=0).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYMHoJP7q2eY"
      },
      "source": [
        "def multi_class_accuracy(Y_hat, Y):\n",
        "    n_values = Y_hat.shape[0]\n",
        "    values = Y_hat.argmax(axis=0)\n",
        "    Y_hat_one_hot = np.eye(n_values)[values].T\n",
        "    return (Y_hat_one_hot == Y).all(axis=0).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW_MvBvMq2ed"
      },
      "source": [
        "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
        "    m = A_prev.shape[1]\n",
        "    \n",
        "    if activation is \"relu\":\n",
        "        backward_activation_func = relu_backward\n",
        "    elif activation is \"sigmoid\":\n",
        "        backward_activation_func = sigmoid_backward\n",
        "    elif activation is \"softmax\":\n",
        "        # TEST OF COURSERA SOLUTION\n",
        "        backward_activation_func = lambda dA_curr, Z_curr: dA_curr\n",
        "    else:\n",
        "        raise Exception('Non-supported activation function')\n",
        "    \n",
        "    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
        "    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
        "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
        "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
        "\n",
        "    return dA_prev, dW_curr, db_curr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSpzfB8-q2eg"
      },
      "source": [
        "def full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture, eps = 0.000000000001):\n",
        "    grads_values = {}\n",
        "    m = Y.shape[1]\n",
        "    Y = Y.reshape(Y_hat.shape)\n",
        "    \n",
        "#     dA_prev = - (np.divide(Y, Y_hat + eps) - np.divide(1 - Y, 1 - Y_hat + eps))\n",
        "    # TEST OF COURSERA SOLUTION\n",
        "    dA_prev = Y_hat - Y\n",
        "    \n",
        "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
        "        layer_idx_curr = layer_idx_prev + 1\n",
        "        activ_function_curr = layer[\"activation\"]\n",
        "        \n",
        "        dA_curr = dA_prev\n",
        "        \n",
        "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
        "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
        "        \n",
        "        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
        "        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
        "        \n",
        "        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n",
        "            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n",
        "        \n",
        "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
        "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
        "    \n",
        "    return grads_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rphZScXvq2ei"
      },
      "source": [
        "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
        "\n",
        "    for layer_idx, layer in enumerate(nn_architecture, 1):\n",
        "        params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
        "        params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
        "\n",
        "    return params_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBXoWIVBq2ek"
      },
      "source": [
        "def train(X, Y, nn_architecture, epochs, learning_rate, verbose=False, callback=None):\n",
        "    params_values = init_layers(nn_architecture, 2)\n",
        "    cost_history = []\n",
        "    accuracy_history = []\n",
        "    \n",
        "    for i in range(epochs):\n",
        "        Y_hat, cashe = full_forward_propagation(X, params_values, nn_architecture)\n",
        "        \n",
        "#         print(Y_hat.shape)\n",
        "#         print(Y.shape)\n",
        "        \n",
        "        cost = multi_class_cross_entropy_loss(Y_hat, Y)\n",
        "        cost_history.append(cost)\n",
        "        \n",
        "#         print(cost)\n",
        "        \n",
        "        accuracy = multi_class_accuracy(Y_hat, Y)\n",
        "        accuracy_history.append(accuracy)\n",
        "        \n",
        "#         print(accuracy)\n",
        "        \n",
        "        grads_values = full_backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)\n",
        "        params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
        "        \n",
        "        if(i % 5 == 0):\n",
        "            if(verbose):\n",
        "                print(\"Iteration: {:05} - cost: {:.5f} - accuracy: {:.5f}\".format(i, cost, accuracy))\n",
        "            if(callback is not None):\n",
        "                callback(i, params_values)\n",
        "            \n",
        "    return params_values, cost_history, accuracy_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4qF2oXyq2en"
      },
      "source": [
        "SUBSET_SIZE = 10000\n",
        "X_train = X_train[:,:SUBSET_SIZE]\n",
        "y_train = y_train[:,:SUBSET_SIZE]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbWU6k8jq2ep",
        "outputId": "8cb1ef72-c204-4ca6-df81-80bc4e14bcd2"
      },
      "source": [
        "params_values, cost_history, accuracy_history = train(X_train, y_train, NN_ARCHITECTURE, 100, 0.01, verbose=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 00000 - cost: 18.05769 - accuracy: 0.09350\n",
            "Iteration: 00005 - cost: 6.55857 - accuracy: 0.34620\n",
            "Iteration: 00010 - cost: 1.29068 - accuracy: 0.67030\n",
            "Iteration: 00015 - cost: 1.01207 - accuracy: 0.73030\n",
            "Iteration: 00020 - cost: 0.85441 - accuracy: 0.76560\n",
            "Iteration: 00025 - cost: 0.75597 - accuracy: 0.78840\n",
            "Iteration: 00030 - cost: 0.68273 - accuracy: 0.80950\n",
            "Iteration: 00035 - cost: 0.62498 - accuracy: 0.82370\n",
            "Iteration: 00040 - cost: 0.57798 - accuracy: 0.83580\n",
            "Iteration: 00045 - cost: 0.53846 - accuracy: 0.84670\n",
            "Iteration: 00050 - cost: 0.50445 - accuracy: 0.85660\n",
            "Iteration: 00055 - cost: 0.47479 - accuracy: 0.86430\n",
            "Iteration: 00060 - cost: 0.44866 - accuracy: 0.87120\n",
            "Iteration: 00065 - cost: 0.42536 - accuracy: 0.87710\n",
            "Iteration: 00070 - cost: 0.40445 - accuracy: 0.88430\n",
            "Iteration: 00075 - cost: 0.38556 - accuracy: 0.88860\n",
            "Iteration: 00080 - cost: 0.36836 - accuracy: 0.89520\n",
            "Iteration: 00085 - cost: 0.35262 - accuracy: 0.89900\n",
            "Iteration: 00090 - cost: 0.33812 - accuracy: 0.90310\n",
            "Iteration: 00095 - cost: 0.32469 - accuracy: 0.90650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cTkhH2Aq2er",
        "outputId": "a74724a8-0479-42b7-b0e8-409eec0f4be3"
      },
      "source": [
        "y_test_hat, _ = full_forward_propagation(X_test, params_values, NN_ARCHITECTURE)\n",
        "accuracy = multi_class_accuracy(y_test_hat, y_test)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8602857142857143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUU80wC4q2es"
      },
      "source": [
        "## Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSLzvWudq2et"
      },
      "source": [
        "https://gombru.github.io/2018/05/23/cross_entropy_loss/\n",
        "http://wiki.fast.ai/index.php/Log_Loss\n",
        "http://cs231n.github.io/neural-networks-case-study/\n",
        "http://saitcelebi.com/tut/output/part2.html\n",
        "https://deepnotes.io/softmax-crossentropy\n",
        "https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsbbAJNTq2et"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}