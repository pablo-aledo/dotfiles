{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "COLAB - gpu_usage.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szkaHk7d_5Js"
      },
      "source": [
        "Adapted by Carlos Toxtli https://www.carlostoxtli.com/#colab-gpu-1\n",
        "\n",
        "Source: https://github.com/gmihaila/machine_learning_platform_gpu/blob/master/gpu_usage.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j05CMG_z_05a"
      },
      "source": [
        "## Overview\n",
        "\n",
        "* #### How to use up to 4 gpus (max allowed on MLP) using Keras.\n",
        "* #### How to supervise your gpu memory allocations.\n",
        "* #### How to deal with common OOM (Out Of Memory) issues.\n",
        "* #### How to partition your GPUs\n",
        "* #### How to save your model for later\n",
        "* #### How to make sure you are not hogging the GPUs.\n",
        "* #### How to deal with crazy models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtMfqLPd_05c"
      },
      "source": [
        "# BUILD MODEL FUNCTION\n",
        "\n",
        "def BuildToyModel():\n",
        "    # generate some dummy data\n",
        "    n_examples = 1000\n",
        "    n_features = 100\n",
        "\n",
        "    x_train = np.random.random((n_examples, n_features))\n",
        "    x_train = np.round(x_train, 2)\n",
        "    x_train *= 100\n",
        "    x_train = np.array(x_train, dtype=int)\n",
        "\n",
        "    n_words = np.max(x_train)\n",
        "\n",
        "    y_train = np.random.random(n_examples)\n",
        "\n",
        "    # number of units in LSTM\n",
        "    n_units = 256\n",
        "    # numbe rof words\n",
        "    n_words = 100500   #vocabulary size\n",
        "    size_emb = 300     #size of embedding has to match\n",
        "    size_seq = 50\n",
        "\n",
        "    embedding_matrix = np.random.random((n_words+2, size_emb))\n",
        "\n",
        "    # build model\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=(n_words+2), output_dim=size_emb, weights=[embedding_matrix],\n",
        "                    mask_zero=False, trainable=False))\n",
        "    # LSTM LAYER/S\n",
        "    model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "    # TREAT AS REGRESSION PROBLEM\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    return model, x_train, y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rnj0Iq_T_05g"
      },
      "source": [
        "## How to use up to 4 gpus (max allowed on MLP) using Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1h7-1Vt_05h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31f0cfd4-0331-47ff-c958-6ae86f95683b"
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "\n",
        "\n",
        "from keras.utils.training_utils import multi_gpu_model #<------------------Esential for GPUs usage\n",
        "\n",
        "\n",
        "# Build model\n",
        "model, x_train, y_train = BuildToyModel()\n",
        "\n",
        "# Add how many GPUs you want/have\n",
        "model = multi_gpu_model(model, gpus=4)          #<------------------Esential Part to run on GPUs\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])\n",
        "\n",
        "# model summary\n",
        "print(model.summary())                          #<------------------See lambda layers distributed on each GPU\n",
        "\n",
        "# batches\n",
        "n_batch = 32       #<------------------make sure your batch is not too small\n",
        "# epochs\n",
        "n_epoch = 10\n",
        "\n",
        "# train model\n",
        "model.fit(x_train, y_train, epochs=n_epoch, batch_size=n_batch, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utGEa3Qz_05j"
      },
      "source": [
        "### How to supervise your gpu memory allocations.Â¶"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrhEFQ61_05k"
      },
      "source": [
        "open terminal and run\n",
        "watch -n 1 nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4KvEbuw_05l"
      },
      "source": [
        "### How to deal with common OOM (Out Of Memory) issues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DANmeX2D_05m"
      },
      "source": [
        "keep batch size regular size. Too small batch size will crash your notebook.  too big will get you OOM\n",
        "\n",
        "If your container allocated 4 gpus and you run your current notebook on all 4 gpus and open a new notebook and try to run on all 4 gpus again, you will get OOM - Even if your model is running anymore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIxncbyy_05m"
      },
      "source": [
        "## How to partition your GPUs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J_sRW_E_05n"
      },
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"enter what GPUs to use\"\n",
        "You will see in your MLP container email what GPUs you receive\n",
        "\n",
        "Don't forget to adjust the number of GPUs you mention to Keras!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mh7UgBHE_05o"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"  #<------ CHECK YOUR MLP EMAIL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be1H0Rhr_05q"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "\n",
        "\n",
        "from keras.utils.training_utils import multi_gpu_model #<------------------Esential for GPUs usage\n",
        "\n",
        "\n",
        "# Build model\n",
        "model, x_train, y_train = BuildToyModel()\n",
        "\n",
        "# Add how many GPUs you want/have\n",
        "model = multi_gpu_model(model, gpus=2)          #<------------------Esential Part to run on GPUs\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])\n",
        "\n",
        "# model summary\n",
        "print(model.summary())                          #<------------------See lambda layers distributed on each GPU\n",
        "\n",
        "# batches\n",
        "n_batch = 32       #<------------------make sure your batch is not too small\n",
        "# epochs\n",
        "n_epoch = 10\n",
        "\n",
        "# train model\n",
        "model.fit(x_train, y_train, epochs=n_epoch, batch_size=n_batch, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FZAFCSe_05t"
      },
      "source": [
        "## How to save your model for later\n",
        "\n",
        "Let's say you train your model on all 4 GPUs and want to save it so you can do transfer learning later or just have the model ready trainned. \n",
        "You have to save the weights of the model. You can't save the whole model. You can only save a whole model if you run on 1 GPU or No GPU.\n",
        "In this case with the model running on 4 GPUs, you need to re-build the model the exact way you did before training and with the exact number of GPUs you used on training. After you compile the model you can simply just load the weights and it will run like a charm (get it? PyCharm!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VM3O6QJ_05u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCbakDwt_05w"
      },
      "source": [
        "## How to make sure you are not hogging the GPUs.\n",
        "\n",
        "When you don't need the model anymore you have to kill your notebook to make sure you are not blocking the GPUs for others"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxWdPBKT_05x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9r5aeIm_05z"
      },
      "source": [
        "## Ho to use GPUs on MLP ðŸ¦Œ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAkXzFIF_050"
      },
      "source": [
        "How to use up to 4 gpus (max allowed) using Keras\n",
        "How to supervise your gpu memory allocations.\n",
        "How to deal with common OOM issues\n",
        "How to make sure you are not hogging the GPUs\n",
        "If model goes crazy, or befor running a model for a few days, restart notebook kernel, shut it down and re-run it. It happend to me! especially if you run on GPUs\n",
        "\n",
        "\n",
        "It works! - deactivate 2 gpus when have 4 and see if contianer cna be created, or use other gpus on other notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw5kWFjW_051"
      },
      "source": [
        "- if use tensorflow image automaticaaly uses 1 gpu. No need for any setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PH1944LP_051"
      },
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"  #<------ DEACTIVATE ALL GPUS\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kEbu393_053"
      },
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"  #<------ DEACTIVATE ALL GPUS\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P21JeUmH_055"
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "\n",
        "\n",
        "from keras.utils.training_utils import multi_gpu_model #<------------------Esential for GPUs usage\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pMa5s9l_057"
      },
      "source": [
        "## Toy Example to see the GPUs in aciton! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "H22HVqwh_058"
      },
      "source": [
        "# generate some dummy data\n",
        "n_examples = 1000\n",
        "n_features = 100\n",
        "\n",
        "x_train = np.random.random((n_examples, n_features))\n",
        "x_train = np.round(x_train, 2)\n",
        "x_train *= 100\n",
        "x_train = np.array(x_train, dtype=int)\n",
        "\n",
        "n_words = np.max(x_train)\n",
        "\n",
        "y_train = np.random.random(n_examples)\n",
        "\n",
        "# number of units in LSTM\n",
        "n_units = 256\n",
        "# numbe rof words\n",
        "n_words = 100500   #vocabulary size\n",
        "size_emb = 300     #size of embedding has to match\n",
        "size_seq = 50\n",
        "\n",
        "\n",
        "embedding_matrix = np.random.random((n_words+2, size_emb))\n",
        "\n",
        "# with tf.device(\"/cpu:0\"):\n",
        "# build model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=(n_words+2), output_dim=size_emb, weights=[embedding_matrix],\n",
        "                    mask_zero=False, trainable=False))\n",
        "\n",
        "# LSTM LAYER/S\n",
        "model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "\n",
        "# TREAT AS REGRESSION PROBLEM\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Add how many GPUs you want/have\n",
        "model = multi_gpu_model(model, gpus=2)          #<------------------Esential Part to run on GPUs\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])\n",
        "\n",
        "\n",
        "# model summary\n",
        "print(model.summary())                          #<------------------See lambda layers distributed on each GPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fo4ua-k_05-"
      },
      "source": [
        "## Train model and see the GPUs working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCuSD3uc_05-"
      },
      "source": [
        "# batches\n",
        "n_batch = 32       #<------------------make sure your batch is not too small\n",
        "# epochs\n",
        "n_epoch = 10\n",
        "# train model\n",
        "model.fit(x_train, y_train, epochs=n_epoch, batch_size=n_batch, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXgXakjR_06A"
      },
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,3\"  #<------ DEACTIVATE ALL GPUS\n",
        "\n",
        "# generate some dummy data\n",
        "n_examples = 1000\n",
        "n_features = 100\n",
        "\n",
        "x_train = np.random.random((n_examples, n_features))\n",
        "x_train = np.round(x_train, 2)\n",
        "x_train *= 100\n",
        "x_train = np.array(x_train, dtype=int)\n",
        "\n",
        "n_words = np.max(x_train)\n",
        "\n",
        "y_train = np.random.random(n_examples)\n",
        "\n",
        "# number of units in LSTM\n",
        "n_units = 256\n",
        "# numbe rof words\n",
        "n_words = 100500   #vocabulary size\n",
        "size_emb = 300     #size of embedding has to match\n",
        "size_seq = 50\n",
        "\n",
        "\n",
        "embedding_matrix = np.random.random((n_words+2, size_emb))\n",
        "\n",
        "# with tf.device(\"/cpu:0\"):\n",
        "# build model\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(input_dim=(n_words+2), output_dim=size_emb, weights=[embedding_matrix],\n",
        "                    mask_zero=False, trainable=False))\n",
        "\n",
        "# LSTM LAYER/S\n",
        "model2.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "\n",
        "# TREAT AS REGRESSION PROBLEM\n",
        "\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Add how many GPUs you want/have\n",
        "model2 = multi_gpu_model(model, gpus=2)          #<------------------Esential Part to run on GPUs\n",
        "\n",
        "model2.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])\n",
        "\n",
        "\n",
        "# model summary\n",
        "print(model2.summary())                          #<------------------See lambda layers distributed on each GPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7pmN32H_06C"
      },
      "source": [
        "# batches\n",
        "n_batch = 32       #<------------------make sure your batch is not too small\n",
        "# epochs\n",
        "n_epoch = 10\n",
        "# train model\n",
        "model2.fit(x_train, y_train, epochs=n_epoch, batch_size=n_batch, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}