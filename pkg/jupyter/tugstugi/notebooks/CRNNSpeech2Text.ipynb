{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CRNNSpeech2Text.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REk8vxKiX4-T",
        "colab_type": "text"
      },
      "source": [
        "English speech recognition demo using [tugstugi/mongolian-speech-recognition](https://github.com/tugstugi/mongolian-speech-recognition) with an OCR network aka [CRNN](https://arxiv.org/abs/1507.05717) :)\n",
        "\n",
        "An OCR model predicts from an image a sequence of characters. If you treat a spectrogram as image, a speech recognition model will also predict from an image (spectogram) a sequence of characters. So it means, you will be able to use an OCR network for the speech recognition task. Also Deepspeech can be used for the optical character recognition.\n",
        "\n",
        "For other deep-learning Colab notebooks, visit [tugstugi/dl-colab-notebooks](https://github.com/tugstugi/dl-colab-notebooks).\n",
        "\n",
        "## Install\n",
        "\n",
        "Clone the project and install the dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XDZyYFZXr6p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "44db55ea-16e8-4b41-8498-52287e63819d"
      },
      "source": [
        "import os\n",
        "import time\n",
        "from os.path import exists, join, basename, splitext\n",
        "\n",
        "git_repo_url = 'https://github.com/tugstugi/mongolian-speech-recognition.git'\n",
        "project_name = splitext(basename(git_repo_url))[0]\n",
        "if not exists(project_name):\n",
        "  !git clone -q {git_repo_url}\n",
        "  !cd {project_name} && git checkout a79b916\n",
        "  !cd {project_name} && pip install -q -r requirements.txt\n",
        "  !pip install -q wget\n",
        "  !pip install -q https://github.com/tugstugi/dl-colab-notebooks/archive/colab_utils.zip\n",
        "\n",
        "import sys  \n",
        "sys.path.append(project_name)\n",
        "  \n",
        "from IPython.display import Audio, display, clear_output\n",
        "import ipywidgets as widgets\n",
        "import numpy as np\n",
        "from scipy.io import wavfile\n",
        "from dl_colab_notebooks.audio import record_audio, upload_audio"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Note: checking out 'a79b916'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at a79b916 fix crnn\n",
            "\u001b[K     |████████████████████████████████| 204kB 4.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 9.2MB/s \n",
            "\u001b[?25h  Building wheel for python-speech-features (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for python-levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     - 51kB 53.4MB/s\n",
            "\u001b[?25h  Building wheel for Deep-Learning-Colab-Notebook-Utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U5VjGarg0kc",
        "colab_type": "text"
      },
      "source": [
        "For the language model support, we need also the `ctcdecode` lib:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw59cU6Mg7c0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not exists('ctcdecode'):\n",
        "  !git clone -q --recursive https://github.com/parlance/ctcdecode.git\n",
        "  !cd ctcdecode && pip install ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW79XHCpYRmT",
        "colab_type": "text"
      },
      "source": [
        "## Download Model\n",
        "\n",
        "Downlad the pre-trained model (16 epochs / 15 hours / 14.6% WER on LibriSpeech dev-clean) and initialize it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6sqJgrsYQMp",
        "colab_type": "code",
        "outputId": "53633188-8913-4dc0-b977-f151e18c19ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "checkpoint_file = 'checkpoint.pth'\n",
        "if not exists(checkpoint_file):\n",
        "  !wget -q -O {checkpoint_file} 'https://docs.google.com/uc?export=download&id=1Bt1TQD2a_RIefPW3iosa-yXtqkjehwt-'\n",
        "\n",
        "import torch\n",
        "from models.crnn import Speech2TextCRNN\n",
        "from datasets.libri_speech import vocab\n",
        "from datasets import *\n",
        "from utils import load_checkpoint\n",
        "from decoder import *\n",
        "model = Speech2TextCRNN(vocab)\n",
        "load_checkpoint(checkpoint_file, model, optimizer=None, use_gpu=True)\n",
        "model = model.float().cuda().eval()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded checkpoint epoch=16 step=118816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6-zRtqHiejP",
        "colab_type": "text"
      },
      "source": [
        "Download a n-gram binary language model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vInntYbdijag",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "13a440a4-90e4-41e2-ae67-43fce0e6f6c0"
      },
      "source": [
        "lm_model = 'lm.binary'\n",
        "if not exists(lm_model):\n",
        "  !wget -q -O {lm_model} http://www.openslr.org/resources/11/3-gram.pruned.1e-7.arpa.gz\n",
        "  !gunzip {lm_model}"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gzip: lm.binary: unknown suffix -- ignored\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsftxHiOZprK",
        "colab_type": "text"
      },
      "source": [
        "## Record or Upload Speech"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-EEy7IcZGdS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title { run: \"auto\" }\n",
        "\n",
        "SAMPLE_RATE = 16000\n",
        "record_or_upload = \"Record\" #@param [\"Record\", \"Upload (.mp3 or .wav)\"]\n",
        "record_seconds =   10#@param {type:\"number\", min:1, max:10, step:1}\n",
        "\n",
        "def _recognize(audio):\n",
        "  display(Audio(audio, rate=SAMPLE_RATE, autoplay=True))\n",
        "  wavfile.write('test.wav', SAMPLE_RATE, (32767*audio).astype(np.int16))\n",
        "\n",
        "  audio = Compose([LoadAudio(), ComputeMelSpectrogram(), ResizeMelSpectrogram()])({'fname': 'test.wav', 'text':''})['input']\n",
        "  torch.set_grad_enabled(False)\n",
        "  inputs = torch.from_numpy(audio).unsqueeze(0)\n",
        "  inputs = inputs.permute(0, 2, 1).cuda()\n",
        "  outputs = model(inputs)\n",
        "  outputs = outputs.softmax(2).permute(1, 0, 2)\n",
        "  greedy_decoder = GreedyDecoder(labels=vocab)\n",
        "  decoded_output, _ = greedy_decoder.decode(outputs)\n",
        "  print(\"\\nwithout LM:\")\n",
        "  print(decoded_output[0][0])\n",
        "  print(\"\\n\")\n",
        "\n",
        "  ALPHA = 0.3  # How much do you trust for LM? 0 means don't use LM, bigger values more trust in LM\n",
        "  BETA = 1.85  # not so important, using DeepSpeech default one\n",
        "  beam_ctc_decoder = BeamCTCDecoder(labels='$' + vocab[1:].upper(), num_processes=4,\n",
        "                                            lm_path=lm_model,\n",
        "                                            alpha=ALPHA, beta=BETA,\n",
        "                                            cutoff_top_n=40, cutoff_prob=1.0, beam_width=1000)\n",
        "  decoded_output, _ = beam_ctc_decoder.decode(outputs)\n",
        "  print(\"with LM:\")\n",
        "  print(decoded_output[0][0].lower())\n",
        "\n",
        "\n",
        "def _record_audio(b):\n",
        "  clear_output()\n",
        "  audio = record_audio(record_seconds, sample_rate=SAMPLE_RATE)\n",
        "  _recognize(audio)\n",
        "def _upload_audio(b):\n",
        "  clear_output()\n",
        "  audio = upload_audio(sample_rate=SAMPLE_RATE)\n",
        "  _recognize(audio)\n",
        "\n",
        "if record_or_upload == \"Record\":\n",
        "  button = widgets.Button(description=\"Record Speech\")\n",
        "  button.on_click(_record_audio)\n",
        "  display(button)\n",
        "else:\n",
        "  try:\n",
        "    _upload_audio(\"\")\n",
        "  except TypeError:\n",
        "    print(\"uploading failed\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}