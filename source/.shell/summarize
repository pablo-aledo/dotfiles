summarize_method=keywords
summarize_evaluate_vects="true"
summarize_dir=$HOME/summarize

ncd.get_features(){
    cp -r $summarize_dir/txts/* $summarize_dir/features
}

ncd.txt2vec() {
	for file in $summarize_dir/features/*
	do
		gzip_distance $1 $file
	done | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g' -e 's/,/, /g'
}

keywords.get_features(){
    [ -e $summarize_dir/features/keywords ] && return
    mkdir -p $summarize_dir/features
    [ -e ~/Dotfiles/keywords/keywords ] && cp ~/Dotfiles/keywords/keywords $summarize_dir/features/keywords
    [ -e ~/Dotfiles/keywords/keywords ] || cat $summarize_dir/txts/* | sed 's/ /\n/g' | sort | uniq > $summarize_dir/features/keywords
}

keywords.txt2vec(){
    normalize="false"
    cat $1 | sed 's/ /\n/g' | sort | uniq -c > /tmp/keywords
    total=$( cat /tmp/keywords | awk '{sum += $1} END {print sum}' )

    for a in $(cat $summarize_dir/features/keywords | grep -v '^#' | grep -v '^$')
    do
         value=$( cat /tmp/keywords | egrep $a | awk '{sum += $1} END {print sum}' )
         [ "$value" = "" ] && value=0
         [ $normalize = "true" ] && echo "print($value/$total.)" | python
         [ $normalize = "true" ] || echo $value
    done | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g' -e 's/,/, /g'
}

summarize_rec(){
	filename="$1"
	destination="$2"

	case $filename in
		(*.rar|*.RAR)
			rm -fr /tmp/summarize
			mkdir -p /tmp/summarize
			cd /tmp/summarize
			unrar x "$filename" >/dev/null 2>/dev/null
			find $PWD | while read line
			do
				summarize_rec "$line" "$destination" 
			done
			[ $PWD = $OLDPWD ] || cd $OLDPWD
			;;
		(*.zip|*.ZIP)
			rm -fr /tmp/summarize
			mkdir -p /tmp/summarize
			cd /tmp/summarize
			unzip "$filename" >/dev/null 2>/dev/null
			find $PWD | while read line
			do
				summarize_rec "$line" "$destination" 
			done
			[ $PWD = $OLDPWD ] || cd $OLDPWD
			;;
		(*.pdf|*.PDF)
			pdftotext -f 1 -l 5 "$filename" - >> "$destination"
			;;
		(*.epub|*.EPUB)
			epub2txt "$filename" | head -n 100 >> "$destination"
			;;
		(*.mobi|*.MOBI)
			ebook-convert "$filename" "$destination"
			;;
		(*.txt|*.TXT)
			cat "$filename" | head -n 100 >> "$destination"
			;;
		(*)
			file "$filename" | grep 'text' >/dev/null && cat "$filename" >> "$destination"
			file "$filename" | grep 'text' >/dev/null || echo "$filename" >> "$destination"
	esac
}

gzip_distance(){
    rm -fr /tmp/s1 /tmp/s2
    summarize.doc2txt "$1" /tmp/s1
    summarize.doc2txt "$2" /tmp/s2
    s1=$( cat /tmp/s1 | gzip -f | wc -c )
    s2=$( cat /tmp/s2 | gzip -f | wc -c )
    s12=$( cat /tmp/s1 /tmp/s2 | gzip | wc -c )

   python -c "print float($s12 - min($s1, $s2))/float(max($s1,$s2))"
}

pca.fit(){
    rm -fr /tmp/softhashes
    for file in $summarize_dir/vectors/*
    do
        cat $file >> /tmp/softhashes
    done
    sed -i -e 's/\[//g' -e 's/\]//g' -e 's/,//g' /tmp/softhashes
	python -c "
import numpy as np
from sklearn.decomposition import PCA
summarize_dir='$summarize_dir'
X=np.loadtxt('/tmp/softhashes')
pca=PCA()
pca.fit(X)
np.savetxt(summarize_dir + '/pca/average',np.mean(X,0))
np.savetxt(summarize_dir + '/pca/pca',pca.components_)
np.savetxt(summarize_dir + '/pca/eigenv',pca.explained_variance_)
"
}

pca_and_rank.fit(){
    rm -fr /tmp/softhashes
    for file in $summarize_dir/vectors/*
    do
        cat $file >> /tmp/softhashes
    done
    sed -i -e 's/\[//g' -e 's/\]//g' -e 's/,//g' /tmp/softhashes
	python -c "
import numpy as np
import math
from sklearn.decomposition import PCA
summarize_dir='$summarize_dir'
X=np.loadtxt('/tmp/softhashes')
pca=PCA()
pca.fit(X)
np.savetxt(summarize_dir + '/pca/average',np.mean(X,0))
np.savetxt(summarize_dir + '/pca/pca',pca.components_)
np.savetxt(summarize_dir + '/pca/eigenv',pca.explained_variance_)

Y = pca.transform(X)

def dotproduct(v1, v2):
  return sum((a*b) for a, b in zip(v1, v2))

def length(v):
  return math.sqrt(dotproduct(v, v))

def angle(v1, v2):
  return math.acos(dotproduct(v1, v2) / (length(v1) * length(v2)))

with open(summarize_dir + '/pca/ranked', 'w') as file:
    for y_axis in np.identity(pca.components_.shape[0]):
        y = min(Y, key=lambda v: angle(y_axis, v))
        x_appr = pca.inverse_transform(y)
        x = min(X, key=lambda x: np.linalg.norm(x - x_appr))
        print >> file, x
"
}

classifier.fit(){

    rm -fr /tmp/classifier.x /tmp/classifier.y
    ls $summarize_dir/hashes/ | sed 's/\..*//g' | sort | uniq | nl - | sed 's/\t/ /g' | sed -e 's/ \+/ /g' > $summarize_dir/classifier/map
    for file in $summarize_dir/hashes/*
    do
        cat $file >> /tmp/classifier.x
        y=$(basename $file | sed 's/\..*//g')
        id=$(cat $summarize_dir/classifier/map | grep " $y$" | awk '{print $1}')
        echo $id >> /tmp/classifier.y
    done

    x=$(cat /tmp/classifier.x | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g')
    y=$(cat /tmp/classifier.y | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g')

	python -c "
import numpy as np
from sklearn import linear_model
from sklearn.externals import joblib
X = np.array($x)
Y = np.array($y)
summarize_dir='$summarize_dir'
clf = linear_model.SGDClassifier()
clf.fit(X, Y)
joblib.dump(clf, summarize_dir + '/classifier/classifier.pkl')
"
}

vect2s(){
    read vect
    summarize.hash2key "$( python -c "
vect=$vect
print([ (i/float(max(vect)+0.00001) - 0.5)/3.0 for i in vect ])
")" | sed -e 's/0/./g' -e 's/^/[/g' -e 's/$/]/g'
}

classifier.evaluate(){

    rm -fr /tmp/classifier.y_true /tmp/classifier.y_pred
    ls $summarize_dir/classifier_test/* | while read line
    do
        classifier.transform "$line" >> /tmp/classifier.y_pred
        echo $(basename "$line") | sed 's/\..*//g' >> /tmp/classifier.y_true
    done

    y_pred=$(cat /tmp/classifier.y_pred | sed -e 's/^/"/g' -e 's/$/"/g' | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g')
    y_true=$(cat /tmp/classifier.y_true | sed -e 's/^/"/g' -e 's/$/"/g' | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g')

	python -c "
from sklearn import metrics

y_pred=$y_pred
y_true=$y_true

score = metrics.accuracy_score(y_true, y_pred)
print('accuracy:   %0.3f' % score)

print('classification report:')
print(metrics.classification_report(y_true, y_pred))

print('confusion matrix:')
print(metrics.confusion_matrix(y_true, y_pred))
"

if [ $summarize_evaluate_vects = "true" ]
then
    rm -fr /tmp/vects
    find $summarize_dir/txts/ -type f | while read line
    do
        echo -n $line " " >> /tmp/vects
        $summarize_method.txt2vec "$line" | vect2s >> /tmp/vects
    done
    echo "=====" >> /tmp/vects
    find $summarize_dir/classifier_test/ -type f | while read line
    do
        y_true=$(basename $line | sed 's/\..*//g')
        y_pred=$(classifier.transform $line)
        [ "$y_true" != "$y_pred" ] && echo -n '*** ' >> /tmp/vects
        echo -n T:$y_true P:$y_pred " " >> /tmp/vects
        echo -n $line " " >> /tmp/vects
        $summarize_method.txt2vec "$line" | vect2s >> /tmp/vects
    done
fi


}

regression.fit(){
    rm -fr /tmp/regression.x /tmp/regression.y
    for file in $summarize_dir/hashes/*
    do
        cat $file >> /tmp/regression.x
        cat $summarize_dir/r_output/$(basename $file) >> /tmp/regression.y
    done

    x=$(cat /tmp/regression.x | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g')
    y=$(cat /tmp/regression.y | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g')

	python -c "
import numpy as np
from sklearn import linear_model
from sklearn.externals import joblib
X = np.array($x)
Y = np.array($y)
summarize_dir='$summarize_dir'
reg = linear_model.LinearRegression()
reg.fit (X, Y)
joblib.dump(reg, summarize_dir + '/regression/regression.pkl')
"
}

cluster.fit(){
    n_clusters=3
    seq 0 $n_clusters | nl -v0 | sed 's/\t/ /g' | sed -e 's/ \+/ /g' > $summarize_dir/cluster/map
    rm -fr /tmp/clusters
    for file in $summarize_dir/hashes/*
    do
        cat $file >> /tmp/clusters
    done

    x=$(cat /tmp/clusters | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g')

python -c "
from sklearn.cluster import KMeans
from sklearn.externals import joblib
import numpy as np
X = np.array($x)
summarize_dir='$summarize_dir'
n_clusters=$n_clusters
kmeans = KMeans(n_clusters=n_clusters, random_state=0)
kmeans.fit(X)
joblib.dump(kmeans, summarize_dir + '/cluster/cluster.pkl')
"
}

pca.transform() {
	python -c "
import numpy as np
summarize_dir='$summarize_dir'
pca=np.loadtxt(summarize_dir+'/pca/pca')
eigenv=np.loadtxt(summarize_dir+'/pca/eigenv')
eigenv=[ i + 0.00001 for i in eigenv ]
average=np.loadtxt(summarize_dir+'/pca/average')
normalized=$1;
normalized=np.subtract(normalized, average)
normalized=np.matmul(pca, normalized)
normalized=np.divide( normalized, np.sqrt(eigenv) )
print normalized
" | sed \
    -e 's/ \+/ /g' \
    -e 's/^\[ /[/g' -e 's/ \]$/]/g' -e 's/ /, /g'
}

summarize.doc2txt(){
	filename=$1
	destination=$2

	[ -e "$destination" ] && return
	[ ${filename[1]} = '/' ] || filename="$PWD"/"$filename"
	[ "$destination" = "" ] && destination=/tmp/summary

	rm -fr "$destination"

	[ -d "$filename" ] &&  ( ls "$filename" | while read line; do ( summarize_rec "$filename"/"$line" "$destination" ); done )
	[ -f "$filename" ] &&  summarize_rec "$filename" "$destination"

	[ "$destination" = "/tmp/summary" ] && cat /tmp/summary
}

summarize.hash(){
    rm -fr /tmp/txt
    summarize.doc2txt $1 /tmp/txt

    vector=$($summarize_method.txt2vec /tmp/txt)

    pca.transform $vector

}

summarize.hash2key(){
	python -c "
import numpy as np
normalized=$1
normalized=[ i*3.0 + 0.5 for i in normalized ]
normalized=[ max(min(i,1.0),0.0) for i in normalized ]
chars=\"0123456789\"
hash=[ chars[int((i)*(len(chars)-1))] for i in normalized ]
print ''.join(hash)
"
}

summarize.key(){
    summarize.hash2key "$(summarize.hash $1)"
}

summarize.fit(){
    rm -fr $summarize_dir/txts; mkdir -p $summarize_dir/txts
    ls $summarize_dir/documents/* | while read line
    do
        summarize.doc2txt "$line" $summarize_dir/txts/$(basename "$line")
    done

    [ $summarize_method != keywords ] && ( rm -fr $summarize_dir/features; mkdir -p $summarize_dir/features )
    $summarize_method.get_features

    rm -fr $summarize_dir/vectors; mkdir -p $summarize_dir/vectors
    ls $summarize_dir/txts/* | while read line
    do
        $summarize_method.txt2vec "$line" > $summarize_dir/vectors/$(basename "$line")
    done

    rm -fr $summarize_dir/pca; mkdir -p $summarize_dir/pca
    pca_and_rank.fit

    rm -fr $summarize_dir/hashes; mkdir -p $summarize_dir/hashes
    [ -e $summarize_dir/documents_to_hash ] || cp -r $summarize_dir/documents $summarize_dir/documents_to_hash
    ls $summarize_dir/documents_to_hash/* | while read line
    do
        summarize.hash "$line" > $summarize_dir/hashes/$(basename "$line")
    done

    rm -fr $summarize_dir/classifier; mkdir -p $summarize_dir/classifier
    classifier.fit
    [ -e $summarize_dir/classifier_test ] && classifier.evaluate

    rm -fr $summarize_dir/regression; mkdir -p $summarize_dir/regression
    [ -e $summarize_dir/r_output ] && regression.fit

    rm -fr $summarize_dir/cluster; mkdir -p $summarize_dir/cluster
    cluster.fit
}

classifier.transform(){
x=$(summarize.hash $1)

y=$(python -c "
x=[$x]
summarize_dir='$summarize_dir'
import numpy as np
from sklearn import linear_model
from sklearn.externals import joblib
clf=joblib.load(summarize_dir + '/classifier/classifier.pkl')
print( clf.predict(x)[0] )
")

cat $summarize_dir/classifier/map | grep "^ $y " | awk '{print $2}'
}

cluster.transform(){
x=$(summarize.hash $1)

y=$(python -c "
x=[$x]
summarize_dir='$summarize_dir'
import numpy as np
from sklearn.cluster import KMeans
from sklearn.externals import joblib
kmeans=joblib.load(summarize_dir + '/cluster/cluster.pkl')
print( kmeans.predict(x)[0] )
")

cat $summarize_dir/cluster/map | grep "^ $y " | awk '{print $2}'
}

regression.transform(){
x=$(summarize.hash $1)

python -c "
x=[$x]
summarize_dir='$summarize_dir'
import numpy as np
from sklearn import linear_model
from sklearn.externals import joblib
reg=joblib.load(summarize_dir + '/regression/regression.pkl')
print(reg.predict(x)[0])
"
}

summarize.compare(){

    rm -fr /tmp/t1 /tmp/t2

    summarize.doc2txt $1 /tmp/t1
    summarize.doc2txt $2 /tmp/t2

    h1=$(summarize.hash /tmp/t1)
    h2=$(summarize.hash /tmp/t2)

    python -c "
import numpy as np
import math
h1=$h1
h2=$h2
print math.sqrt(np.sum(np.square(np.subtract(h1, h2))))
"

}

summarize.distance(){
    python -c "
import numpy as np
import math
h1=$1
h2=$2
print math.sqrt(np.sum(np.square(np.subtract(h1, h2))))
"

}

#summarize.closest(){
    #h1=$(summarize.hash $1)
	#for file in $summarize_dir/hashes/*
	#do
        #d=$(summarize.distance "$h1" "$(cat $file)")
		#echo \"$(basename "$file")\" ":" $d ","
	#done | paste -d"  " -s | sed -e 's/^/{/g' -e 's/,$/}/g' | read distances
	#python -c "
#distances=$distances;
#print min(distances, key=distances.get)
#"
#}

summarize.sortdist(){
    h1=$(summarize.hash $1)
	for file in $summarize_dir/hashes/*
	do
        d=$(summarize.distance "$h1" "$(cat $file)")
		echo \"$(basename "$file")\" ":" $d ","
	done | paste -d"  " -s | sed -e 's/^/{/g' -e 's/,$/}/g' | read distances
	python -c "
distances=$distances;
for key, value in sorted(distances.iteritems(), key=lambda (k,v): (v,k)):
  print '%s' % (key)
"
}

summarize.closest(){
    summarize.sortdist $1 | head -n1
}

summarize.knn(){
    summarize.sortdist $1 | head -n $2 | sed 's/\..*//g' | sort | uniq -c | sort -g | tail -n1 | sed 's/^ *[^ ]* //g'
}

