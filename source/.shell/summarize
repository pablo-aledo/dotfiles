learning_method=keywords
learning_evaluate_vects="true"
learning_dir=$HOME/learning
learning_src=/media/removable/2TB2/homes/admin/Alejandria
learning_docs_per_class=5
keywords_normalize="false"
summarize_page_limit=99999
summarize_line_limit=99999
pca_method=pca

ncd.get_features(){
    cp -r $learning_dir/txts/* $learning_dir/features
}

ncd.txt2vec() {
	for file in $learning_dir/features/*
	do
		gzip_distance $1 $file
	done | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g' -e 's/,/, /g'
}

keywords.get_features(){
    [ -e $learning_dir/features/keywords ] && return
    mkdir -p $learning_dir/features
    [ -e ~/Dotfiles/keywords/keywords ] && cp ~/Dotfiles/keywords/keywords $learning_dir/features/keywords
    [ -e ~/Dotfiles/keywords/keywords ] || cat $learning_dir/txts/* | sed 's/ /\n/g' | sort | uniq > $learning_dir/features/keywords
}

tfidf.get_features(){
    mkdir -p $learning_dir/features

    if ! [ -e $learning_dir/features/keywords ]
    then
        [ -e ~/Dotfiles/keywords/keywords ] && cp ~/Dotfiles/keywords/keywords $learning_dir/features/keywords
        [ -e ~/Dotfiles/keywords/keywords ] || cat $learning_dir/txts/* | sed 's/ /\n/g' | sort | uniq > $learning_dir/features/keywords
    fi

    cat $learning_dir/txts/* > /tmp/all_docs
    getwords /tmp/all_docs

    for a in $(cat $learning_dir/features/keywords | grep -v '^#' | grep -v '^$')
    do
         [ $a[0,1] = '^' ] && regex=$(echo $a | sed 's/^.//g' )
         [ $a[0,1] = '^' ] || regex=$a
         value=$( cat /tmp/keywords | egrep '^ *[0-9]* '$regex | awk '{sum += $1} END {print sum}' )
         [ "$value" = "" ] && value=0
         echo $value
    done | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g' -e 's/,/, /g' > $learning_dir/features/idf

}

getwords(){
    cat $1 | sed \
        -e 's/ /\n/g'  \
        -e 's/\./\n/g' \
        -e 's/\t/\n/g' \
        -e 's/-/\n/g'  \
        -e 's/\//\n/g' \
        -e 's/\\/\n/g' \
        -e 's/,/\n/g'  \
        -e 's/;/\n/g'  \
        -e 's/\*/\n/g' \
        -e 's/>/\n/g'  \
        -e 's/</\n/g'  \
        -e 's/_/\n/g'  \
        -e 's/"/\n/g'  \
        -e 's/%/\n/g'  \
        -e 's/&/\n/g'  \
        -e 's/(/\n/g'  \
        -e 's/)/\n/g'  \
        -e 's/\[/\n/g' \
        -e 's/\]/\n/g' \
        -e 's/{/\n/g'  \
        -e 's/}/\n/g'  \
        -e 's/=/\n/g'  \
        -e 's/!/\n/g'  \
        -e 's/@/\n/g'  \
        -e 's/:/\n/g'  \
        -e 's/~/\n/g'  \
        | sort | uniq -c > /tmp/keywords
        # -e 's/\+/\n/g' \
}

keywords.txt2vec(){
    getwords $1

    vect=$(for a in $(cat $learning_dir/features/keywords | grep -v '^#' | grep -v '^$')
    do
         [ $a[0,1] = '^' ] && regex=$(echo $a | sed 's/^.//g' )
         [ $a[0,1] = '^' ] || regex=$a
         value=$( cat /tmp/keywords | egrep '^ *[0-9]* '$regex | awk '{sum += $1} END {print sum}' )
         [ "$value" = "" ] && value=0
         echo $value
         sum_kw=$(( $sum_kw + $value ))
         echo $sum_kw > /tmp/sum_kw
    done | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g' -e 's/,/, /g')

    [ $keywords_normalize = "full" ] && total=$( cat /tmp/keywords | awk '{sum += $1} END {print sum}' )
    [ $keywords_normalize = "keys" ] && total=$(cat /tmp/sum_kw)
    [ $keywords_normalize = "false" ] && total=1

    python -c "
import numpy as np
np.set_printoptions(threshold=np.nan)
vect=$vect
total=$total
print(np.divide(vect, float(total+0.00001)))
" | sed -e 's/ \+/ /g' -e 's/ /, /g' -e 's/\[,/\[/g' -e 's/, \]/\]/g' | paste -s -d' '

}

keywords.doc2vec_debug(){
    learning.doc2txt $1 > /tmp/txt
    getwords /tmp/txt

    for a in $(cat $learning_dir/features/keywords | grep -v '^#' | grep -v '^$')
    do
         [ $a[0,1] = '^' ] && regex=$(echo $a | sed 's/^.//g' )
         [ $a[0,1] = '^' ] || regex=$a
         value=$( cat /tmp/keywords | egrep '^ *[0-9]* '$regex | awk '{sum += $1} END {print sum}' )
         [ "$value" != "" ] && echo $a $value
    done
}

keywords.doc2vec_debug_sorted(){
    learning.doc2txt $1 > /tmp/txt
    getwords /tmp/txt

    for a in $(cat $learning_dir/features/keywords | grep -v '^#' | grep -v '^$')
    do
         [ $a[0,1] = '^' ] && regex=$(echo $a | sed 's/^.//g' )
         [ $a[0,1] = '^' ] || regex=$a
         value=$( cat /tmp/keywords | egrep '^ *[0-9]* '$regex | awk '{sum += $1} END {print sum}' )
         [ "$value" != "" ] && echo $value $a
    done  | sort -g | tac
}

tfidf.txt2vec(){
    getwords $1

    total=$( cat /tmp/keywords | awk '{sum += $1} END {print sum}' )

    y=$(for a in $(cat $learning_dir/features/keywords | grep -v '^#' | grep -v '^$')
    do
         [ $a[0,1] = '^' ] && regex=$(echo $a | sed 's/^.//g' )
         [ $a[0,1] = '^' ] || regex=$a
         value=$( cat /tmp/keywords | egrep '^ *[0-9]* '$regex | awk '{sum += $1} END {print sum}' )
         [ "$value" = "" ] && value=0
         [ $keywords_normalize = "true" ] && echo "print($value/$total.)" | python
         [ $keywords_normalize = "true" ] || echo $value
     done | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g' -e 's/,/, /g')

    idf=$(cat $learning_dir/features/idf)

python -c "
import numpy as np
np.set_printoptions(threshold=np.nan)
y=$y
idf=$idf
idf=[ i+0.00001 for i in idf ]
print( np.divide(y, idf) )
" | sed -e 's/ \+/ /g' -e 's/ /, /g' -e 's/\[,/\[/g' | paste -s

}

doc2txt_rec(){
	filename="$1"
	destination="$2"

	case $filename in
		(*.rar|*.RAR)
			rm -fr /tmp/learning
			mkdir -p /tmp/learning
			cd /tmp/learning
			unrar x "$filename" >/dev/null 2>/dev/null
			find $PWD | while read line
			do
				doc2txt_rec "$line" "$destination" 
			done
			[ $PWD = $OLDPWD ] || cd $OLDPWD
			;;
		(*.zip|*.ZIP)
			rm -fr /tmp/learning
			mkdir -p /tmp/learning
			cd /tmp/learning
			unzip "$filename" >/dev/null 2>/dev/null
			find $PWD | while read line
			do
				doc2txt_rec "$line" "$destination" 
			done
			[ $PWD = $OLDPWD ] || cd $OLDPWD
			;;
		(*.pdf|*.PDF)
			pdftotext -f 1 -l $summarize_page_limit "$filename" - >> "$destination"
			;;
		(*.epub|*.EPUB)
			epub2txt "$filename" | head -n $summarize_line_limit >> "$destination"
			;;
		(*.mobi|*.MOBI)
			ebook-convert "$filename" /tmp/mobi.txt
			cat /tmp/mobi.txt | head -n $summarize_line_limit >> "$destination"
			;;
		(*.txt|*.TXT)
			cat "$filename" | head -n $summarize_line_limit >> "$destination"
			;;
		(*)
			file "$filename" | grep 'text' >/dev/null && cat "$filename" >> "$destination"
			file "$filename" | grep 'text' >/dev/null || echo "$filename" >> "$destination"
	esac
}

gzip_distance(){
    rm -fr /tmp/s1 /tmp/s2
    learning.doc2txt "$1" /tmp/s1
    learning.doc2txt "$2" /tmp/s2
    s1=$( cat /tmp/s1 | gzip -f | wc -c )
    s2=$( cat /tmp/s2 | gzip -f | wc -c )
    s12=$( cat /tmp/s1 /tmp/s2 | gzip | wc -c )

   python -c "print float($s12 - min($s1, $s2))/float(max($s1,$s2))"
}

pca.fit(){
    rm -fr /tmp/softhashes
    for file in $learning_dir/vectors/*
    do
        cat $file >> /tmp/softhashes
    done
    sed -i -e 's/\[//g' -e 's/\]//g' -e 's/,//g' /tmp/softhashes
	python -c "
import numpy as np
from sklearn.decomposition import PCA
learning_dir='$learning_dir'
X=np.loadtxt('/tmp/softhashes')
pca=PCA()
pca.fit(X)
np.savetxt(learning_dir + '/pca/average',np.mean(X,0))
np.savetxt(learning_dir + '/pca/pca',pca.components_)
np.savetxt(learning_dir + '/pca/eigenv',pca.explained_variance_)
"
}

pca_pickle.fit(){
    rm -fr /tmp/softhashes
    for file in $learning_dir/vectors/*
    do
        cat $file >> /tmp/softhashes
    done
    sed -i -e 's/\[//g' -e 's/\]//g' -e 's/,//g' /tmp/softhashes
	python -c "
import numpy as np
from sklearn.decomposition import PCA
from sklearn.externals import joblib
learning_dir='$learning_dir'
X=np.loadtxt('/tmp/softhashes')
pca=PCA()
pca.fit(X)
joblib.dump(pca, learning_dir + '/pca/pca.pkl')
"
}

pca_scaler.fit(){
    rm -fr /tmp/softhashes
    for file in $learning_dir/vectors/*
    do
        cat $file >> /tmp/softhashes
    done
    x=$( cat /tmp/softhashes | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g')

	python -c "
import numpy as np
from sklearn.decomposition import PCA
from sklearn.externals import joblib
from sklearn.preprocessing import StandardScaler
learning_dir='$learning_dir'
X=$x
scaler = StandardScaler()
scaler.fit(X)
X=scaler.transform(X)
pca=PCA()
pca.fit(X)
joblib.dump(pca, learning_dir + '/pca/pca.pkl')
joblib.dump(scaler, learning_dir + '/pca/scaler.pkl')
"
}

lda.fit(){

    rm -fr /tmp/lda.x /tmp/lda.y
    ls $learning_dir/vectors/ | sed 's/\..*//g' | sort | uniq | nl - | sed 's/\t/ /g' | sed -e 's/ \+/ /g' > $learning_dir/pca/map

    for file in $learning_dir/vectors/*
    do
        cat $file >> /tmp/lda.x
        y=$(basename $file | sed 's/\..*//g')
        id=$(cat $learning_dir/pca/map | grep " $y$" | awk '{print $1}')
        echo $id >> /tmp/lda.y
    done

    x=$(cat /tmp/lda.x | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g')
    y=$(cat /tmp/lda.y | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g')

	python -c "
import numpy as np
np.set_printoptions(threshold=np.nan)
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.externals import joblib
X = np.array($x)
Y = np.array($y)
learning_dir='$learning_dir'
clf = LinearDiscriminantAnalysis()
clf.fit(X, Y)
joblib.dump(clf, learning_dir + '/pca/lda.pkl')
"
}

pca.rank(){

    doc_2_vect=$(for file in $learning_dir/vectors/*
    do
        echo "'"$(basename $file)"'":$(cat $file),
    done | paste -s -d' ' | sed -e 's/^/{/g' -e 's/$/}/g' -e 's/,}/}/g')

	python -c "
import numpy as np
from sklearn.decomposition import PCA
n_components=3
doc_2_vect=$doc_2_vect
doc_2_rank={k:99999 for k in doc_2_vect}
pca=PCA(n_components=n_components)
original_pca=PCA(n_components=n_components)
original_pca.fit(doc_2_vect.values())

def cost(a,b):
    return np.sum( np.absolute( np.subtract(a.components_, b.components_) ) )

for round in range(1,len(doc_2_vect)-(n_components-1)):
    doc_2_cost={k:99999 for k in doc_2_vect}
    for item in doc_2_vect.keys():
        if(doc_2_rank[item] < round):
            continue
        doc_2_rank[item] = round
        selection=[ v for (k,v) in doc_2_vect.items() if doc_2_rank[k] > round ]
        pca.fit(selection)
        doc_2_cost[item] = cost(pca, original_pca)
        doc_2_rank[item] = 99999
    min_doc=min(doc_2_cost, key=doc_2_cost.get)
    doc_2_rank[min_doc] = round

#print doc_2_rank

for a in sorted(doc_2_rank, key=doc_2_rank.get):
    print a
"
}

classifier.fit(){

    rm -fr /tmp/classifier.x /tmp/classifier.y
    ls $learning_dir/vectors_reduced/ | sed 's/\..*//g' | sort | uniq | nl - | sed 's/\t/ /g' | sed -e 's/ \+/ /g' > $learning_dir/classifier/map
    for file in $learning_dir/vectors_reduced/*
    do
        cat $file >> /tmp/classifier.x
        y=$(basename $file | sed 's/\..*//g')
        id=$(cat $learning_dir/classifier/map | grep " $y$" | awk '{print $1}')
        echo $id >> /tmp/classifier.y
    done

    x=$(cat /tmp/classifier.x | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g')
    y=$(cat /tmp/classifier.y | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g')

	python -c "
import numpy as np
from sklearn import linear_model
from sklearn.externals import joblib
X = np.array($x)
Y = np.array($y)
learning_dir='$learning_dir'
clf = linear_model.SGDClassifier()
clf.fit(X, Y)
joblib.dump(clf, learning_dir + '/classifier/classifier.pkl')
"
}

vect2s(){
    read vect
    learning.hash2key "$( python -c "
vect=$vect
print([ (i/float(max(vect)+0.00001) - 0.5)/3.0 for i in vect ])
")" | sed -e 's/0/./g' -e 's/^/[/g' -e 's/$/]/g'
}

classifier.evaluate(){

    rm -fr /tmp/classifier.y_true /tmp/classifier.y_pred
    ls $learning_dir/classifier_test/* | while read line
    do
        classifier.transform "$line" >> /tmp/classifier.y_pred
        echo $(basename "$line") | sed 's/\..*//g' >> /tmp/classifier.y_true
    done

    y_pred=$(cat /tmp/classifier.y_pred | sed -e 's/^/"/g' -e 's/$/"/g' | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g')
    y_true=$(cat /tmp/classifier.y_true | sed -e 's/^/"/g' -e 's/$/"/g' | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g')

	python -c "
from sklearn import metrics

y_pred=$y_pred
y_true=$y_true

score = metrics.accuracy_score(y_true, y_pred)
print('accuracy:   %0.3f' % score)

print('classification report:')
print(metrics.classification_report(y_true, y_pred))

print('confusion matrix:')
print(metrics.confusion_matrix(y_true, y_pred))
"

if [ $learning_evaluate_vects = "true" ]
then
    echo "\e[34m vectors \e[0m "
    rm -fr /tmp/vects
    find $learning_dir/classifier_train/ -type f | while read line
    do
        echo -n $line " " >> /tmp/vects
        $learning_method.txt2vec "$line" | vect2s >> /tmp/vects
    done
    echo "=====" >> /tmp/vects
    find $learning_dir/classifier_test/ -type f | while read line
    do
        y_true=$(basename $line | sed 's/\..*//g')
        y_pred=$(classifier.transform $line)
        [ "$y_true" != "$y_pred" ] && echo -n '*** ' >> /tmp/vects
        echo -n T:$y_true P:$y_pred " " >> /tmp/vects
        echo -n $line " " >> /tmp/vects
        $learning_method.txt2vec "$line" | vect2s >> /tmp/vects
    done
fi


}

regression.fit(){
    rm -fr /tmp/regression.x /tmp/regression.y
    for file in $learning_dir/vectors_reduced/*
    do
        cat $file >> /tmp/regression.x
        cat $learning_dir/regression_fit/$(basename $file) >> /tmp/regression.y
    done

    x=$(cat /tmp/regression.x | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g')
    y=$(cat /tmp/regression.y | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g')

	python -c "
import numpy as np
from sklearn import linear_model
from sklearn.externals import joblib
X = np.array($x)
Y = np.array($y)
learning_dir='$learning_dir'
reg = linear_model.LinearRegression()
reg.fit (X, Y)
joblib.dump(reg, learning_dir + '/regression/regression.pkl')
"
}

cluster.fit(){
    n_clusters=3
    seq 0 $n_clusters | nl -v0 | sed 's/\t/ /g' | sed -e 's/ \+/ /g' > $learning_dir/cluster/map
    rm -fr /tmp/clusters
    for file in $learning_dir/vectors_reduced/*
    do
        cat $file >> /tmp/clusters
    done

    x=$(cat /tmp/clusters | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g')

python -c "
from sklearn.cluster import KMeans
from sklearn.externals import joblib
import numpy as np
X = np.array($x)
learning_dir='$learning_dir'
n_clusters=$n_clusters
kmeans = KMeans(n_clusters=n_clusters, random_state=0)
kmeans.fit(X)
joblib.dump(kmeans, learning_dir + '/cluster/cluster.pkl')
"
}

pca.transform() {
	python -c "
import numpy as np
learning_dir='$learning_dir'
pca=np.loadtxt(learning_dir+'/pca/pca')
eigenv=np.loadtxt(learning_dir+'/pca/eigenv')
eigenv=[ i + 0.00001 for i in eigenv ]
average=np.loadtxt(learning_dir+'/pca/average')
normalized=$1;
normalized=np.subtract(normalized, average)
normalized=np.matmul(pca, normalized)
normalized=np.divide( normalized, np.sqrt(eigenv) )
print normalized
" | sed \
    -e 's/ \+/ /g' \
    -e 's/^\[ /[/g' -e 's/ \]$/]/g' -e 's/ /, /g' | paste -d' ' -s
}

pca_pickle.transform() {
	python -c "
import numpy as np
np.set_printoptions(threshold=np.nan)
from sklearn.externals import joblib
learning_dir='$learning_dir'
pca=joblib.load(learning_dir + '/pca/pca.pkl')
x=[$1];
print pca.transform(x)[0]
" | sed \
    -e 's/ \+/ /g' \
    -e 's/^\[ /[/g' -e 's/ \]$/]/g' -e 's/ /, /g'
}

pca_scaler.transform() {
	python -c "
import numpy as np
np.set_printoptions(threshold=np.nan)
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.externals import joblib
learning_dir='$learning_dir'
pca=joblib.load(learning_dir + '/pca/pca.pkl')
scaler=joblib.load(learning_dir + '/pca/scaler.pkl')
x=[$1]
x=scaler.transform(x)
print pca.transform(x)[0]
" | sed \
    -e 's/ \+/ /g' \
    -e 's/^\[ /[/g' -e 's/ \]$/]/g' -e 's/ /, /g' | paste -s
}

lda.transform() {
	python -c "
import numpy as np
np.set_printoptions(threshold=np.nan)
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.externals import joblib
learning_dir='$learning_dir'
clf=joblib.load(learning_dir + '/pca/lda.pkl')
x=[$1];
print clf.transform(x)[0]
" | sed \
    -e 's/ \+/ /g' \
    -e 's/^\[ /[/g' -e 's/ \]$/]/g' -e 's/ /, /g'
}

learning.doc2txt(){
	filename=$1
	destination=$2

	[ -e "$destination" ] && return
	[ ${filename[1]} = '/' ] || filename="$PWD"/"$filename"
	[ "$destination" = "" ] && destination=/tmp/summary

	rm -fr "$destination"

	[ -d "$filename" ] &&  ( ls "$filename" | while read line; do ( doc2txt_rec "$filename"/"$line" "$destination" ); done )
	[ -f "$filename" ] &&  doc2txt_rec "$filename" "$destination"

	[ "$destination" = "/tmp/summary" ] && cat /tmp/summary
}

learning.hash(){
    rm -fr /tmp/txt
    learning.doc2txt $1 /tmp/txt

    vector=$($learning_method.txt2vec /tmp/txt)

    $pca_method.transform $vector

}

learning.hash2key(){
	python -c "
import numpy as np
normalized=$1
normalized=[ i*3.0 + 0.5 for i in normalized ]
normalized=[ max(min(i,1.0),0.0) for i in normalized ]
chars=\"0123456789\"
hash=[ chars[int((i)*(len(chars)-1))] for i in normalized ]
print ''.join(hash)
"
}

learning.key(){
    learning.hash2key "$(learning.hash $1)"
}

learning.fit(){

    [ -e $learning_dir/documents ] || mkdir -p $learning_dir/documents
    [ -e $learning_dir/topics ] && [ -e $learning_src ] && learning.get_documents

    rm -fr $learning_dir/txts; mkdir -p $learning_dir/txts
    ls $learning_dir/documents/* | while read line
    do
        echo "\e[34m doc2txt \e[0m " $line
        learning.doc2txt "$line" $learning_dir/txts/$(basename "$line")
    done

    [ $learning_method != keywords ] && ( rm -fr $learning_dir/features; mkdir -p $learning_dir/features )
    echo "\e[34m get_features \e[0m "
    $learning_method.get_features

    rm -fr $learning_dir/vectors; mkdir -p $learning_dir/vectors
    ls $learning_dir/txts/* | while read line
    do
        echo "\e[34m txt2vec \e[0m " $line
        $learning_method.txt2vec "$line" > $learning_dir/vectors/$(basename "$line")
    done

    rm -fr $learning_dir/pca; mkdir -p $learning_dir/pca
    echo "\e[34m pca \e[0m "
    $pca_method.fit

    rm -fr $learning_dir/vectors_reduced; mkdir -p $learning_dir/vectors_reduced
    [ -e $learning_dir/classifier_train ] || cp -r $learning_dir/documents $learning_dir/classifier_train
    ls $learning_dir/classifier_train/* | while read line
    do
        echo "\e[34m hash \e[0m " $line
        learning.hash "$line" > $learning_dir/vectors_reduced/$(basename "$line")
    done

    rm -fr $learning_dir/classifier; mkdir -p $learning_dir/classifier
    echo "\e[34m classifier \e[0m "
    classifier.fit
    [ -e $learning_dir/classifier_test ] && classifier.evaluate

    rm -fr $learning_dir/regression; mkdir -p $learning_dir/regression
    echo "\e[34m regression \e[0m "
    [ -e $learning_dir/regression_fit ] && regression.fit

    rm -fr $learning_dir/cluster; mkdir -p $learning_dir/cluster
    echo "\e[34m cluster \e[0m "
    cluster.fit
}

classifier.transform(){
x=$(learning.hash $1)

y=$(python -c "
x=[$x]
learning_dir='$learning_dir'
import numpy as np
from sklearn import linear_model
from sklearn.externals import joblib
clf=joblib.load(learning_dir + '/classifier/classifier.pkl')
print( clf.predict(x)[0] )
")

cat $learning_dir/classifier/map | grep "^ $y " | awk '{print $2}'
}

cluster.transform(){
x=$(learning.hash $1)

y=$(python -c "
x=[$x]
learning_dir='$learning_dir'
import numpy as np
from sklearn.cluster import KMeans
from sklearn.externals import joblib
kmeans=joblib.load(learning_dir + '/cluster/cluster.pkl')
print( kmeans.predict(x)[0] )
")

cat $learning_dir/cluster/map | grep "^ $y " | awk '{print $2}'
}

regression.transform(){
x=$(learning.hash $1)

python -c "
x=[$x]
learning_dir='$learning_dir'
import numpy as np
np.set_printoptions(threshold=np.nan)
from sklearn import linear_model
from sklearn.externals import joblib
reg=joblib.load(learning_dir + '/regression/regression.pkl')
print(reg.predict(x)[0])
"
}

learning.compare(){

    rm -fr /tmp/t1 /tmp/t2

    learning.doc2txt $1 /tmp/t1
    learning.doc2txt $2 /tmp/t2

    h1=$(learning.hash /tmp/t1)
    h2=$(learning.hash /tmp/t2)

    python -c "
import numpy as np
import math
h1=$h1
h2=$h2
print math.sqrt(np.sum(np.square(np.subtract(h1, h2))))
"

}

learning.distance(){
    python -c "
import numpy as np
import math
h1=$1
h2=$2
print math.sqrt(np.sum(np.square(np.subtract(h1, h2))))
"

}

#learning.closest(){
    #h1=$(learning.hash $1)
	#for file in $learning_dir/vectors_reduced/*
	#do
        #d=$(learning.distance "$h1" "$(cat $file)")
		#echo \"$(basename "$file")\" ":" $d ","
	#done | paste -d"  " -s | sed -e 's/^/{/g' -e 's/,$/}/g' | read distances
	#python -c "
#distances=$distances;
#print min(distances, key=distances.get)
#"
#}

learning.sortdist(){
    h1=$(learning.hash $1)
	for file in $learning_dir/vectors_reduced/*
	do
        d=$(learning.distance "$h1" "$(cat $file)")
		echo \"$(basename "$file")\" ":" $d ","
	done | paste -d"  " -s | sed -e 's/^/{/g' -e 's/,$/}/g' | read distances
	python -c "
distances=$distances;
for key, value in sorted(distances.iteritems(), key=lambda (k,v): (v,k)):
  print '%s' % (key)
"
}

learning.closest(){
    learning.sortdist $1 | head -n1
}

learning.knn(){
    learning.sortdist $1 | head -n $2 | sed 's/\..*//g' | sort | uniq -c | sort -g | tail -n1 | sed 's/^ *[^ ]* //g'
}

keywords_filter () {
	cat $1 | while read line
	do
		found=0
		echo $line > /tmp/line
		getwords /tmp/line
		for a in $(cat ~/learning/features/keywords | grep -v '^#' | grep -v '^$')
		do
			[ $a[0,1] = '^' ] && regex=$(echo $a | sed 's/^.//g' )
			[ $a[0,1] = '^' ] || regex=$a
			value=$( cat /tmp/keywords | egrep '^ *[0-9]* '$regex | awk '{sum += $1} END {print sum}' )
			[ "$value" != "" ] && found=1  && break
		done
		[ $found -eq 1 ] && echo $line
	done
}

learning.get_documents(){

    inipwd=$PWD

    cp /tmp/topics /tmp/topics_to_get
    [ -e $learning_dir/skip ] && comm -13 <(cat $learning_dir/skip | sort | uniq) <(cat $learning_dir/topics | sort | uniq) > /tmp/topics_to_get

    for topic in $( cat /tmp/topics_to_get )
    do
        echo "\e[34m ===== $topic ===== \e[0m"

        if [ $(ls $learning_dir/documents/ | egrep "^$topic\..*" | wc -l) -ge $learning_docs_per_class ]
        then
            echo "\e[34m enough documents \e[0m $topic"
            continue
        fi

        if [ ! -e $learning_src/$topic ]
        then
            echo "\e[31m topic not found \e[0m $topic"
            continue
        fi

        if [ $(find $learning_src/$topic -maxdepth 1 -type f | wc -l) -lt $learning_docs_per_class ]
        then
            echo "\e[33m not enough documents \e[0m $topic"
        fi

        cd "$learning_src/$topic"
        find -maxdepth 1 -type f | sort -R | head -n $learning_docs_per_class | while read file
        do
            crctitle=$(echo "$file" | md5sum | cut -d' ' -f1)
            echo "\e[34m doc2txt \e[0m $file $topic.$crctitle"
            learning.doc2txt "$file" "$learning_dir/documents/$topic.$crctitle"
        done
        cd $inipwd
    done

    find $learning_dir/documents/ -type f | while read line
    do
        if [ "$( file $line | grep text )" = "" ]
        then
            echo "\e[31m $line is not text \e[0m"
            rm -rf $line
            continue
        fi

        if [ "$( du -bc $line | head -n1 | awk '{print $1}' )" -lt 1000 ]
        then
            echo "\e[31m $line is very small \e[0m"
            rm -rf $line
            continue
        fi


    done
}
