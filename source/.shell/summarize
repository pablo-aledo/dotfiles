summarize_method=keywords
summarize_evaluate_vects="true"
summarize_dir=$HOME/summarize
keywords_normalize="false"
summarize_page_limit=99999
summarize_line_limit=99999
pca_method=pca_scaler

ncd.get_features(){
    cp -r $summarize_dir/txts/* $summarize_dir/features
}

ncd.txt2vec() {
	for file in $summarize_dir/features/*
	do
		gzip_distance $1 $file
	done | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g' -e 's/,/, /g'
}

keywords.get_features(){
    [ -e $summarize_dir/features/keywords ] && return
    mkdir -p $summarize_dir/features
    [ -e ~/Dotfiles/keywords/keywords ] && cp ~/Dotfiles/keywords/keywords $summarize_dir/features/keywords
    [ -e ~/Dotfiles/keywords/keywords ] || cat $summarize_dir/txts/* | sed 's/ /\n/g' | sort | uniq > $summarize_dir/features/keywords
}

tfidf.get_features(){
    mkdir -p $summarize_dir/features

    if ! [ -e $summarize_dir/features/keywords ]
    then
        [ -e ~/Dotfiles/keywords/keywords ] && cp ~/Dotfiles/keywords/keywords $summarize_dir/features/keywords
        [ -e ~/Dotfiles/keywords/keywords ] || cat $summarize_dir/txts/* | sed 's/ /\n/g' | sort | uniq > $summarize_dir/features/keywords
    fi

    cat $summarize_dir/txts/* > /tmp/all_docs
    getwords /tmp/all_docs

    for a in $(cat $summarize_dir/features/keywords | grep -v '^#' | grep -v '^$')
    do
         [ $a[0,1] = '^' ] && regex=$(echo $a | sed 's/^.//g' )
         [ $a[0,1] = '^' ] || regex=$a
         value=$( cat /tmp/keywords | egrep '^ *[0-9]* '$regex | awk '{sum += $1} END {print sum}' )
         [ "$value" = "" ] && value=0
         echo $value
    done | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g' -e 's/,/, /g' > $summarize_dir/features/idf

}

getwords(){
    cat $1 | sed \
        -e 's/ /\n/g'  \
        -e 's/\./\n/g' \
        -e 's/\t/\n/g' \
        -e 's/-/\n/g'  \
        -e 's/\//\n/g' \
        -e 's/\\/\n/g' \
        -e 's/,/\n/g'  \
        -e 's/;/\n/g'  \
        -e 's/\+/\n/g' \
        -e 's/\*/\n/g' \
        -e 's/>/\n/g'  \
        -e 's/</\n/g'  \
        -e 's/_/\n/g'  \
        -e 's/"/\n/g'  \
        -e 's/%/\n/g'  \
        -e 's/&/\n/g'  \
        -e 's/(/\n/g'  \
        -e 's/)/\n/g'  \
        -e 's/\[/\n/g' \
        -e 's/\]/\n/g' \
        -e 's/{/\n/g'  \
        -e 's/}/\n/g'  \
        -e 's/=/\n/g'  \
        -e 's/!/\n/g'  \
        -e 's/@/\n/g'  \
        -e 's/:/\n/g'  \
        -e 's/~/\n/g'  \
        | sort | uniq -c > /tmp/keywords
}

keywords.txt2vec(){
    getwords $1

    total=$( cat /tmp/keywords | awk '{sum += $1} END {print sum}' )

    for a in $(cat $summarize_dir/features/keywords | grep -v '^#' | grep -v '^$')
    do
         [ $a[0,1] = '^' ] && regex=$(echo $a | sed 's/^.//g' )
         [ $a[0,1] = '^' ] || regex=$a
         value=$( cat /tmp/keywords | egrep '^ *[0-9]* '$regex | awk '{sum += $1} END {print sum}' )
         [ "$value" = "" ] && value=0
         [ $keywords_normalize = "true" ] && echo "print($value/$total.)" | python
         [ $keywords_normalize = "true" ] || echo $value
    done | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g' -e 's/,/, /g'
}

keywords.doc2vec_debug(){
    summarize.doc2txt $1 > /tmp/txt
    getwords /tmp/txt

    for a in $(cat $summarize_dir/features/keywords | grep -v '^#' | grep -v '^$')
    do
         [ $a[0,1] = '^' ] && regex=$(echo $a | sed 's/^.//g' )
         [ $a[0,1] = '^' ] || regex=$a
         value=$( cat /tmp/keywords | egrep '^ *[0-9]* '$regex | awk '{sum += $1} END {print sum}' )
         [ "$value" != "" ] && echo $a $value
    done
}

tfidf.txt2vec(){
    getwords $1

    total=$( cat /tmp/keywords | awk '{sum += $1} END {print sum}' )

    y=$(for a in $(cat $summarize_dir/features/keywords | grep -v '^#' | grep -v '^$')
    do
         [ $a[0,1] = '^' ] && regex=$(echo $a | sed 's/^.//g' )
         [ $a[0,1] = '^' ] || regex=$a
         value=$( cat /tmp/keywords | egrep '^ *[0-9]* '$regex | awk '{sum += $1} END {print sum}' )
         [ "$value" = "" ] && value=0
         [ $keywords_normalize = "true" ] && echo "print($value/$total.)" | python
         [ $keywords_normalize = "true" ] || echo $value
     done | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g' -e 's/,/, /g')

    idf=$(cat $summarize_dir/features/idf)

python -c "
import numpy as np
y=$y
idf=$idf
idf=[ i+0.00001 for i in idf ]
print( np.divide(y, idf) )
" | sed -e 's/ \+/ /g' -e 's/ /, /g' -e 's/\[,/\[/g' | paste -s

}

summarize_rec(){
	filename="$1"
	destination="$2"

	case $filename in
		(*.rar|*.RAR)
			rm -fr /tmp/summarize
			mkdir -p /tmp/summarize
			cd /tmp/summarize
			unrar x "$filename" >/dev/null 2>/dev/null
			find $PWD | while read line
			do
				summarize_rec "$line" "$destination" 
			done
			[ $PWD = $OLDPWD ] || cd $OLDPWD
			;;
		(*.zip|*.ZIP)
			rm -fr /tmp/summarize
			mkdir -p /tmp/summarize
			cd /tmp/summarize
			unzip "$filename" >/dev/null 2>/dev/null
			find $PWD | while read line
			do
				summarize_rec "$line" "$destination" 
			done
			[ $PWD = $OLDPWD ] || cd $OLDPWD
			;;
		(*.pdf|*.PDF)
			pdftotext -f 1 -l $summarize_page_limit "$filename" - >> "$destination"
			;;
		(*.epub|*.EPUB)
			epub2txt "$filename" | head -n $summarize_line_limit >> "$destination"
			;;
		(*.mobi|*.MOBI)
			ebook-convert "$filename" /tmp/mobi.txt
			cat /tmp/mobi.txt >> "$destination"
			;;
		(*.txt|*.TXT)
			cat "$filename" | head -n $summarize_line_limit >> "$destination"
			;;
		(*)
			file "$filename" | grep 'text' >/dev/null && cat "$filename" >> "$destination"
			file "$filename" | grep 'text' >/dev/null || echo "$filename" >> "$destination"
	esac
}

gzip_distance(){
    rm -fr /tmp/s1 /tmp/s2
    summarize.doc2txt "$1" /tmp/s1
    summarize.doc2txt "$2" /tmp/s2
    s1=$( cat /tmp/s1 | gzip -f | wc -c )
    s2=$( cat /tmp/s2 | gzip -f | wc -c )
    s12=$( cat /tmp/s1 /tmp/s2 | gzip | wc -c )

   python -c "print float($s12 - min($s1, $s2))/float(max($s1,$s2))"
}

pca.fit(){
    rm -fr /tmp/softhashes
    for file in $summarize_dir/vectors/*
    do
        cat $file >> /tmp/softhashes
    done
    sed -i -e 's/\[//g' -e 's/\]//g' -e 's/,//g' /tmp/softhashes
	python -c "
import numpy as np
from sklearn.decomposition import PCA
summarize_dir='$summarize_dir'
X=np.loadtxt('/tmp/softhashes')
pca=PCA()
pca.fit(X)
np.savetxt(summarize_dir + '/pca/average',np.mean(X,0))
np.savetxt(summarize_dir + '/pca/pca',pca.components_)
np.savetxt(summarize_dir + '/pca/eigenv',pca.explained_variance_)
"
}

pca_pickle.fit(){
    rm -fr /tmp/softhashes
    for file in $summarize_dir/vectors/*
    do
        cat $file >> /tmp/softhashes
    done
    sed -i -e 's/\[//g' -e 's/\]//g' -e 's/,//g' /tmp/softhashes
	python -c "
import numpy as np
from sklearn.decomposition import PCA
from sklearn.externals import joblib
summarize_dir='$summarize_dir'
X=np.loadtxt('/tmp/softhashes')
pca=PCA()
pca.fit(X)
joblib.dump(pca, summarize_dir + '/pca/pca.pkl')
"
}

pca_scaler.fit(){
    rm -fr /tmp/softhashes
    for file in $summarize_dir/vectors/*
    do
        cat $file >> /tmp/softhashes
    done
    x=$( cat /tmp/softhashes | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g')

	python -c "
import numpy as np
from sklearn.decomposition import PCA
from sklearn.externals import joblib
from sklearn.preprocessing import StandardScaler
summarize_dir='$summarize_dir'
X=$x
scaler = StandardScaler()
scaler.fit(X)
X=scaler.transform(X)
pca=PCA()
pca.fit(X)
joblib.dump(pca, summarize_dir + '/pca/pca.pkl')
joblib.dump(scaler, summarize_dir + '/pca/scaler.pkl')
"
}

lda.fit(){

    rm -fr /tmp/lda.x /tmp/lda.y
    ls $summarize_dir/vectors/ | sed 's/\..*//g' | sort | uniq | nl - | sed 's/\t/ /g' | sed -e 's/ \+/ /g' > $summarize_dir/pca/map

    for file in $summarize_dir/vectors/*
    do
        cat $file >> /tmp/lda.x
        y=$(basename $file | sed 's/\..*//g')
        id=$(cat $summarize_dir/pca/map | grep " $y$" | awk '{print $1}')
        echo $id >> /tmp/lda.y
    done

    x=$(cat /tmp/lda.x | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g')
    y=$(cat /tmp/lda.y | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g')

	python -c "
import numpy as np
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.externals import joblib
X = np.array($x)
Y = np.array($y)
summarize_dir='$summarize_dir'
clf = LinearDiscriminantAnalysis()
clf.fit(X, Y)
joblib.dump(clf, summarize_dir + '/pca/lda.pkl')
"
}

classifier.fit(){

    rm -fr /tmp/classifier.x /tmp/classifier.y
    ls $summarize_dir/hashes/ | sed 's/\..*//g' | sort | uniq | nl - | sed 's/\t/ /g' | sed -e 's/ \+/ /g' > $summarize_dir/classifier/map
    for file in $summarize_dir/hashes/*
    do
        cat $file >> /tmp/classifier.x
        y=$(basename $file | sed 's/\..*//g')
        id=$(cat $summarize_dir/classifier/map | grep " $y$" | awk '{print $1}')
        echo $id >> /tmp/classifier.y
    done

    x=$(cat /tmp/classifier.x | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g')
    y=$(cat /tmp/classifier.y | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g')

	python -c "
import numpy as np
from sklearn import linear_model
from sklearn.externals import joblib
X = np.array($x)
Y = np.array($y)
summarize_dir='$summarize_dir'
clf = linear_model.SGDClassifier()
clf.fit(X, Y)
joblib.dump(clf, summarize_dir + '/classifier/classifier.pkl')
"
}

vect2s(){
    read vect
    summarize.hash2key "$( python -c "
vect=$vect
print([ (i/float(max(vect)+0.00001) - 0.5)/3.0 for i in vect ])
")" | sed -e 's/0/./g' -e 's/^/[/g' -e 's/$/]/g'
}

classifier.evaluate(){

    rm -fr /tmp/classifier.y_true /tmp/classifier.y_pred
    ls $summarize_dir/classifier_test/* | while read line
    do
        classifier.transform "$line" >> /tmp/classifier.y_pred
        echo $(basename "$line") | sed 's/\..*//g' >> /tmp/classifier.y_true
    done

    y_pred=$(cat /tmp/classifier.y_pred | sed -e 's/^/"/g' -e 's/$/"/g' | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g')
    y_true=$(cat /tmp/classifier.y_true | sed -e 's/^/"/g' -e 's/$/"/g' | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g')

	python -c "
from sklearn import metrics

y_pred=$y_pred
y_true=$y_true

score = metrics.accuracy_score(y_true, y_pred)
print('accuracy:   %0.3f' % score)

print('classification report:')
print(metrics.classification_report(y_true, y_pred))

print('confusion matrix:')
print(metrics.confusion_matrix(y_true, y_pred))
"

if [ $summarize_evaluate_vects = "true" ]
then
    rm -fr /tmp/vects
    find $summarize_dir/txts/ -type f | while read line
    do
        echo -n $line " " >> /tmp/vects
        $summarize_method.txt2vec "$line" | vect2s >> /tmp/vects
    done
    echo "=====" >> /tmp/vects
    find $summarize_dir/classifier_test/ -type f | while read line
    do
        y_true=$(basename $line | sed 's/\..*//g')
        y_pred=$(classifier.transform $line)
        [ "$y_true" != "$y_pred" ] && echo -n '*** ' >> /tmp/vects
        echo -n T:$y_true P:$y_pred " " >> /tmp/vects
        echo -n $line " " >> /tmp/vects
        $summarize_method.txt2vec "$line" | vect2s >> /tmp/vects
    done
fi


}

regression.fit(){
    rm -fr /tmp/regression.x /tmp/regression.y
    for file in $summarize_dir/hashes/*
    do
        cat $file >> /tmp/regression.x
        cat $summarize_dir/r_output/$(basename $file) >> /tmp/regression.y
    done

    x=$(cat /tmp/regression.x | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g')
    y=$(cat /tmp/regression.y | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g')

	python -c "
import numpy as np
from sklearn import linear_model
from sklearn.externals import joblib
X = np.array($x)
Y = np.array($y)
summarize_dir='$summarize_dir'
reg = linear_model.LinearRegression()
reg.fit (X, Y)
joblib.dump(reg, summarize_dir + '/regression/regression.pkl')
"
}

cluster.fit(){
    n_clusters=3
    seq 0 $n_clusters | nl -v0 | sed 's/\t/ /g' | sed -e 's/ \+/ /g' > $summarize_dir/cluster/map
    rm -fr /tmp/clusters
    for file in $summarize_dir/hashes/*
    do
        cat $file >> /tmp/clusters
    done

    x=$(cat /tmp/clusters | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g')

python -c "
from sklearn.cluster import KMeans
from sklearn.externals import joblib
import numpy as np
X = np.array($x)
summarize_dir='$summarize_dir'
n_clusters=$n_clusters
kmeans = KMeans(n_clusters=n_clusters, random_state=0)
kmeans.fit(X)
joblib.dump(kmeans, summarize_dir + '/cluster/cluster.pkl')
"
}

pca.transform() {
	python -c "
import numpy as np
summarize_dir='$summarize_dir'
pca=np.loadtxt(summarize_dir+'/pca/pca')
eigenv=np.loadtxt(summarize_dir+'/pca/eigenv')
eigenv=[ i + 0.00001 for i in eigenv ]
average=np.loadtxt(summarize_dir+'/pca/average')
normalized=$1;
normalized=np.subtract(normalized, average)
normalized=np.matmul(pca, normalized)
normalized=np.divide( normalized, np.sqrt(eigenv) )
print normalized
" | sed \
    -e 's/ \+/ /g' \
    -e 's/^\[ /[/g' -e 's/ \]$/]/g' -e 's/ /, /g'
}

pca_pickle.transform() {
	python -c "
import numpy as np
from sklearn.externals import joblib
summarize_dir='$summarize_dir'
pca=joblib.load(summarize_dir + '/pca/pca.pkl')
x=[$1];
print pca.transform(x)[0]
" | sed \
    -e 's/ \+/ /g' \
    -e 's/^\[ /[/g' -e 's/ \]$/]/g' -e 's/ /, /g'
}

pca_scaler.transform() {
	python -c "
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.externals import joblib
summarize_dir='$summarize_dir'
pca=joblib.load(summarize_dir + '/pca/pca.pkl')
scaler=joblib.load(summarize_dir + '/pca/scaler.pkl')
x=[$1]
x=scaler.transform(x)
print pca.transform(x)[0]
" | sed \
    -e 's/ \+/ /g' \
    -e 's/^\[ /[/g' -e 's/ \]$/]/g' -e 's/ /, /g' | paste -s
}

lda.transform() {
	python -c "
import numpy as np
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.externals import joblib
summarize_dir='$summarize_dir'
clf=joblib.load(summarize_dir + '/pca/lda.pkl')
x=[$1];
print clf.transform(x)[0]
" | sed \
    -e 's/ \+/ /g' \
    -e 's/^\[ /[/g' -e 's/ \]$/]/g' -e 's/ /, /g'
}

summarize.doc2txt(){
	filename=$1
	destination=$2

	[ -e "$destination" ] && return
	[ ${filename[1]} = '/' ] || filename="$PWD"/"$filename"
	[ "$destination" = "" ] && destination=/tmp/summary

	rm -fr "$destination"

	[ -d "$filename" ] &&  ( ls "$filename" | while read line; do ( summarize_rec "$filename"/"$line" "$destination" ); done )
	[ -f "$filename" ] &&  summarize_rec "$filename" "$destination"

	[ "$destination" = "/tmp/summary" ] && cat /tmp/summary
}

summarize.hash(){
    rm -fr /tmp/txt
    summarize.doc2txt $1 /tmp/txt

    vector=$($summarize_method.txt2vec /tmp/txt)

    $pca_method.transform $vector

}

summarize.hash2key(){
	python -c "
import numpy as np
normalized=$1
normalized=[ i*3.0 + 0.5 for i in normalized ]
normalized=[ max(min(i,1.0),0.0) for i in normalized ]
chars=\"0123456789\"
hash=[ chars[int((i)*(len(chars)-1))] for i in normalized ]
print ''.join(hash)
"
}

summarize.key(){
    summarize.hash2key "$(summarize.hash $1)"
}

summarize.fit(){
    rm -fr $summarize_dir/txts; mkdir -p $summarize_dir/txts
    ls $summarize_dir/documents/* | while read line
    do
        echo "\e[34m doc2txt \e[0m " $line
        summarize.doc2txt "$line" $summarize_dir/txts/$(basename "$line")
    done

    [ $summarize_method != keywords ] && ( rm -fr $summarize_dir/features; mkdir -p $summarize_dir/features )
    echo "\e[34m get_features \e[0m "
    $summarize_method.get_features

    rm -fr $summarize_dir/vectors; mkdir -p $summarize_dir/vectors
    ls $summarize_dir/txts/* | while read line
    do
        echo "\e[34m txt2vec \e[0m " $line
        $summarize_method.txt2vec "$line" > $summarize_dir/vectors/$(basename "$line")
    done

    rm -fr $summarize_dir/pca; mkdir -p $summarize_dir/pca
    echo "\e[34m pca \e[0m "
    $pca_method.fit

    rm -fr $summarize_dir/hashes; mkdir -p $summarize_dir/hashes
    [ -e $summarize_dir/documents_to_hash ] || cp -r $summarize_dir/documents $summarize_dir/documents_to_hash
    ls $summarize_dir/documents_to_hash/* | while read line
    do
        echo "\e[34m hash \e[0m " $line
        summarize.hash "$line" > $summarize_dir/hashes/$(basename "$line")
    done

    rm -fr $summarize_dir/classifier; mkdir -p $summarize_dir/classifier
    echo "\e[34m classifier \e[0m "
    classifier.fit
    [ -e $summarize_dir/classifier_test ] && classifier.evaluate

    rm -fr $summarize_dir/regression; mkdir -p $summarize_dir/regression
    echo "\e[34m regression \e[0m "
    [ -e $summarize_dir/r_output ] && regression.fit

    rm -fr $summarize_dir/cluster; mkdir -p $summarize_dir/cluster
    echo "\e[34m cluster \e[0m "
    cluster.fit
}

classifier.transform(){
x=$(summarize.hash $1)

y=$(python -c "
x=[$x]
summarize_dir='$summarize_dir'
import numpy as np
from sklearn import linear_model
from sklearn.externals import joblib
clf=joblib.load(summarize_dir + '/classifier/classifier.pkl')
print( clf.predict(x)[0] )
")

cat $summarize_dir/classifier/map | grep "^ $y " | awk '{print $2}'
}

cluster.transform(){
x=$(summarize.hash $1)

y=$(python -c "
x=[$x]
summarize_dir='$summarize_dir'
import numpy as np
from sklearn.cluster import KMeans
from sklearn.externals import joblib
kmeans=joblib.load(summarize_dir + '/cluster/cluster.pkl')
print( kmeans.predict(x)[0] )
")

cat $summarize_dir/cluster/map | grep "^ $y " | awk '{print $2}'
}

regression.transform(){
x=$(summarize.hash $1)

python -c "
x=[$x]
summarize_dir='$summarize_dir'
import numpy as np
from sklearn import linear_model
from sklearn.externals import joblib
reg=joblib.load(summarize_dir + '/regression/regression.pkl')
print(reg.predict(x)[0])
"
}

summarize.compare(){

    rm -fr /tmp/t1 /tmp/t2

    summarize.doc2txt $1 /tmp/t1
    summarize.doc2txt $2 /tmp/t2

    h1=$(summarize.hash /tmp/t1)
    h2=$(summarize.hash /tmp/t2)

    python -c "
import numpy as np
import math
h1=$h1
h2=$h2
print math.sqrt(np.sum(np.square(np.subtract(h1, h2))))
"

}

summarize.distance(){
    python -c "
import numpy as np
import math
h1=$1
h2=$2
print math.sqrt(np.sum(np.square(np.subtract(h1, h2))))
"

}

#summarize.closest(){
    #h1=$(summarize.hash $1)
	#for file in $summarize_dir/hashes/*
	#do
        #d=$(summarize.distance "$h1" "$(cat $file)")
		#echo \"$(basename "$file")\" ":" $d ","
	#done | paste -d"  " -s | sed -e 's/^/{/g' -e 's/,$/}/g' | read distances
	#python -c "
#distances=$distances;
#print min(distances, key=distances.get)
#"
#}

summarize.sortdist(){
    h1=$(summarize.hash $1)
	for file in $summarize_dir/hashes/*
	do
        d=$(summarize.distance "$h1" "$(cat $file)")
		echo \"$(basename "$file")\" ":" $d ","
	done | paste -d"  " -s | sed -e 's/^/{/g' -e 's/,$/}/g' | read distances
	python -c "
distances=$distances;
for key, value in sorted(distances.iteritems(), key=lambda (k,v): (v,k)):
  print '%s' % (key)
"
}

summarize.closest(){
    summarize.sortdist $1 | head -n1
}

summarize.knn(){
    summarize.sortdist $1 | head -n $2 | sed 's/\..*//g' | sort | uniq -c | sort -g | tail -n1 | sed 's/^ *[^ ]* //g'
}

keywords_filter () {
	cat $1 | while read line
	do
		found=0
		echo $line > /tmp/line
		getwords /tmp/line
		for a in $(cat ~/summarize/features/keywords | grep -v '^#' | grep -v '^$')
		do
			[ $a[0,1] = '^' ] && regex=$(echo $a | sed 's/^.//g' )
			[ $a[0,1] = '^' ] || regex=$a
			value=$( cat /tmp/keywords | egrep '^ *[0-9]* '$regex | awk '{sum += $1} END {print sum}' )
			[ "$value" != "" ] && found=1  && break
		done
		[ $found -eq 1 ] && echo $line
	done
}

