vectorizer_method=keywords
pca_method=identity
classifier_method=closest_metadata_regex
regression_method=linear_regression
indexer_method=sliding_indexer
clustering_method=kmeans_clustering
learning_evaluate_vects="true"
learning_dir=$HOME/learning
tmp=$learning_dir/tmp
learning_src=/media/removable/2TB2/homes/admin/Alejandria
learning_queue=/media/removable/2TB2/homes/admin/Descargas
learning_docs_per_class=5
keywords_normalize="false"
summarize_page_limit=3
summarize_line_limit=20
generate_topic_documents=true
keywords_lowercase=true
window_lowercase=true
learning_scale=3.0
learning_distance=jaccard
indexer_add_kw=true
classify_highlight=diff

ncd.get_features(){
    cp -r $learning_dir/txts/* $learning_dir/features
}

ncd.txt2vec() {
	for file in $learning_dir/features/*
	do
		gzip_distance $1 $file
	done | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g' -e 's/,/, /g'
}

keywords.get_features(){
    [ -e $learning_dir/features/keywords ] && return
    mkdir -p $learning_dir/features
    [ -e ~/Dotfiles/keywords/keywords ] && cp ~/Dotfiles/keywords/keywords $learning_dir/features/keywords
    [ -e ~/Dotfiles/keywords/keywords ] || cat $learning_dir/txts/* | sed 's/ /\n/g' | sort | uniq > $learning_dir/features/keywords
}

rules.transform(){
    cat | sed \
        -e 's/c++/cpp/g'  \
        -e 's/c#/csharp/g'  \
        -e 's/?/ questionmark/g' \
        -e 's/\(machine.learning\)/\1 MachineLearning /gI' \
        -e 's/\(black.hat\)/\1 BlackHat /gI' \
        -e 's/\(def.con\)/\1 DefCon /gI' \
        -e 's/\(big.data\)/\1 BigData /gI' \
        -e 's/\(artificial.intelligence\)/\1 ArtificialIntelligence /gI' \
        -e 's/\(programming.language\(s\|\)\)/\1 ProgrammingLanguages /gI' \
        -e 's/\(neural.network\(s\|\)\)/\1 NeuralNetworks /gI' \
        -e 's/\(open.source\)/\1 OpenSource /gI' \
        -e 's/\([m|M]ake:\)/\1 Makezine /gI' \
        -e 's/\(data.science\(s\|\)\)/\1 DataScience /gI' \
        -e 's/\(high.performance.comput\)/HighPerfComp \1/gI' \
        -e 's/\(hand\(.\|\)book\)/\1 HandBook /gI' \
        -e 's/\[pdf\]/PdfFile/gI'
}

camelcase(){
    a=$(cat)
    b=$(echo $a | sed 's/\([a-z]\)\([A-Z]\)/\1 \2/g' | tr '[A-Z]' '[a-z]')
    c=$(echo $a | tr '[A-Z]' '[a-z]')
    echo $c $b
}

tokenize(){
    [ $keywords_lowercase = "true" ] && lowercmd="tr '[A-Z]' '[a-z]'"
    [ $keywords_lowercase = "false" ] && lowercmd="cat"
    [ $keywords_lowercase = "camelcase" ] && lowercmd="camelcase"
    cat | sed -r "s/[[:cntrl:]]\[[0-9]{1,3}m//g" | sed \
        -e 's/á/a/g' \
        -e 's/é/e/g' \
        -e 's/í/i/g' \
        -e 's/ó/o/g' \
        -e 's/ú/u/g' \
        -e 's/à/a/g' \
        -e 's/è/e/g' \
        -e 's/ì/i/g' \
        -e 's/ò/o/g' \
        -e 's/ù/u/g' \
        -e 's/ä/a/g' \
        -e 's/ë/e/g' \
        -e 's/ï/i/g' \
        -e 's/ö/o/g' \
        -e 's/ü/u/g' \
        | strings -n1 | $(echo $lowercmd) | \
        rules.transform | sed \
        -e 's/ /\n/g'  \
        -e 's/\./\n/g' \
        -e 's/\t/\n/g' \
        -e 's/\f/\n/g' \
        -e 's/\v/\n/g' \
        -e 's/\r/\n/g' \
        -e 's/-/\n/g'  \
        -e 's/\//\n/g' \
        -e 's/\\/\n/g' \
        -e 's/\+/\n/g' \
        -e 's/,/\n/g'  \
        -e 's/;/\n/g'  \
        -e 's/\*/\n/g' \
        -e 's/>/\n/g'  \
        -e 's/</\n/g'  \
        -e 's/_/\n/g'  \
        -e 's/"/\n/g'  \
        -e 's/%/\n/g'  \
        -e 's/&/\n/g'  \
        -e 's/(/\n/g'  \
        -e 's/)/\n/g'  \
        -e 's/\[/\n/g' \
        -e 's/\]/\n/g' \
        -e 's/{/\n/g'  \
        -e 's/}/\n/g'  \
        -e 's/=/\n/g'  \
        -e 's/!/\n/g'  \
        -e 's/@/\n/g'  \
        -e 's/:/\n/g'  \
        -e "s/'/\n/g"  \
        -e "s/?/\n/g"  \
        -e 's/~/\n/g'
}

countwords(){
    cat | sort | uniq -c | egrep -v '^ *[0-9]* $'
}

getkeywords_(){
    cat > $tmp/keywords
    for a in $(cat $learning_dir/features/keywords | grep -v '^#' | grep -v '^$')
    do
         [ $a[0,1] = '^' ] && regex='^ *[0-9]* '$(echo $a | sed 's/^.//g' )
         [ $a[0,1] = '^' ] || regex='^ *[0-9]* .*'$a
         value=$( cat $tmp/keywords | egrep $regex | awk '{sum += $1} END {print sum}' )
         [ "$value" = "" ] && value=0
         echo $value $a
    done
}

getkeywords(){
	cat | awk '{print $1 " " $2}' > $tmp/keywords

    python -c "
import re

with open('$tmp/keywords') as f:
    lines = [ line.rstrip('\n') for line in f ]
with open('$learning_dir' + '/features/keywords') as f:
    keywords = [ line.rstrip('\n') for line in f ]
    keywords = [ keyword for keyword in keywords if keyword != '' ]
with open('$learning_dir' + '/features/stopwords') as f:
    stopwords = [ line.rstrip('\n') for line in f ]

lines = [ line for line in lines if not line in stopwords ]

for keyword in keywords:
    value = 0

    regex = keyword

    if(keyword[0] == '^'):
        pass
    else:
        regex='.*' + regex

    if(keyword[-1] == '$'):
        pass
    else:
        regex= regex + '.*'

    for line in lines:
        word=line.split(' ')[1]
        if(re.match(regex,word)):
            value += int( line.split(' ')[0] )

    print( repr(value) + ' ' + keyword )
"
}

haskeyword_(){
    cat > $tmp/keywords
    for a in $(cat $learning_dir/features/keywords | grep -v '^#' | grep -v '^$')
    do
         [ $a[0,1] = '^' ] && regex=$(echo $a | sed 's/^.//g' )
         [ $a[0,1] = '^' ] || regex=$a
         value=$( cat $tmp/keywords | egrep '^ *[0-9]* .*'$regex | awk '{sum += $1} END {print sum}' )
         [ "$value" = "" ] && continue
         echo true && return
    done
    echo false
}

haskeyword(){
    cat | sed 's/^ *[0-9]* //g' > $tmp/keywords

    python -c "
import re

with open('$tmp/keywords') as f:
    lines = [ line.rstrip('\n') for line in f ]
with open('$learning_dir' + '/features/keywords') as f:
    keywords = [ line.rstrip('\n') for line in f ]
    keywords = [ keyword for keyword in keywords if keyword != '' ]
with open('$learning_dir' + '/features/stopwords') as f:
    stopwords = [ line.rstrip('\n') for line in f ]

lines = [ line for line in lines if not line in stopwords ]

for keyword in keywords:
    regex=keyword

    if(keyword[0] == '^'):
        pass
    else:
        regex='.*' + regex

    if(keyword[-1] == '$'):
        pass
    else:
        regex= regex + '.*'

    for line in lines:
        if(re.match(regex,line)):
            print('true')
            exit()

print('false')
"
}

keywords.txt2vec(){

    cat $1 | tokenize | countwords | getkeywords > $tmp/kwords

    vect=$(cat $tmp/kwords | awk '{print $1}' | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g' -e 's/,/, /g')
    [ $keywords_normalize = "full" ] && total=$( cat $1 | wc -w )
    [ $keywords_normalize = "keys" ] && total=$(cat $tmp/kwords | awk '{sum += $1} END {print sum}' )
    [ $keywords_normalize = "false" ] && total=1

    python -c "
import numpy as np
import sys
np.set_printoptions(threshold=sys.maxsize)
vect=$vect
total=$total
print(np.divide(vect, float(total+0.00001)))
" | sed -e 's/ \+/ /g' -e 's/ /, /g' -e 's/\[,/\[/g' -e 's/, \]/\]/g' | paste -s -d' '

}

windowkeywords.get_features(){
    cat $learning_dir/documents/* | tokenize | sort | uniq | keywords.filter | tee $learning_dir/features/full_keywords
}

windowkeywords.txt2vec(){

    local line
    cat $1 | tokenize | awk '{a[NR]=$1} NR>5{print a[NR-5]" "a[NR-4]" "a[NR-3]" "a[NR-2]" "a[NR-1]" "a[NR]}' | while read line
    do
        comm -12 <(cat $learning_dir/features/full_keywords | sort | uniq | egrep -v '^$') <(echo $line | tokenize | sort | uniq | egrep -v '^$') | paste -d' ' -s
    done | grep -v '^$' > $tmp/intersection

    mkdir -p $learning_dir/trmemo
    cat $tmp/intersection | while read line
    do
        md5line=$(echo $line | md5sum | awk '{print $1}')
        if [ -e $learning_dir/trmemo/$md5line ]
        then
            cat $learning_dir/trmemo/$md5line
        else
            echo $line | sed 's/ /\n/g' > $tmp/line
            newline=$( cat $tmp/line | tokenize | countwords | getkeywords | grep -v '^0' | awk '{print $2}' | paste -d' ' -s )
            echo $newline > $learning_dir/trmemo/$md5line
            echo $newline
        fi
    done > $tmp/intersection2
    \mv $tmp/intersection2 $tmp/intersection

    cat $tmp/intersection | while read line
    do
        echo $line | md5sum | sed 's/^\(.....\).*/\1/g' | tr '[a-z]' '[A-Z]' | sed -e 's/^/ibase=16;/g' | bc | sed -e 's/$/%1000/g' | bc
    done | sort | uniq -c | sed -e 's/^ *\([0-9]*\) *\([0-9]*\)/(\2,\1)/g' > $tmp/words

	python -c "
with open('$tmp/words') as f:
    pairs = [ eval(line) for line in f ]

dic = dict(pairs)

for i in range(0,1000):
    print dic.get(i) or 0
" | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/\]/g' -e 's/,/, /g'
}

windowwords.txt2vec(){

    local line
    cat $1 | tokenize | awk '{a[NR]=$1} NR>5{print a[NR-5]" "a[NR-4]" "a[NR-3]" "a[NR-2]" "a[NR-1]" "a[NR]}' | while read line
    do
python -c "
line='$line'
linesp=line.split(' ')
for a in range(1, pow(2,len(linesp))):
    binrep=bin(a)[2:]
    sel1 = range(len(linesp)-len(binrep), len(linesp))
    sel2 = [ sel1[i] for i in range(0, len(binrep)) if binrep[i] == '1' ]
    words=[linesp[i] for i in sel2]
    print(' '.join(words))
"
    done | while read line
    do
        echo $line | md5sum | sed 's/^\(.....\).*/\1/g' | tr '[a-z]' '[A-Z]' | sed -e 's/^/ibase=16;/g' | bc | sed -e 's/$/%1000/g' | bc
    done | sort | uniq -c | sed -e 's/^ *\([0-9]*\) *\([0-9]*\)/(\2,\1)/g' > $tmp/words

    python -c "
with open('$tmp/words') as f:
    pairs = [ eval(line) for line in f ]

dic = dict(pairs)

for i in range(0,1000):
    print dic.get(i) or 0
" | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/\]/g' -e 's/,/, /g'

}

winshingles.txt2vec(){
    local line
    cat $1 | tokenize | while read line
    do
        echo $line | md5sum | sed 's/^\(.....\).*/\1/g' | tr '[a-z]' '[A-Z]' | sed -e 's/^/ibase=16;/g' | bc | sed -e 's/$/%1000/g' | bc
    done | awk '{a[NR]=$1} NR>2{print a[NR-2]" "a[NR-1]" "a[NR]}' | while read line
    do
        echo $line | md5sum | sed 's/^\(.....\).*/\1/g' | tr '[a-z]' '[A-Z]' | sed -e 's/^/ibase=16;/g' | bc | sed -e 's/$/%1000/g' | bc
    done | sort | uniq -c | sed -e 's/^ *\([0-9]*\) *\([0-9]*\)/(\2,\1)/g' > $tmp/words

    python -c "
with open('$tmp/words') as f:
    pairs = [ eval(line) for line in f ]

dic = dict(pairs)

for i in range(0,1000):
    print dic.get(i) or 0
" | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/\]/g' -e 's/,/, /g'
}

bagofwords.txt2vec(){
    local line

    [ $window_lowercase = "true" ] && lowercmd="tr '[A-Z]' '[a-z]'"
    [ $window_lowercase = "true" ] || lowercmd="cat"

    cat $1 | $(echo $lowercmd) | tokenize | while read line
    do
        echo $line | md5sum | cut -d' ' -f1 | tr '[a-z]' '[A-Z]' | sed -e 's/^/ibase=16;/g' | bc | sed -e 's/$/%1000/g' | bc
    done | sort | uniq -c | sed -e 's/^ *\([0-9]*\) *\([0-9]*\)/(\2,\1)/g' > $tmp/words

	python -c "
with open('$tmp/words') as f:
    pairs = [ eval(line) for line in f ]

dic = dict(pairs)

for i in range(0,1000):
    print dic.get(i) or 0
" | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/\]/g' -e 's/,/, /g'
}

window.txt2vec(){
    local line

    [ $window_lowercase = "true" ] && lowercmd="tr '[A-Z]' '[a-z]'"
    [ $window_lowercase = "true" ] || lowercmd="cat"

    cat $1 | $(echo $lowercmd) | sed -e 's/ /./g' -e 's/\(.\)/\1\n/g' | awk '{a[NR]=$1} NR>2{print a[NR-2]a[NR-1]a[NR]}' | while read line
    do
        echo $line | md5sum | sed 's/^\(.....\).*/\1/g' | tr '[a-z]' '[A-Z]' | sed -e 's/^/ibase=16;/g' | bc | sed -e 's/$/%1000/g' | bc
    done | sort | uniq -c | sed -e 's/^ *\([0-9]*\) *\([0-9]*\)/(\2,\1)/g' > $tmp/words

	python -c "
with open('$tmp/words') as f:
    pairs = [ eval(line) for line in f ]

dic = dict(pairs)

for i in range(0,1000):
    print dic.get(i) or 0
" | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/\]/g' -e 's/,/, /g'
}

window.get_features(){}

windowpy.txt2vec(){
    [ $window_lowercase = "true" ] && lowercmd="tr '[A-Z]' '[a-z]'"
    [ $window_lowercase = "true" ] || lowercmd="cat"

    line=$( cat $1 | $(echo $lowercmd) | paste -d' ' -s )
python -c "
import numpy as np
import hashlib
import sys
np.set_printoptions(threshold=sys.maxsize)
out=np.zeros(1000)
line='$line'
sliding=[line[i:i+3] for i in xrange(len(line)-2)]
for b in sliding:
    #hash=(ord(b[0])*255*255+ord(b[1])*255+ord(b[2]))%1000
    hash=int(hashlib.md5(b).hexdigest(),16)%1000
    out[hash] += 1
print out
" | sed 's/  /, /g' | paste -d' ' -s
}

windowpy.get_features(){}

bog_and_win.txt2vec(){
    ( bagofwords.txt2vec $1; windowpy.txt2vec $1 ) | paste -d' ' -s | sed -e 's/\.//g' -e 's/\] \[/,/g'
}

bog_and_win.get_features(){}

keywords_multiscale.txt2vec(){

    vect1=$(cat $1 | head -n 1     | tokenize | countwords | getkeywords | awk '{print $1}')
    vect2=$(cat $1 | head -n 10    | tokenize | countwords | getkeywords | awk '{print $1}')
    vect3=$(cat $1 | head -n 10000 | tokenize | countwords | getkeywords | awk '{print $1}')

    echo $vect1 $vect2 $vect3 | paste -d',' -s | sed -e 's/^/[/g' -e 's/$/]/g' -e 's/,/, /g'
}

keywords_multiscale.get_features(){
    keywords.get_features
}

keywords.doc2vec_debug(){

    if [ $# -gt 1 ]
    then
        for a in $*
        do
            echo "===== $a ====="
            keywords.doc2vec_debug $a
        done
        return
    fi

    learning.doc2txt $1 > $tmp/txt
    cat $tmp/txt | tokenize | countwords > $tmp/keywords

    for a in $(cat $learning_dir/features/keywords | grep -v '^#' | grep -v '^$')
    do
         [ $a[0,1] = '^' ] && regex=$(echo $a | sed 's/^.//g' )
         [ $a[0,1] = '^' ] || regex=$a
         value=$( cat $tmp/keywords | egrep '^ *[0-9]* '$regex | awk '{sum += $1} END {print sum}' )
         [ "$value" != "" ] && echo $a $value
    done
}

keywords.doc2vec_debug(){

    if [ $# -eq 1 ] && [ $1 = '-' ]
    then
        cat > $tmp/stdin.txt
        keywords.doc2vec_debug $tmp/stdin.txt
        return
    fi

    if [ $# -gt 1 ]
    then
        for a in $*
        do
            echo "===== $a ====="
            keywords.doc2vec_debug $a
        done
        return
    fi

    cat $1 | tokenize | countwords | getkeywords | grep -v '^0'

}

txtsimp(){
    cat | sed -r "s/[[:cntrl:]]\[[0-9]{1,3}m//g" | sed \
        -e 's/á/a/g' \
        -e 's/é/e/g' \
        -e 's/í/i/g' \
        -e 's/ó/o/g' \
        -e 's/ú/u/g' \
        -e 's/à/a/g' \
        -e 's/è/e/g' \
        -e 's/ì/i/g' \
        -e 's/ò/o/g' \
        -e 's/ù/u/g' \
        -e 's/ä/a/g' \
        -e 's/ë/e/g' \
        -e 's/ï/i/g' \
        -e 's/ö/o/g' \
        -e 's/ü/u/g'
}

doc2txt_rec(){
    local line

	filename="$1"
	destination="$2"

	case $filename in
		(*/title|title)
			cat "$filename" | head -n $summarize_line_limit | sed -r "s/[[:cntrl:]]\[[0-9]{1,3}m//g" | txtsimp | strings -n1 >> "$destination"
			;;
		(*.rar|*.RAR)
			rm -fr $tmp/learning
			mkdir -p $tmp/learning
			cd $tmp/learning
			unrar x "$filename" >/dev/null 2>/dev/null
			find $PWD | while read line
			do
				doc2txt_rec "$line" "$destination" 
			done
			sed -i "s|$tmp/learning||g" "$destination"
			[ $PWD = $OLDPWD ] || cd $OLDPWD
			;;
		(*.zip|*.ZIP)
			rm -fr $tmp/learning
			mkdir -p $tmp/learning
			cd $tmp/learning
			unzip "$filename" >/dev/null 2>/dev/null
			find $PWD | while read line
			do
				doc2txt_rec "$line" "$destination" 
			done
			sed -i "s|$tmp/learning||g" "$destination"
			[ $PWD = $OLDPWD ] || cd $OLDPWD
			;;
		(*.pdf|*.PDF)
			pdftotext -f 1 -l $summarize_page_limit "$filename" - | head -n $summarize_line_limit | txtsimp | strings -n1 >> "$destination"
			;;
		(*.djvu|*.djVu)
			djvutxt "$filename" | head -n $summarize_line_limit | txtsimp | strings -n1 >> "$destination"
			;;
		(*.epub|*.EPUB)
			epub2txt "$filename" | head -n $summarize_line_limit | txtsimp | strings -n1 >> "$destination"
			;;
		(*.mobi|*.MOBI)
			ebook-convert "$filename" $tmp/mobi.txt
			cat $tmp/mobi.txt | head -n $summarize_line_limit | txtsimp | strings -n1 >> "$destination"
			;;
		(*.html|*.htm)
			#html2text "$filename" | head -n $summarize_line_limit >> "$destination"
			w3m -dump "$filename" | head -n $summarize_line_limit >> "$destination"
			;;
		(*.txt|*.TXT)
			cat "$filename" | head -n $summarize_line_limit | sed -r "s/[[:cntrl:]]\[[0-9]{1,3}m//g" | txtsimp | strings -n1 >> "$destination"
			;;
		(*)
			file "$filename" | grep 'text' >/dev/null && cat "$filename" | head -n $summarize_line_limit | sed -r "s/[[:cntrl:]]\[[0-9]{1,3}m//g" | txtsimp | strings -n1 >> "$destination"
			file "$filename" | grep 'text' >/dev/null || echo "$filename" >> "$destination"
	esac
}

gzip_distance(){
    rm -fr $tmp/s1 $tmp/s2
    learning.doc2txt "$1" $tmp/s1
    learning.doc2txt "$2" $tmp/s2
    s1=$( cat $tmp/s1 | gzip -f | wc -c )
    s2=$( cat $tmp/s2 | gzip -f | wc -c )
    s12=$( cat $tmp/s1 $tmp/s2 | gzip | wc -c )

   python -c "print float($s12 - min($s1, $s2))/float(max($s1,$s2))"
}

jaccard_distance(){
    rm -fr $tmp/s1 $tmp/s2
    learning.doc2txt "$1" $tmp/s1
    learning.doc2txt "$2" $tmp/s2

    cat $tmp/s1 | tokenize | countwords | sort | uniq > $tmp/k1
    cat $tmp/s2 | tokenize | countwords | sort | uniq > $tmp/k2

    si=$( comm -12 $tmp/k1 $tmp/k2 | wc -l )
    su=$( cat      $tmp/k1 $tmp/k2 | sort | uniq | wc -l )

    python -c "print 1.0-float($si)/float($su+0.00001)"
}

closest_classifier.transform(){
    closest_classifier.sortdist $1 | head -n 1 | sed 's/\..*//g'
}

closest_classifier.fit(){
        for file in $learning_dir/vectors_reduced/*
        do
            cat $file | sed -e 's/\[//g' -e 's/\]//g' -e 's/, / /g' >> $learning_dir/classifier/sortvectors
            echo $(basename "$file") >> $learning_dir/classifier/sortnames
        done
}

naive_bayes.transform(){
x=$(learning.doc2rvect $1)

y=$(python -c "
x=[$x]
learning_dir='$learning_dir'
import numpy as np
from sklearn.naive_bayes import GaussianNB
from sklearn.externals import joblib
gnb=joblib.load(learning_dir + '/classifier/classifier.pkl')
print( int(gnb.predict(x)[0]) )
")

cat $learning_dir/classifier/map | grep "^ $y " | awk '{print $2}'
}

naive_bayes.fit(){
    rm -fr $tmp/classifier.x $tmp/classifier.y
    ls $learning_dir/vectors_reduced/ | sed 's/\..*//g' | sort | uniq | nl - | sed 's/\t/ /g' | sed -e 's/ \+/ /g' > $learning_dir/classifier/map
    for file in $learning_dir/vectors_reduced/*
    do
        cat $file | sed -e 's/\[//g' -e 's/\]//g' -e 's/, / /g' >> $tmp/classifier.x
        y=$(basename $file | sed 's/\..*//g')
        id=$(cat $learning_dir/classifier/map | grep " $y$" | awk '{print $1}')
        echo $id >> $tmp/classifier.y
    done

	python -c "
import numpy as np
from sklearn.naive_bayes import GaussianNB
from sklearn.externals import joblib
X = np.loadtxt('$tmp/classifier.x')
Y = np.loadtxt('$tmp/classifier.y')
learning_dir='$learning_dir'
gnb = GaussianNB()
gnb.fit(X, Y)
joblib.dump(gnb, learning_dir + '/classifier/classifier.pkl')
"
}

double_keywords.fit(){}

double_keywords.transform(){

    line=$(cat $1)

    \cp $learning_dir/features/negative $learning_dir/features/keywords
    haskeyword=$( echo $line | tokenize | countwords | haskeyword )
    [ $haskeyword = "true" ] && echo "false" && return

    \cp $learning_dir/features/positive $learning_dir/features/keywords
    haskeyword=$( echo $line | tokenize | countwords | haskeyword )
    [ $haskeyword = "true" ] && echo "true" && return

    echo "false"
}

closest_metadata.transform(){
    [ -e "$learning_dir/metadata/$(basename $1)/title" ] && closest_classifier.transform "$learning_dir/metadata/$(basename $1)/title" && return
    [ -e "$learning_dir/metadata/$(basename $1 | sed 's/\.txt$//g')/title" ] && closest_classifier.transform "$learning_dir/metadata/$(basename $1 | sed 's/\.txt$//g')/title" && return
    [ -e "$learning_dir/metadata/$(basename $1)/title" ] || closest_classifier.transform "$1" && return
}

closest_metadata.sortdist(){
    [ -e "$learning_dir/metadata/$(basename $1)/title" ] && closest_classifier.sortdist "$learning_dir/metadata/$(basename $1)/title" && return
    [ -e "$learning_dir/metadata/$(basename $1 | sed 's/\.txt$//g')/title" ] && closest_classifier.sortdist "$learning_dir/metadata/$(basename $1 | sed 's/\.txt$//g')/title" && return
    [ -e "$learning_dir/metadata/$(basename $1)/title" ] || closest_classifier.sortdist "$1" && return
}

closest_metadata.fit(){
    closest_classifier.fit
}

regex_classifier.transform(){

    cat $1 | tokenize | countwords | awk '{print $2}' > $tmp/regex_tokens

    python -c "
import re

with open('$learning_dir/features/regex') as f:
    keywords = [ line.rstrip('\n').split(' ') for line in f ]

with open('$tmp/regex_tokens') as f:
    words = [ line.rstrip('\n') for line in f ]

for keyword in keywords:

    regex = keyword[0]
    topic = keyword[1]

    if(keyword[0] == '^'):
        pass
    else:
        regex='.*' + regex
    if(keyword[-1] == '$'):
        pass
    else:
        regex= regex + '.*'

    for word in words:
        if(re.match(regex,word)):
            print topic
            exit
"
}

closest_metadata_regex.transform(){
    regclass=""
    [ -e "$learning_dir/metadata/$(basename $1)/title" ] && regclass=$(regex_classifier.transform "$learning_dir/metadata/$(basename $1)/title")
    [ -e "$learning_dir/metadata/$(basename $1 | sed 's/\.txt$//g')/title" ] && regclass=$(regex_classifier.transform "$learning_dir/metadata/$(basename $1 | sed 's/\.txt$//g')/title")
    [ "$regclass" != "" ] && echo $regclass | head -n1 && return
    closest_metadata.transform $1
}

closest_metadata_regex.fit(){
    closest_classifier.fit
}

closest_filter.transform(){
    local line

    learning.doc2txt $1 > $tmp/filter
    head -n$summarize_line_limit $tmp/filter | while read line
    do
        echo $line | tokenize | countwords | getkeywords > $tmp/kwords
        nkeys=$(cat $tmp/kwords | awk '{sum += $1} END {print sum}' )
        nwords=$(echo $line | wc -w)
        echo $(( $nkeys*100/$nwords )) $line
    done | sort -g > $tmp/lines_sorted
    #line=$(tail -n1 $tmp/lines_sorted)
    line=$(cat $tmp/lines_sorted | awk '$1>70' | sed 's/^ *[0-9]* //g' | paste -s)
    echo $line > $tmp/line
    closest_classifier.transform $tmp/line
}

identity.fit(){
}

pca.fit(){
    rm -fr $tmp/reduced_vectors
    for file in $learning_dir/vectors/*
    do
        cat $file | sed -e 's/\[//g' -e 's/\]//g' -e 's/,//g' >> $tmp/reduced_vectors
    done
	python -c "
import numpy as np
from sklearn.decomposition import PCA
learning_dir='$learning_dir'
X=np.loadtxt('$tmp/reduced_vectors')
pca=PCA()
pca.fit(X)
np.savetxt(learning_dir + '/pca/average',np.mean(X,0))
np.savetxt(learning_dir + '/pca/pca',pca.components_)
np.savetxt(learning_dir + '/pca/eigenv',pca.explained_variance_)
"
}

index.fit(){
    rm -fr $tmp/vectors
    for file in $learning_dir/vectors/*
    do
        cat $file | sed -e 's/\[//g' -e 's/\]//g' -e 's/,//g' >> $tmp/vectors
    done
	python -c "
import numpy as np
learning_dir='$learning_dir'
X=np.loadtxt('$tmp/vectors')
sumvect=sum(X)
#indices=[ i for i in range(0, len(sumvect)) if sumvect[i] > 0 ]
dictionary=dict( { (k,v) for (k,v) in zip(range(0, len(X)), X) if v > 0} )
std=sorted(dictionary, key=dictionary.get, reverse=True)
np.savetxt(learning_dir + '/pca/indices',std)
"
}

pca100.fit(){
    rm -fr $tmp/reduced_vectors
    for file in $learning_dir/vectors/*
    do
        cat $file | sed -e 's/\[//g' -e 's/\]//g' -e 's/,//g' >> $tmp/reduced_vectors
    done
	python -c "
import numpy as np
from sklearn.decomposition import PCA
learning_dir='$learning_dir'
X=np.loadtxt('$tmp/reduced_vectors')
pca=PCA(n_components=100)
pca.fit(X)
np.savetxt(learning_dir + '/pca/average',np.mean(X,0))
np.savetxt(learning_dir + '/pca/pca',pca.components_)
np.savetxt(learning_dir + '/pca/eigenv',pca.explained_variance_)
"
}

pca_pickle.fit(){
    rm -fr $tmp/reduced_vectors
    for file in $learning_dir/vectors/*
    do
        cat $file | sed -e 's/\[//g' -e 's/\]//g' -e 's/,//g' >> $tmp/reduced_vectors
    done
	python -c "
import numpy as np
from sklearn.decomposition import PCA
from sklearn.externals import joblib
learning_dir='$learning_dir'
X=np.loadtxt('$tmp/reduced_vectors')
pca=PCA()
pca.fit(X)
joblib.dump(pca, learning_dir + '/pca/pca.pkl')
"
}

pca_scaler.fit(){
    rm -fr $tmp/reduced_vectors
    for file in $learning_dir/vectors/*
    do
        cat $file | sed -e 's/\[//g' -e 's/\]//g' -e 's/, / /g' >> $tmp/reduced_vectors
    done

	python -c "
import numpy as np
from sklearn.decomposition import PCA
from sklearn.externals import joblib
from sklearn.preprocessing import StandardScaler
learning_dir='$learning_dir'
X=np.loadtxt('$tmp/reduced_vectors')
scaler = StandardScaler()
scaler.fit(X)
X=scaler.transform(X)
pca=PCA()
pca.fit(X)
joblib.dump(pca, learning_dir + '/pca/pca.pkl')
joblib.dump(scaler, learning_dir + '/pca/scaler.pkl')
"
}

lda.fit(){

    rm -fr $tmp/lda.x $tmp/lda.y
    ls $learning_dir/vectors/ | sed 's/\..*//g' | sort | uniq | nl - | sed 's/\t/ /g' | sed -e 's/ \+/ /g' > $learning_dir/pca/map

    for file in $learning_dir/vectors/*
    do
        cat $file | sed -e 's/\[//g' -e 's/\]//g' -e 's/, / /g' >> $tmp/lda.x
        y=$(basename $file | sed 's/\..*//g')
        id=$(cat $learning_dir/pca/map | grep " $y$" | awk '{print $1}')
        echo $id >> $tmp/lda.y
    done

	python -c "
import numpy as np
import sys
np.set_printoptions(threshold=sys.maxsize)
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.externals import joblib
X = np.loadtxt('$tmp/lda.x')
Y = np.loadtxt('$tmp/lda.y')
learning_dir='$learning_dir'
clf = LinearDiscriminantAnalysis()
clf.fit(X, Y)
joblib.dump(clf, learning_dir + '/pca/lda.pkl')
"
}

pca.rank(){

    #doc_2_vect=$(for file in $learning_dir/vectors/*
    #do
        #echo "'"$(basename $file)"'":$(cat $file),
    #done | paste -s -d' ' | sed -e 's/^/{/g' -e 's/$/}/g' -e 's/,}/}/g')
    rm -rf $tmp/names $tmp/vectors
    for file in $learning_dir/vectors/*
    do
        echo $(basename $file) >> $tmp/names
        cat $file | sed -e 's/\[//g' -e 's/\]//g' -e 's/, / /g' >> $tmp/vectors
    done

	python -c "
import numpy as np
from sklearn.decomposition import PCA
n_components=3

with open('$tmp/names') as f:
    names = [ line.rstrip('\n') for line in f ]
with open('$tmp/vectors') as f:
    vectors = [ line.rstrip('\n') for line in f ]
doc_2_vect={ names[i]:np.fromstring(vectors[i], dtype=float, sep=' ') for i in range(0,len(names)) }

doc_2_rank={k:99999 for k in doc_2_vect}
pca=PCA(n_components=n_components)
original_pca=PCA(n_components=n_components)
original_pca.fit(doc_2_vect.values())

def cost(a,b):
    return np.sum( np.absolute( np.subtract(a.components_, b.components_) ) )

for round in range(1,len(doc_2_vect)-(n_components-1)):
    doc_2_cost={k:99999 for k in doc_2_vect}
    for item in doc_2_vect.keys():
        if(doc_2_rank[item] < round):
            continue
        doc_2_rank[item] = round
        selection=[ v for (k,v) in doc_2_vect.items() if doc_2_rank[k] > round ]
        pca.fit(selection)
        doc_2_cost[item] = cost(pca, original_pca)
        doc_2_rank[item] = 99999
    min_doc=min(doc_2_cost, key=doc_2_cost.get)
    doc_2_rank[min_doc] = round

#print doc_2_rank

for a in sorted(doc_2_rank, key=doc_2_rank.get):
    print a
"
}

linear_classifier.fit(){

    rm -fr $tmp/classifier.x $tmp/classifier.y
    ls $learning_dir/vectors_reduced/ | sed 's/\..*//g' | sort | uniq | nl - | sed 's/\t/ /g' | sed -e 's/ \+/ /g' > $learning_dir/classifier/map
    for file in $learning_dir/vectors_reduced/*
    do
        cat $file | sed -e 's/\[//g' -e 's/\]//g' -e 's/, / /g' >> $tmp/classifier.x
        y=$(basename $file | sed 's/\..*//g')
        id=$(cat $learning_dir/classifier/map | grep " $y$" | awk '{print $1}')
        echo $id >> $tmp/classifier.y
    done

	python -c "
import numpy as np
from sklearn import linear_model
from sklearn.externals import joblib
X = np.loadtxt('$tmp/classifier.x')
Y = np.loadtxt('$tmp/classifier.y')
learning_dir='$learning_dir'
clf = linear_model.SGDClassifier()
clf.fit(X, Y)
joblib.dump(clf, learning_dir + '/classifier/classifier.pkl')
"
}

logistic_classifier.fit(){

    rm -fr $tmp/classifier.x $tmp/classifier.y
    ls $learning_dir/vectors_reduced/ | sed 's/\..*//g' | sort | uniq | nl - | sed 's/\t/ /g' | sed -e 's/ \+/ /g' > $learning_dir/classifier/map
    for file in $learning_dir/vectors_reduced/*
    do
        cat $file | sed -e 's/\[//g' -e 's/\]//g' -e 's/, / /g' >> $tmp/classifier.x
        y=$(basename $file | sed 's/\..*//g')
        id=$(cat $learning_dir/classifier/map | grep " $y$" | awk '{print $1}')
        echo $id >> $tmp/classifier.y
    done

	python -c "
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.externals import joblib
X = np.loadtxt('$tmp/classifier.x')
Y = np.loadtxt('$tmp/classifier.y')
learning_dir='$learning_dir'
clf = LogisticRegression()
clf.fit(X, Y)
joblib.dump(clf, learning_dir + '/classifier/classifier.pkl')
"
}

vect2s(){
    read vect
    learning.rvect2key "$( python -c "
vect=$vect
print([ (i/float(max(vect)+0.00001) - 0.5)/$learning_scale for i in vect ])
")" | sed -e 's/0/./g' -e 's/^/[/g' -e 's/$/]/g'
}

learning.classify(){
    if [ "$1" = "-" ]
    then
        cat > $tmp/stdin.txt
        $classifier_method.transform $tmp/stdin.txt
        return
    fi

    $classifier_method.transform "$1"
}

classifier.evaluate(){
    local line

    rm -fr $tmp/classifier.y_true $tmp/classifier.y_pred

    mkdir -p $learning_dir/classifier_test_results
    ls $learning_dir/classifier_test/* | while read line
    do
        y_pred=$($classifier_method.transform "$line")
        name=$(basename "$line")
        echo $y_pred >> $tmp/classifier.y_pred
        echo $(basename "$line") | sed 's/\..*//g' >> $tmp/classifier.y_true
        [ "$(basename $line | sed 's/\..*//g')"  = "$y_pred" ] && echo "\e[34m transform \e[0m $name \e[33m $y_pred \e[0m"
        [ "$(basename $line | sed 's/\..*//g')" != "$y_pred" ] && echo "\e[34m transform \e[0m $name \e[31m $y_pred \e[0m"
        echo $y_pred > $learning_dir/classifier_test_results/$name
    done

	python -c "
from sklearn import metrics

with open('$tmp/classifier.y_pred') as f:
    y_pred = [ line.rstrip('\n') for line in f ]
with open('$tmp/classifier.y_true') as f:
    y_true = [ line.rstrip('\n') for line in f ]

score = metrics.accuracy_score(y_true, y_pred)
print('accuracy:   %0.3f' % score)

print('classification report:')
print(metrics.classification_report(y_true, y_pred))

print('confusion matrix:')
print(metrics.confusion_matrix(y_true, y_pred))
"

if [ $learning_evaluate_vects = "true" ]
then
    echo "\e[34m vectors \e[0m "
    rm -fr $tmp/vects
    find $learning_dir/classifier_train/ -type f | sort | while read line
    do
        echo -n $line " " >> $tmp/vects
        if [ -e "$learning_dir/vectors/$(basename $line)" ]
        then
            cat "$learning_dir/vectors/$(basename $line)" | vect2s >> $tmp/vects
        else
            $vectorizer_method.txt2vec "$line" | vect2s >> $tmp/vects
        fi
        echo "\e[34m vector \e[0m " $(basename $line)
    done
    echo "=====" >> $tmp/vects
    find $learning_dir/classifier_test/ -type f | sort | while read line
    do
        y_true=$(basename $line | sed 's/\..*//g')
        [ -e $learning_dir/classifier_test_results/$(basename $line) ] && y_pred=$(cat $learning_dir/classifier_test_results/$(basename $line))
        [ -e $learning_dir/classifier_test_results/$(basename $line) ] || y_pred=$($classifier_method.transform $line)
        [ "$y_true"  = "$y_pred" ] && echo "\e[34m vector \e[0m $(basename $line) \e[32m $y_true \e[0m \e[33m $y_pred \e[0m"
        [ "$y_true" != "$y_pred" ] && echo "\e[34m vector \e[0m $(basename $line) \e[32m $y_true \e[0m \e[31m $y_pred \e[0m"
        [ "$y_true" != "$y_pred" ] && echo -n '*** ' >> $tmp/vects
        echo -n T:$y_true P:$y_pred " " >> $tmp/vects
        echo -n $line " " >> $tmp/vects
        if [ -e "$learning_dir/vectors/$(basename $line)" ]
        then
            cat "$learning_dir/vectors/$(basename $line)" | vect2s >> $tmp/vects
        else
            $vectorizer_method.txt2vec "$line" | vect2s >> $tmp/vects
        fi
        # echo "\e[34m vector \e[0m " $(basename $line)
    done
fi
}

linear_regression.fit(){
    rm -fr $tmp/regression.x $tmp/regression.y
    for file in $learning_dir/vectors_reduced/*
    do
        cat $file | sed -e 's/\[//g' -e 's/\]//g' -e 's/, / /g' >> $tmp/regression.x
        cat $learning_dir/regression_fit/$(basename $file) >> $tmp/regression.y
    done

	python -c "
import numpy as np
from sklearn import linear_model
from sklearn.externals import joblib
X = np.loadtxt('$tmp/regression.x')
Y = np.loadtxt('$tmp/regression.y')
learning_dir='$learning_dir'
reg = linear_model.LinearRegression()
reg.fit (X, Y)
joblib.dump(reg, learning_dir + '/regression/regression.pkl')
"
}

kmeans_clustering.fit(){
    n_clusters=3
    seq 0 $n_clusters | nl -v0 | sed 's/\t/ /g' | sed -e 's/ \+/ /g' > $learning_dir/cluster/map
    rm -fr $tmp/clusters
    for file in $learning_dir/vectors_reduced/*
    do
        cat $file | sed -e 's/\[//g' -e 's/\]//g' -e 's/, / /g' >> $tmp/clusters.x
    done

python -c "
from sklearn.cluster import KMeans
from sklearn.externals import joblib
import numpy as np
X = np.loadtxt('$tmp/clusters.x')
learning_dir='$learning_dir'
n_clusters=$n_clusters
kmeans = KMeans(n_clusters=n_clusters, random_state=0)
kmeans.fit(X)
joblib.dump(kmeans, learning_dir + '/cluster/cluster.pkl')
"
}

pca.transform() {
	python -c "
import numpy as np
learning_dir='$learning_dir'
pca=np.loadtxt(learning_dir+'/pca/pca')
eigenv=np.loadtxt(learning_dir+'/pca/eigenv')
eigenv=[ i + 0.00001 for i in eigenv ]
average=np.loadtxt(learning_dir+'/pca/average')
normalized=$1;
normalized=np.subtract(normalized, average)
normalized=np.matmul(pca, normalized)
normalized=np.divide( normalized, np.sqrt(eigenv) )
print normalized
" | sed \
    -e 's/ \+/ /g' \
    -e 's/^\[ /[/g' -e 's/ \]$/]/g' -e 's/ /, /g' | paste -d' ' -s
}

index.transform() {
	python -c "
import numpy as np
learning_dir='$learning_dir'
indices=np.loadtxt(learning_dir+'/pca/indices')
X=$1;
normalized=[ X[i] for i in indices ]
print normalized
" | sed \
    -e 's/ \+/ /g' \
    -e 's/^\[ /[/g' -e 's/ \]$/]/g' -e 's/ /, /g' | paste -d' ' -s
}

identity.transform(){
    echo $1
}

pca100.transform() {
	python -c "
import numpy as np
learning_dir='$learning_dir'
pca=np.loadtxt(learning_dir+'/pca/pca')
eigenv=np.loadtxt(learning_dir+'/pca/eigenv')
eigenv=[ i + 0.00001 for i in eigenv ]
average=np.loadtxt(learning_dir+'/pca/average')
normalized=$1;
normalized=np.subtract(normalized, average)
normalized=np.matmul(pca, normalized)
normalized=np.divide( normalized, np.sqrt(eigenv) )
print normalized
" | sed \
    -e 's/ \+/ /g' \
    -e 's/^\[ /[/g' -e 's/ \]$/]/g' -e 's/ /, /g' | paste -d' ' -s
}

pca_pickle.transform() {
	python -c "
import numpy as np
import sys
np.set_printoptions(threshold=sys.maxsize)
from sklearn.externals import joblib
learning_dir='$learning_dir'
pca=joblib.load(learning_dir + '/pca/pca.pkl')
x=[$1];
print pca.transform(x)[0]
" | sed \
    -e 's/ \+/ /g' \
    -e 's/^\[ /[/g' -e 's/ \]$/]/g' -e 's/ /, /g'
}

pca_scaler.transform() {
	python -c "
import numpy as np
import sys
np.set_printoptions(threshold=sys.maxsize)
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.externals import joblib
learning_dir='$learning_dir'
pca=joblib.load(learning_dir + '/pca/pca.pkl')
scaler=joblib.load(learning_dir + '/pca/scaler.pkl')
x=[$1]
x=scaler.transform(x)
print pca.transform(x)[0]
" | sed \
    -e 's/ \+/ /g' \
    -e 's/^\[ /[/g' -e 's/ \]$/]/g' -e 's/ /, /g' | paste -s
}

lda.transform() {
	python -c "
import numpy as np
import sys
np.set_printoptions(threshold=sys.maxsize)
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.externals import joblib
learning_dir='$learning_dir'
clf=joblib.load(learning_dir + '/pca/lda.pkl')
x=[$1];
print clf.transform(x)[0]
" | sed \
    -e 's/ \+/ /g' \
    -e 's/^\[ /[/g' -e 's/ \]$/]/g' -e 's/ /, /g'
}

learning.configure(){
    [ "$1" != "" ] && cd $1 && export learning_dir=$PWD
    cat    $learning_dir/configure/configure
    source $learning_dir/configure/configure
}

learning.doc2txt(){
    local line
	filename=$1
	destination=$2

	[ -e "$destination" ] && return
	[ ${filename[1]} = '/' ] || filename="$PWD"/"$filename"
	[ "$destination" = "" ] && destination=$tmp/summary

	rm -fr "$destination"

	[ -d "$filename" ] &&  ( ls "$filename" | while read line; do ( doc2txt_rec "$filename"/"$line" "$destination" ); done )
	[ -f "$filename" ] &&  doc2txt_rec "$filename" "$destination"

	[ "$destination" = "$tmp/summary" ] && cat $tmp/summary
}

learning.doc2rvect(){
    if [ -e "$learning_dir/vectors/$(basename $1)" ]
    then
        vector=$(cat $learning_dir/vectors/$(basename $1))
        $pca_method.transform $vector
    else
        rm -fr $tmp/txt
        learning.doc2txt $1 $tmp/txt
        vector=$($vectorizer_method.txt2vec $tmp/txt)
        $pca_method.transform $vector
    fi
}

learning.rvect2key(){
	python -c "
import numpy as np
normalized=$1
normalized=[ i*$learning_scale + 0.5 for i in normalized ]
normalized=[ max(min(i,1.0),0.0) for i in normalized ]
chars=\"0123456789\"
rvect=[ chars[int((i)*(len(chars)-1))] for i in normalized ]
print ''.join(rvect)
"
}

learning.rvect2key_2(){
	python -c "
import numpy as np
normalized=$1
normalized=[ i-min(normalized) for i in normalized ]
normalized=[ i/(max(normalized)+0.00001) for i in normalized ]
normalized=[ max(min(i,1.0),0.0) for i in normalized ]
chars=\"0123456789\"
rvect=[ chars[int((i)*(len(chars)))] for i in normalized ]
print ''.join(rvect)
"
}

learning.key(){
    learning.rvect2key "$(learning.doc2rvect $1)"
}

learning.fit(){
    local line

    [ -e $learning_dir/documents ] || mkdir -p $learning_dir/documents
    #[ -e $learning_dir/topics ] && learning.get_documents

    #rm -fr $learning_dir/txts;
    mkdir -p $learning_dir/txts
    ls $learning_dir/documents/* | sort | while read line
    do
        [ -e $learning_dir/txts/$(basename "$line") ] && echo "\e[34m skip \e[0m " $line && continue
        echo "\e[34m doc2txt \e[0m " $line
        learning.doc2txt "$line" $learning_dir/txts/$(basename "$line")
    done
    #cp -r $learning_dir/documents $learning_dir/txts

    [ $( echo $vectorizer_method | grep keywords ) ] || ( rm -fr $learning_dir/features; mkdir -p $learning_dir/features )
    echo "\e[34m get_features \e[0m "
    $vectorizer_method.get_features

    #rm -fr $learning_dir/vectors;
    mkdir -p $learning_dir/vectors
    ls $learning_dir/txts/* | sort | while read line
    do
        [ -e $learning_dir/vectors/$(basename "$line") ] && echo "\e[34m skip \e[0m " $line && continue
        echo "\e[34m txt2vec \e[0m " $line
        $vectorizer_method.txt2vec "$line" > $learning_dir/vectors/$(basename "$line")
    done

    rm -fr $learning_dir/pca; mkdir -p $learning_dir/pca
    echo "\e[34m pca \e[0m "
    $pca_method.fit

    #rm -fr $learning_dir/vectors_reduced;
    mkdir -p $learning_dir/vectors_reduced
    [ -e $learning_dir/classifier_train ] || cp -r $learning_dir/documents $learning_dir/classifier_train
    ls $learning_dir/classifier_train/* | sort | while read line
    do
        [ -e $learning_dir/vectors_reduced/$(basename "$line") ] && echo "\e[34m skip \e[0m " $line && continue
        echo "\e[34m reduce \e[0m " $line
        learning.doc2rvect "$line" > $learning_dir/vectors_reduced/$(basename "$line")
    done

    rm -fr $learning_dir/classifier; mkdir -p $learning_dir/classifier
    echo "\e[34m classifier \e[0m "
    $classifier_method.fit
    cp -r $learning_dir/classifier_train $learning_dir/classifier_test
    [ -e $learning_dir/classifier_test ] && classifier.evaluate

    rm -fr $learning_dir/regression; mkdir -p $learning_dir/regression
    echo "\e[34m regression \e[0m "
    [ -e $learning_dir/regression_fit ] && $regression_method.fit

    rm -fr $learning_dir/cluster; mkdir -p $learning_dir/cluster
    echo "\e[34m cluster \e[0m "
    $clustering_method.fit
}

linear_classifier.transform(){
x=$(learning.doc2rvect $1)

y=$(python -c "
x=[$x]
learning_dir='$learning_dir'
import numpy as np
from sklearn import linear_model
from sklearn.externals import joblib
clf=joblib.load(learning_dir + '/classifier/classifier.pkl')
print( int(clf.predict(x)[0]) )
")

cat $learning_dir/classifier/map | grep "^ $y " | awk '{print $2}'
}

logistic_classifier.doc2ps(){
x=$(learning.doc2rvect $1)

python -c "
x=[$x]
learning_dir='$learning_dir'
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.externals import joblib
clf=joblib.load(learning_dir + '/classifier/classifier.pkl')
print (' '.join( map( str, clf.predict_proba(x)[0] )))
"
}

ensemble.txt2vec(){
    for a in logistic_classifier
    do
        $a.doc2ps "$1"
    done | paste -d' ' -s | sed -e 's/ /,/g' -e 's/^/[/g' -e 's/$/]/g' -e 's/,/, /g'
}

kmeans_clustering.transform(){
x=$(learning.doc2rvect $1)

y=$(python -c "
x=[$x]
learning_dir='$learning_dir'
import numpy as np
from sklearn.cluster import KMeans
from sklearn.externals import joblib
kmeans=joblib.load(learning_dir + '/cluster/cluster.pkl')
print( kmeans.predict(x)[0] )
")

cat $learning_dir/cluster/map | grep "^ $y " | awk '{print $2}'
}

linear_regression.transform(){
x=$(learning.doc2rvect $1)

python -c "
x=[$x]
learning_dir='$learning_dir'
import numpy as np
import sys
np.set_printoptions(threshold=sys.maxsize)
from sklearn import linear_model
from sklearn.externals import joblib
reg=joblib.load(learning_dir + '/regression/regression.pkl')
print(reg.predict(x)[0])
"
}

learning.distance(){
    python -c "
import numpy as np
import math
import scipy.spatial
from sets import Set
h1=$1
h2=$2
if( '$learning_distance' == 'jaccard' ):
    h1_set = Set( [ i for i in range(0, len(h1)) if h1[i] != 0 ] )
    h2_set = Set( [ i for i in range(0, len(h2)) if h2[i] != 0 ] )
    print 1.0-float(len( h1_set & h2_set ))/float(len( h1_set | h2_set ) + 0.00001)
if( '$learning_distance' == 'euclidean' ):
    print math.sqrt(np.sum(np.square(np.subtract(h1, h2))))
if( '$learning_distance' == 'cosine' ):
    print scipy.spatial.distance.cosine(h1,h2)
if( '$learning_distance' == 'scipy_jaccard' ):
    print scipy.spatial.distance.jaccard(h1, h2)
"
}

learning.txtdistance(){
    learning.distance "$(learning.doc2rvect $1)" "$(learning.doc2rvect $2)"
}

closest_classifier.sortdist(){
    h1=$(learning.doc2rvect $1)

    python -c "
import sys, errno
import numpy as np
import math
import scipy.spatial
from sets import Set

learning_dir='$learning_dir'

with open(learning_dir + '/classifier/sortnames') as f:
    names = [ line.rstrip('\n') for line in f ]
with open(learning_dir + '/classifier/sortvectors') as f:
    vectors = [ line.rstrip('\n') for line in f ]
    vectors = [ np.fromstring(vector, dtype=float, sep=' ') for vector in vectors ]

def distance(h1,h2):
    if( '$learning_distance' == 'jaccard' ):
        h1_set = Set( [ i for i in range(0, len(h1)) if h1[i] != 0 ] )
        h2_set = Set( [ i for i in range(0, len(h2)) if h2[i] != 0 ] )
        return 1.0-float(len( h1_set & h2_set ))/float(len( h1_set | h2_set ) + 0.00001)
    if( '$learning_distance' == 'euclidean' ):
        return math.sqrt(np.sum(np.square(np.subtract(h1, h2))))
    if( '$learning_distance' == 'cosine' ):
        return scipy.spatial.distance.cosine(h1,h2)
    if( '$learning_distance' == 'scipy_jaccard' ):
        return scipy.spatial.distance.jaccard(h1, h2)

distances = { names[i]:distance(vectors[i],$h1) for i in range(0, len(names)) }

try:
    for key, value in sorted(distances.iteritems(), key=lambda (k,v): (v,k)):
        print '%s' % (key)
    sys.stdout.flush()
except IOError as e:
        pass
"
}

closest_classifier.pairsdist(){

    python -c "
import sys, errno
import numpy as np
import math
import scipy.spatial
from sets import Set

learning_dir='$learning_dir'

with open(learning_dir + '/classifier/sortnames') as f:
    names = [ line.rstrip('\n') for line in f ]
with open(learning_dir + '/classifier/sortvectors') as f:
    vectors = [ line.rstrip('\n') for line in f ]
    vectors = [ np.fromstring(vector, dtype=float, sep=' ') for vector in vectors ]

def distance(h1,h2):
    if( '$learning_distance' == 'jaccard' ):
        h1_set = Set( [ i for i in range(0, len(h1)) if h1[i] != 0 ] )
        h2_set = Set( [ i for i in range(0, len(h2)) if h2[i] != 0 ] )
        return 1.0-float(len( h1_set & h2_set ))/float(len( h1_set | h2_set ) + 0.00001)
    if( '$learning_distance' == 'euclidean' ):
        return math.sqrt(np.sum(np.square(np.subtract(h1, h2))))
    if( '$learning_distance' == 'cosine' ):
        return scipy.spatial.distance.cosine(h1,h2)
    if( '$learning_distance' == 'scipy_jaccard' ):
        return scipy.spatial.distance.jaccard(h1, h2)

mydict = {}
for i in range(0, len(names)):
    for j in range(i+1, len(names)):
        dist = distance(vectors[i],vectors[j])
        mydict[frozenset({names[i], names[j]})] = dist

std=sorted(mydict,key=mydict.get)

for a in std:
    if len(a) == 2:
        name1, name2 = a
        print name1
        print name2
        print
"
}

closest_classifier.coverage(){
    python -c "
import numpy as np
import random

learning_dir='$learning_dir'

with open(learning_dir + '/classifier/sortnames') as f:
    names = [ line.rstrip('\n') for line in f ]
with open(learning_dir + '/classifier/sortvectors') as f:
    vectors = [ line.rstrip('\n') for line in f ]
    vectors = [ np.fromstring(vector, dtype=float, sep=' ') for vector in vectors ]

optimum={ 'names':[], 'cost': 0. }
for i in range(0, 1000):
    n=random.randint(1,len(names))
    selections=np.random.permutation(np.concatenate( [np.zeros(len(names)-n), np.ones(n)] ))
    vssel=[ vectors[i] for i in range(0,len(names)) if selections[i] == 1 ]
    nssel=[ names[i] for i in range(0,len(names)) if selections[i] == 1 ]
    covered=np.sum(vssel,axis=0)
    ncovered=len(np.where( covered > 0 ))
    if( float(ncovered)/float(n) > optimum['cost'] ):
        optimum['names'] = nssel
        optimum['cost'] = float(ncovered)/float(n)
        print optimum

for a in optimum['names']:
    print a
"
}

cluster_closest.fit(){
    rm -fr $learning_dir/classifier/{0..2}
    for a in $learning_dir/txts/*
    do
        cluster=$(kmeans_clustering.transform $a)
        mkdir -p $learning_dir/classifier/$cluster
        cat $learning_dir/vectors_reduced/$(basename $a) >> $learning_dir/classifier/$cluster/sortvectors
        echo $(basename $a) >> $learning_dir/classifier/$cluster/sortnames
        echo -n .
    done
}

cluster_closest.sortdist(){
    h1=$(learning.doc2rvect $1)

cluster=$(python -c "
x=[$h1]
learning_dir='$learning_dir'
import numpy as np
from sklearn.cluster import KMeans
from sklearn.externals import joblib
kmeans=joblib.load(learning_dir + '/cluster/cluster.pkl')
print( kmeans.predict(x)[0] )
")

    python -c "
import sys, errno
import numpy as np
import math
import scipy.spatial
from sets import Set

learning_dir='$learning_dir'

with open(learning_dir + '/classifier/"$cluster"/sortnames') as f:
    names = [ line.rstrip('\n') for line in f ]
with open(learning_dir + '/classifier/"$cluster"/sortvectors') as f:
    vectors = [ line.rstrip('\n') for line in f ]
    vectors = [ np.fromstring(vector, dtype=float, sep=' ') for vector in vectors ]

def distance(h1,h2):
    if( '$learning_distance' == 'jaccard' ):
        h1_set = Set( [ i for i in range(0, len(h1)) if h1[i] != 0 ] )
        h2_set = Set( [ i for i in range(0, len(h2)) if h2[i] != 0 ] )
        return 1.0-float(len( h1_set & h2_set ))/float(len( h1_set | h2_set ) + 0.00001)
    if( '$learning_distance' == 'euclidean' ):
        return math.sqrt(np.sum(np.square(np.subtract(h1, h2))))
    if( '$learning_distance' == 'cosine' ):
        return scipy.spatial.distance.cosine(h1,h2)
    if( '$learning_distance' == 'scipy_jaccard' ):
        return scipy.spatial.distance.jaccard(h1, h2)

distances = { names[i]:distance(vectors[i],$h1) for i in range(0, len(names)) }

try:
    for key, value in sorted(distances.iteritems(), key=lambda (k,v): (v,k)):
        print '%s' % (key)
    sys.stdout.flush()
except IOError as e:
        pass
"
}


keywords.filter () {
    local line

    [ $# -eq 0 ] && condition="true"
    [ $# -eq 1 ] && [ $1 = "-v" ] && condition="false" && shift

	cat | while read line
	do
        haskeyword=$( echo $line | tokenize | countwords | haskeyword )
		[ "$haskeyword" = $condition ] && echo $line
	done
}

learning.mapmv(){
    src_topic=$(echo $1 | sed 's/\..*//g')
    src_filename=$(cat $learning_dir/map_documents/map | grep $1 | awk '{$1=""; print}' | sed 's/^...//g' )
    src_crc=$(echo $1 | sed 's/^[^\.]*\.//g')
    dst_topic=$2
    dst_crc=$src_crc

    # learning_src
    [ "$src_filename" != "" ] && echo mv \"$learning_src/$src_topic/$src_filename\" \"$learning_src/$dst_topic/\"

    # documents
    echo mv $learning_dir/documents/$src_topic.$src_crc $learning_dir/documents/$dst_topic.$dst_crc
}

learning.join_topics(){
    local src_filename
    topic_src=$1
    topic_dst=$2

    #learning_src
    echo join $learning_src/$topic_src $learning_src/$topic_dst

    #documents
    ls $learning_dir/documents/ | egrep "^$topic_src\." | grep -v '.topic' | while read src_filename
    do
        dst_filename=$( echo $src_filename | sed "s/^$topic_src\./$topic_dst\./g" )
        echo mv $learning_dir/documents/$src_filename $learning_dir/documents/$dst_filename
        [ -e $learning_dir/map_documents ] && echo "cat $learning_dir/map_documents/map | sed 's/$src_filename/$dst_filename/g' > $tmp/map && mv $tmp/map $learning_dir/map_documents/map"
    done
    echo rm -rf $learning_dir/documents/$topic_src.topic

    #topics
    echo "cat $learning_dir/topics/topics | sed 's/ $topic_src\$/ $topic_dst/g' | sort | uniq > $tmp/topics && mv $tmp/topics $learning_dir/topics/topics"

}

learning.get_documents(){
    local line
    local file

    inipwd=$PWD

    mkdir -p $learning_dir/map_documents

    cat $learning_dir/topics/topics | awk '{print $NF}' > $tmp/topics_to_get
    [ -e $learning_dir/topics/skip ] && comm -13 <(cat $learning_dir/topics/skip | sort | uniq) <(cat $learning_dir/topics/topics | awk '{print $NF}' | sort | uniq) > $tmp/topics_to_get
    [ $# -gt 0 ] && echo $* | sed 's/ /\n/g' > $tmp/topics_to_get

    cat $tmp/topics_to_get | while read line
    do
        echo "\e[34m ===== $line ===== \e[0m"

        if [ -e $learning_src ]
        then
            if [ $(ls $learning_dir/documents/ | egrep "^$line\..*" | wc -l) -ge $learning_docs_per_class ]
            then
                echo "\e[34m enough documents \e[0m $line"
                continue
            fi

            if [ ! -e $learning_src/$line ]
            then
                echo "\e[31m topic not found \e[0m $line"
                continue
            fi

            if [ $(find $learning_src/$line -maxdepth 1 -type f | wc -l) -lt $learning_docs_per_class ]
            then
                echo "\e[33m not enough documents \e[0m $line"
            fi

            topic=$line
            cd "$learning_src/$line"
            find -maxdepth 1 -type f | sort -R | head -n $learning_docs_per_class | while read file
            do
                crctitle=$(basename "$file" | md5sum | cut -d' ' -f1)
                echo "\e[34m doc2txt \e[0m $file $topic.$crctitle"
                learning.doc2txt "$file" "$learning_dir/documents/$topic.$crctitle"
                echo "$topic.$crctitle" "$file" >> $learning_dir/map_documents/map
            done
        fi

        if [ $generate_topic_documents = "true" ]
        then
            echo $line | sed 's/_/ /g' > "$learning_dir/documents/$line.topic"
        fi
    done

    cd $inipwd

    find $learning_dir/documents/ -type f | while read line
    do

        #if [ "$( file $line | grep text )" = "" ]
        #then
            #echo "\e[31m $line is not text \e[0m"
            #rm -rf $line
            #continue
        #fi

        #if [ "$( du -bc $line | head -n1 | awk '{print $1}' )" -lt 1000 ]
        #then
            #echo "\e[31m $line is very small \e[0m"
            #rm -rf $line
            #continue
        #fi

        if [ "$( cat $line | tokenize | countwords | haskeyword )" = "false" ]
        then
            echo "\e[31m $line does not contain any keyword \e[0m"
            rm -rf $line
            continue
        fi

    done

    if [ $generate_topic_documents = "true" ]
    then
        echo '' > "$learning_dir/documents/00_unknown.topic"
    fi

}

learning.restart(){
    rm -rf $learning_dir
    mkdir -p $learning_dir/documents
    mkdir -p $learning_dir/features
    mkdir -p $learning_dir/topics
    cp ~/topics     $learning_dir/topics
    cp ~/skip       $learning_dir/topics
    cp ~/keywords   $learning_dir/features
}

learning.restart(){
    cd $learning_dir && ls | egrep -v '^documents$' | egrep -v '^features$' | egrep -v '^topics$' | egrep -v 'summaries' | egrep -v '^metadata$' | egrep -v '^configure$' | xargs rm -rf
    mkdir -p $tmp
}

learning.queue_get(){
    local line

    mkdir -p $learning_dir/summaries

    [ $# -eq 0 ] && limit=10
    [ $# -eq 1 ] && limit=$1

    cd $learning_queue
    ls -- | egrep '(pdf$|epub$)' | head -n $limit | while read line
    do
        echo "\e[34m doc2txt \e[0m $line"
        learning.doc2txt "$line" "$learning_dir/summaries/$line.txt"
    done

    cd $learning_dir/summaries

}

keywords.highlight(){
    local a
    cat > $tmp/highlight
    keywords=$(cat $tmp/highlight | tokenize | sort | uniq | keywords.filter)

    esc=$(printf '\033')
    echo $keywords | while read a
    do
        sed -i "s|\<$a\>|${esc}[31m$a${esc}[0m|gI" $tmp/highlight
    done
    cat $tmp/highlight
}

keywords.diffhighlight(){
    local l
    local a
    cat > $tmp/highlight.txt
    closest=$(closest_classifier.sortdist $tmp/highlight.txt | head -n1 | sed "s|^|$learning_dir/documents/|g")

    cat $tmp/highlight.txt | tokenize | countwords | getkeywords | egrep -v '^0' | awk '{print $2}' | sort > $tmp/kwr1
    cat $closest           | tokenize | countwords | getkeywords | egrep -v '^0' | awk '{print $2}' | sort > $tmp/kwr2

    comm -12 $tmp/kwr1 $tmp/kwr2 > $tmp/commonregex

    cat $tmp/commonregex | while read l
    do
        cat $tmp/highlight.txt | tokenize | sort | uniq | egrep $l
    done > $tmp/keywords

    esc=$(printf '\033')

    cat $tmp/keywords | while read a
    do
        sed -i "s|_| UnderScore |g" $tmp/highlight.txt
        sed -i "s|\<$a\>|${esc}[31m$a${esc}[0m|gI" $tmp/highlight.txt
        sed -i "s| UnderScore |_|g" $tmp/highlight.txt
    done

    cat $tmp/highlight.txt
}

learning.queue_classify(){
    local line
    local metadata

    [ $classify_highlight = "true"  ] && cmdhl=keywords.highlight
    [ $classify_highlight = "diff"  ] && cmdhl=keywords.diffhighlight
    [ $classify_highlight = "false" ] && cmdhl=cat

    mkdir -p $learning_dir/summaries_classified/

    cd $learning_dir/summaries/
    ls | while read line
    do
        closest=$(closest_metadata.sortdist "$line" | head -n1)
        #classification=$(echo $closest | sed 's/\..*//g')
        classification=$( $classifier_method.transform "$line" )
        echo $classification > $learning_dir/summaries_classified/$line
        echo $closest | sed "s|^|closest : $learning_dir/documents/|g" >> $learning_dir/summaries_classified/$line

        if [ -e "$learning_dir/metadata/$(echo $line | sed 's/\.txt$//g')/" ]
        then
            ls "$learning_dir/metadata/$(echo $line | sed 's/\.txt$//g')/"* | while read metadata
            do
                echo $(basename "$metadata") : $(cat "$learning_dir/metadata/$(basename "$line" | sed 's/\.txt$//g')/$(basename "$metadata")") | $cmdhl >> $learning_dir/summaries_classified/$line
            done
        fi

        echo "====="         >> $learning_dir/summaries_classified/$line
        cat "$line" | $cmdhl >> $learning_dir/summaries_classified/$line
        echo "\e[34m $line \e[0m \e[33m $classification \e[0m"
    done

    cd $learning_dir/summaries_classified
}

learning.queue_mv(){
    local line

    cd $learning_dir/summaries_classified/

    #sed -i -r "s/[[:cntrl:]]\[[0-9]{1,3}m//g" *

    rm -fr $tmp/script
    ls | while read line
    do
        file=$(echo $line | sed 's/\.txt$//g')
        true_tag=$(cat "$line" | head -n1)

        [ "$true_tag" = "" ] && continue

        rm -fr $tmp/part00 $tmp/part01
        csplit "$line" -s -f $tmp/part /=====/

        rm -fr "$tmp/$line"
        mv $tmp/part01 "$tmp/$line"
        pred_tag=$($classifier_method.transform "$tmp/$line")
        rm -fr "$tmp/$line"


        if [ -e $learning_dir/topics/topics ]
        then
            correct=$(cat $learning_dir/topics/topics | awk '{print $NF}' | head -n1 | egrep -i "^$true_tag$" )
            [ $? = 0 ] && true_tag=$correct
        fi

        if [ "$true_tag" = "$pred_tag" ]
        then
            echo "\e[34m $file \e[0m \e[32m $true_tag \e[0m"
            [ -e "$learning_src/$true_tag" ] || echo "mkdir -p \"$learning_src/$true_tag/\"" | tee -a $tmp/script
            echo "mv \"$learning_queue/$file\" \"$learning_src/$true_tag/\"" | tee -a $tmp/script
        fi

        if [ ! "$true_tag" = "$pred_tag" ]
        then
            echo "\e[34m $file \e[0m \e[31m $pred_tag \e[0m \e[32m $true_tag \e[0m"
            crctitle=$(basename "$file" | md5sum | cut -d' ' -f1)
            [ -e "$learning_src/$true_tag" ] || echo "mkdir -p \"$learning_src/$true_tag/\"" | tee -a $tmp/script
            echo "mv \"$learning_queue/$file\" \"$learning_src/$true_tag/\"" | tee -a $tmp/script
            echo "cat \"$learning_dir/summaries_classified/$line\" > $learning_dir/documents/$true_tag.$crctitle" | tee -a $tmp/script
        fi

        if [ -e $learning_dir/metadata/$file/ ]
        then
            echo "echo $true_tag > \"$learning_dir/metadata/$file/classification\"" | tee -a $tmp/script
            echo "cat \"$learning_dir/summaries_classified/$line\" > \"$learning_dir/metadata/$file/summary\"" | tee -a $tmp/script
        fi

    done
}

learning.queue_loop(){

    [ $# -eq 0 ] && queue_n=100
    [ $# -eq 1 ] && queue_n=$1

    [ -e $learning_dir/summaries ] && rm -rf $learning_dir/summaries
    [ -e $learning_dir/summaries_classified ] && rm -rf $learning_dir/summaries_classified

    learning.queue_get $queue_n
    learning.queue_classify
    vim * '+:AnsiEsc' '+:au BufEnter * :AnsiEsc'
    learning.queue_mv
    vim $tmp/script
    source $tmp/script
}

# add \ to {}
# add match with \(\)

fill_isbn10n_metadata(){
    filename="$1"
    filename2=$(basename "$filename" .txt)
    isbn=$(echo "$filename" | sed 's/[^0-9]*\([0-9]\{10\}\)[^0-9]*/\1/g')
    echo "$filename \e[34m $isbn \e[0m"
    mkdir -p "$learning_dir/metadata/$filename2"
    echo $isbn > "$learning_dir/metadata/$filename2/isbn"
}

fill_isbn10l_metadata(){
    filename="$1"
    filename2=$(basename "$filename" .txt)
    isbn=$(echo "$filename" | sed 's/[^0-9]*\([0-9]\{9\}X\)[^0-9]*/\1/g')
    echo "$filename \e[34m $isbn \e[0m"
    mkdir -p "$learning_dir/metadata/$filename2"
    echo $isbn > "$learning_dir/metadata/$filename2/isbn"
}

fill_isbn13_metadata(){
    filename="$1"
    filename2=$(basename "$filename" .txt)
    isbn=$(echo "$filename" | sed 's/^[^0-9]*\(978[0-9]\{10\}\)[^0-9]*$/\1/g')
    echo "$filename \e[34m $isbn \e[0m"
    mkdir -p "$learning_dir/metadata/$filename2"
    echo $isbn > "$learning_dir/metadata/$filename2/isbn"
}

metadata.isbn(){

    local line

    if [ $# -eq 0 ]
    then
        cd $learning_dir/summaries
        ls | while read line
        do
            metadata.isbn $line
        done
        return
    fi

    line="$1"
    line2="$( echo $line | sed 's/-//g' )"
    [ "$(echo $line2 | egrep '^[^0-9]*[0-9]{10}[^0-9]*$')" ]    && fill_isbn10n_metadata "$line2"
    [ "$(echo $line2 | egrep '^[^0-9]*[0-9]{9}X[^0-9]*$')" ]    && fill_isbn10l_metadata "$line2"
    [ "$(echo $line2 | egrep '^[^0-9]*978[0-9]{10}[^0-9]*$')" ] && fill_isbn13_metadata  "$line2"
}

metadata.filename(){

    local line

    if [ $# -eq 0 ]
    then
        cd $learning_dir/summaries
        ls | while read line
        do
            metadata.filename "$line"
        done
        return
    fi

    line="$1"
    file=$(echo "$line" | sed 's/\.txt$//g')
    name=$(basename "$file" | sed 's/\.[^.]*$//')

    echo "$file \e[34m $name \e[0m"

    mkdir -p "$learning_dir/metadata/$file"
    echo "$name" > "$learning_dir/metadata/$file/filename"
}

metadata.keywords(){

    local line

    if [ $# -eq 0 ]
    then
        cd $learning_dir/documents
        ls | while read line
        do
            metadata.keywords "$line"
        done
        return
    fi

    file=$1

    mkdir -p "$learning_dir/metadata/$file"
    keywords=$(cat $file | tokenize | countwords | keywords.filter | sed 's/^[^ ] //g' | paste -d' ' -s)
    echo "$file \e[34m $keywords \e[0m"
    echo $keywords > "$learning_dir/metadata/$file/keywords"
}

metadata.keywords_idx(){

    local line

    if [ $# -eq 0 ]
    then
        cd $learning_dir/documents
        ls | while read line
        do
            metadata.keywords "$line"
        done
        return
    fi

    file=$1

    mkdir -p "$learning_dir/metadata/$file"
    words1=$(cat $file | tokenize | sort | uniq)
    words2=$(cat $learning_dir/features/full_keywords | sort | uniq)
    keywords=$(comm -12 <(echo $words1) <(echo $words2) | paste -d' ' -s)
    echo "$file \e[34m $keywords \e[0m"
    echo $keywords > "$learning_dir/metadata/$file/keywords"
}

metadata.isbn2title(){
    local line
    if [ $# -eq 0 ]
    then
        cd $learning_dir/metadata
        ls | while read line
        do
            if [ -e "$line/isbn" ] && [ ! -e "$line/title" ]
            then
                metadata.isbn2title "$line"
            fi
        done
        return
    fi

    filename=$1
    isbn=$(cat "$learning_dir/metadata/$filename/isbn")
    #title=$(wget "http://libgen.io/search.php?req=$isbn&lg_topic=libgen&open=0&view=simple&res=25&phrase=1&column=def" -q -O - | grep 'td width=500' | cut -d'>' -f3 | cut -d'<' -f1 | head -n1)
    title=$(wget -q -U "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1)" https://isbn.nu/$isbn -O - | grep big_title | cut -d'>' -f2 | cut -d'<' -f1)

    echo "$filename \e[34m $title \e[0m"

    echo "$title" > "$learning_dir/metadata/$filename/title"
}

metadata.isbn2topics(){
    local line
    if [ $# -eq 0 ]
    then
        cd $learning_dir/metadata
        ls | while read line
        do
            if [ -e "$line/isbn" ] && [ ! -e "$line/topics" ]
            then
                metadata.isbn2topics "$line"
            fi
        done
        return
    fi

    filename=$1
    isbn=$(cat "$learning_dir/metadata/$filename/isbn")
    topics=$(wget -q -U "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1)" https://isbn.nu/$isbn -O - | grep sisbn | cut -d'>' -f3 | cut -d'<' -f1)

    esc=$(printf '\033')
    echo "$filename \e[34m $(echo $topics | paste -s -d'@' | sed "s/@/${esc}[0m,${esc}[34m/g") \e[0m"

    echo "$topics" > "$learning_dir/metadata/$filename/title"
}

keywords.new_kwords(){
cat $tmp/vects | egrep '\*\*\*' | awk '{print $4}' | xargs cat | tokenize | countwords | awk '{print $2}' | sort | uniq | keywords.filter -v
}

learning.loop(){
local line

for a in $(cat $tmp/vects | grep '\*\*\*' | awk '{print $4}' )
do
    echo "===== $(echo $a | sed 's|/classifier_test/|/documents/|g' ) ====="
    closest=$(closest_classifier.sortdist $a | head -n1 | sed "s|^|$learning_dir/documents/|g")
    echo $closest
    cat $a | tokenize | countwords | awk '{print $2}' | keywords.filter -v       | sed "s/^/+ /g"
    cat $closest | tokenize | countwords | awk '{print $2}' | keywords.filter -v | sed "s/^/+ /g"

    cat $a       | tokenize | countwords | getkeywords | egrep -v '^0' | awk '{print $2}' | sort > $tmp/kwr1
    cat $closest | tokenize | countwords | getkeywords | egrep -v '^0' | awk '{print $2}' | sort > $tmp/kwr2

    comm -12 $tmp/kwr1 $tmp/kwr2 > $tmp/commonregex

    cat $tmp/commonregex | while read line
    do
        cat $a $closest | tokenize | sort | uniq | egrep $line
    done | sed 's/^/- /g'

done
}

sliding_indexer(){
    file=$1
    echo "\e[34m $file \e[0m "
    linesnr=$(cat $file | wc -l)
    keybase=$(echo $file | md5sum | awk '{print $1}')
    for linenr in $(seq 1 $linesnr)
    do
        echo -n .
        [ "$(cat $file | tail -n+$linenr | head -n 1 | sed -e 's/ //g' -e 's/\t//g')" = "" ] && continue
        lines=$(cat $file | tail -n+$linenr | head -n 1 | paste -d' ' -s)
        [ $indexer_add_kw = "true" ] && [ -e "$learning_dir/metadata/$(basename "$file")/keywords" ] && lines="$(cat "$learning_dir/metadata/$(basename "$file")/keywords") $lines"
        key="$keybase:$linenr"
        echo $lines > $learning_dir/txts/$key
        echo $file:$linenr > $learning_dir/map/$key
    done
    echo
}

learning.index(){
    local file

    if [ "$1" != "" ]
    then
        cd $1
        find -type f | while read file
        do
            src=$file
            dst=$(echo $file | sed "s|^\.|$learning_dir/documents|g")
            [ -e $dst ] && continue
            dir=$(dirname $dst)
            mkdir -p $dir
            echo "\e[34m doc2txt \e[0m $src -> $dst"
            doc2txt_rec "$src" "$dst"
        done
    fi

    mkdir -p $learning_dir/txts $learning_dir/map
    find $learning_dir/documents -type f | while read file
    do
        echo "\e[34m index \e[0m $file"
        $indexer_method $file
    done

    [ $( echo $vectorizer_method | grep keywords ) ] || ( rm -fr $learning_dir/features; mkdir -p $learning_dir/features )
    echo "\e[34m get_features \e[0m "
    $vectorizer_method.get_features

    #rm -fr $learning_dir/vectors;
    mkdir -p $learning_dir/vectors
    ls $learning_dir/txts/* | sort | while read line
    do
        [ -e $learning_dir/vectors/$(basename "$line") ] && echo "\e[34m skip \e[0m " $line && continue
        echo "\e[34m txt2vec \e[0m " $line
        $vectorizer_method.txt2vec "$line" > $learning_dir/vectors/$(basename "$line")
    done

    #rm -fr $learning_dir/vectors_reduced;
    cp -r $learning_dir/vectors $learning_dir/vectors_reduced

    rm -fr $learning_dir/classifier
    mkdir -p $learning_dir/classifier
    closest_classifier.fit
}

learning.normalize_txt(){
    [ $# -gt 1 ] || dst=$1
    [ $# -gt 1 ] && dst=$2

    [ "$dst" = "-" ] && dst=/dev/stdout

    cat $1 | paste -d' ' -s | sed -e 's/[0-9]\. //g' -e 's/\. /.\n/g' > $tmp/document
    cat $tmp/document > $dst
}

classify_folders(){
    ls $1 | while read line
    do
        classification=$(echo $line | learning.classify -)
        hierarchy=$(cat $learning_dir/topics/topics | grep "\<$classification\$" | head -n1)
        echo "$line -> $classification -> $hierarchy"
    done
}

classify_and_mirror(){
    [ $# -eq 2 ] || return
    mkdir "$2"
    cd "$2" || return
    classify_folders "$1" | tee ~/videos_cl
    cat ~/videos_cl | grep -v '\-> $' | sed 's/\(.*\) -> \(.*\) -> \(.*\)/\1 -> \3/g' > /tmp/videos_cl
    (echo SRC=$1; echo; lnk_map /tmp/videos_cl) > /tmp/script
    source /tmp/script
    lnk_hierarchy_fill_scale_3
}

list_all_topics(){
    ls $learning_src
    echo '====='
    cat $learning_dir/topics/topics | awk '{print $NF}'
    echo '====='
    ls $learning_dir/documents | cut -d. -f1
}

learning.leven_topics(){
    list_all_topics | sort | uniq | levenshtein.pl > topics_scr
}


